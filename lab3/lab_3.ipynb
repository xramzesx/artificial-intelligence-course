{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Sieci neuronowe"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Wstęp\n",
            "\n",
            "Celem laboratorium jest zapoznanie się z podstawami sieci neuronowych oraz uczeniem głębokim (*deep learning*). Zapoznasz się na nim z następującymi tematami:\n",
            "- treningiem prostych sieci neuronowych, w szczególności z:\n",
            "  - regresją liniową w sieciach neuronowych\n",
            "  - optymalizacją funkcji kosztu\n",
            "  - algorytmem spadku wzdłuż gradientu\n",
            "  - siecią typu Multilayer Perceptron (MLP)\n",
            "- frameworkiem PyTorch, w szczególności z:\n",
            "  - ładowaniem danych\n",
            "  - preprocessingiem danych\n",
            "  - pisaniem pętli treningowej i walidacyjnej\n",
            "  - walidacją modeli\n",
            "- architekturą i hiperaprametrami sieci MLP, w szczególności z:\n",
            "  - warstwami gęstymi (w pełni połączonymi)\n",
            "  - funkcjami aktywacji\n",
            "  - regularyzacją: L2, dropout"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Wykorzystywane biblioteki\n",
            "\n",
            "Zaczniemy od pisania ręcznie prostych sieci w bibliotece Numpy, służącej do obliczeń numerycznych na CPU. Później przejdziemy do wykorzystywania frameworka PyTorch, służącego do obliczeń numerycznych na CPU, GPU oraz automatycznego różniczkowania, wykorzystywanego głównie do treningu sieci neuronowych.\n",
            "\n",
            "Wykorzystamy PyTorcha ze względu na popularność, łatwość instalacji i użycia, oraz dużą kontrolę nad niskopoziomowymi aspektami budowy i treningu sieci neuronowych. Framework ten został stworzony do zastosowań badawczych i naukowych, ale ze względu na wygodę użycia stał się bardzo popularny także w przemyśle. W szczególności całkowicie zdominował przetwarzanie języka naturalnego (NLP) oraz uczenie na grafach.\n",
            "\n",
            "Pierwszy duży framework do deep learningu, oraz obecnie najpopularniejszy, to TensorFlow, wraz z wysokopoziomową nakładką Keras. Są jednak szanse, że Google (autorzy) będzie go powoli porzucać na rzecz ich nowego frameworka JAX ([dyskusja](https://www.reddit.com/r/MachineLearning/comments/vfl57t/d_google_quietly_moving_its_products_from/), [artykuł Business Insidera](https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6?IR=T)), który jest bardzo świeżym, ale ciekawym narzędziem.\n",
            "\n",
            "Trzecia, ale znacznie mniej popularna od powyższych opcja to Apache MXNet."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Konfiguracja własnego komputera\n",
            "\n",
            "Jeżeli korzystasz z własnego komputera, to musisz zainstalować trochę więcej bibliotek (Google Colab ma je już zainstalowane).\n",
            "\n",
            "Jeżeli nie masz GPU lub nie chcesz z niego korzystać, to wystarczy znaleźć odpowiednią komendę CPU [na stronie PyTorcha](https://pytorch.org/get-started/locally/). Dla Anacondy odpowiednia komenda została podana poniżej, dla pip'a znajdź ją na stronie.\n",
            "\n",
            "Jeżeli chcesz korzystać ze wsparcia GPU (na tym laboratorium nie będzie potrzebne, na kolejnych może przyspieszyć nieco obliczenia), to musi być to odpowiednio nowa karta NVidii, mająca CUDA compatibility ([lista](https://developer.nvidia.com/cuda-gpus)). Poza PyTorchem będzie potrzebne narzędzie NVidia CUDA w wersji 11.6 lub 11.7. Instalacja na Windowsie jest bardzo prosta (wystarczy ściągnąć plik EXE i zainstalować jak każdy inny program). Instalacja na Linuxie jest trudna i można względnie łatwo zepsuć sobie system, ale jeżeli chcesz spróbować, to [ten tutorial](https://www.youtube.com/results?search_query=nvidia+cuda+install+ubuntu+20.04) jest bardzo dobry."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# for conda users\n",
            "!conda install -y matplotlib pandas pytorch torchvision -c pytorch -c conda-forge"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "Othm3C2lLAsj"
         },
         "source": [
            "## Wprowadzenie\n",
            "\n",
            "Zanim zaczniemy naszą przygodę z sieciami neuronowymi, przyjrzyjmy się prostemu przykładowi regresji liniowej na syntetycznych danych:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {
            "id": "rnJsfxbnLAsj"
         },
         "outputs": [],
         "source": [
            "from typing import Tuple, Dict\n",
            "\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/",
               "height": 282
            },
            "id": "EaYpEXzBLAsl",
            "outputId": "2f8d2922-72f0-4d38-8548-d1262adf522e"
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "<matplotlib.collections.PathCollection at 0x219fe7b7c10>"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 640x480 with 1 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "np.random.seed(0)\n",
            "\n",
            "x = np.linspace(0, 1, 100)\n",
            "y = x + np.random.normal(scale=0.1, size=x.shape)\n",
            "\n",
            "plt.scatter(x, y)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "PEM_-yKELAsl"
         },
         "source": [
            "W przeciwieństwie do laboratorium 1, tym razem będziemy chcieli rozwiązać ten problem własnoręcznie, bez użycia wysokopoziomowego interfejsu Scikit-learn'a. W tym celu musimy sobie przypomnieć sformułowanie naszego **problemu optymalizacyjnego (optimization problem)**.\n",
            "\n",
            "W przypadku prostej regresji liniowej (1 zmienna) mamy model postaci $\\hat{y} = \\alpha x + \\beta$, z dwoma parametrami, których będziemy się uczyć. Miarą niedopasowania modelu o danych parametrach jest **funkcja kosztu (cost function)**, nazywana też funkcją celu. Najczęściej używa się **błędu średniokwadratowego (mean squared error, MSE)**:\n",
            "$$\\large\n",
            "MSE = \\frac{1}{N} \\sum_{i}^{N} (y - \\hat{y})^2\n",
            "$$\n",
            "\n",
            "Od jakich $\\alpha$ i $\\beta$ zacząć? W najprostszym wypadku wystarczy po prostu je wylosować jako niewielkie liczby zmiennoprzecinkowe.\n",
            "\n",
            "#### Zadanie 1 (0.5 punkt)\n",
            "\n",
            "Uzupełnij kod funkcji `mse`, obliczającej błąd średniokwadratowy. Wykorzystaj Numpy'a w celu wektoryzacji obliczeń dla wydajności."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "RaA7Q46TLAsm",
            "outputId": "5c57fe58-1934-4d21-9a7b-d14e9a23140b"
         },
         "outputs": [],
         "source": [
            "def mse(y: np.ndarray, y_hat: np.ndarray) -> float:\n",
            "    return np.sum((y - y_hat) ** 2) / len(y)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/",
               "height": 282
            },
            "id": "qSGfamGbLAsm",
            "outputId": "733ce15f-ae75-466b-e7ee-eb3c9d9d534c"
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "MSE: 0.133\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "[<matplotlib.lines.Line2D at 0x219fe8d7f70>]"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 640x480 with 1 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "a = np.random.rand()\n",
            "b = np.random.rand()\n",
            "print(f\"MSE: {mse(y, a * x + b):.3f}\")\n",
            "\n",
            "plt.scatter(x, y)\n",
            "plt.plot(x, a * x + b, color=\"g\", linewidth=4)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "Y--E9Mp9LAsn"
         },
         "source": [
            "Losowe parametry radzą sobie nie najlepiej. Jak lepiej dopasować naszą prostą do danych? Zawsze możemy starać się wyprowadzić rozwiązanie analitycznie, i w tym wypadku nawet nam się uda. Jest to jednak szczególny i dość rzadki przypadek, a w szczególności nie będzie to możliwe w większych sieciach neuronowych.\n",
            "\n",
            "Potrzebna nam będzie **metoda optymalizacji (optimization method)**, dającą wartości parametrów minimalizujące dowolną różniczkowalną funkcję kosztu. Zdecydowanie najpopularniejszy jest tutaj **spadek wzdłuż gradientu (gradient descent)**.\n",
            "\n",
            "Metoda ta wywodzi się z prostych obserwacji, które tutaj przedstawimy. Bardziej szczegółowe rozwinięcie dla zainteresowanych: [sekcja 4.3 \"Deep Learning Book\"](https://www.deeplearningbook.org/contents/numerical.html), [ten praktyczny kurs](https://cs231n.github.io/optimization-1/), [analiza oryginalnej publikacji Cauchy'ego](https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf) (oryginał w języku francuskim).\n",
            "\n",
            "Pochodna jest dokładnie równa granicy funkcji. Dla małego $\\epsilon$ można ją przybliżyć jako:\n",
            "$$\\large\n",
            "\\frac{f(x)}{dx} \\approx \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}\n",
            "$$\n",
            "\n",
            "Przyglądając się temu równaniu widzimy, że: \n",
            "* dla funkcji rosnącej ($f(x+\\epsilon) > f(x)$) wyrażenie $\\frac{f(x)}{dx}$ będzie miało znak dodatni \n",
            "* dla funkcji malejącej ($f(x+\\epsilon) < f(x)$) wyrażenie $\\frac{f(x)}{dx}$ będzie miało znak ujemny \n",
            "\n",
            "Widzimy więc, że potrafimy wskazać kierunek zmniejszenia wartości funkcji, patrząc na znak pochodnej. Zaobserwowano także, że amplituda wartości w $\\frac{f(x)}{dx}$ jest tym większa, im dalej jesteśmy od minimum (maximum). Pochodna wyznacza więc, w jakim kierunku funkcja najszybciej rośnie, zaś przeciwny zwrot to ten, w którym funkcja najszybciej spada.\n",
            "\n",
            "Stosując powyższe do optymalizacji, mamy:\n",
            "$$\\large\n",
            "x_{t+1} = x_{t} -  \\alpha * \\frac{f(x)}{dx}\n",
            "$$\n",
            "\n",
            "$\\alpha$ to niewielka wartość (rzędu zwykle $10^{-5}$ - $10^{-2}$), wprowadzona, aby trzymać się założenia o małej zmianie parametrów ($\\epsilon$). Nazywa się ją **stałą uczącą (learning rate)** i jest zwykle najważniejszym hiperparametrem podczas nauki sieci.\n",
            "\n",
            "Metoda ta zakłada, że używamy całego zbioru danych do aktualizacji parametrów w każdym kroku, co nazywa się po prostu GD (od *gradient descent*) albo *full batch GD*. Wtedy każdy krok optymalizacji nazywa się **epoką (epoch)**.\n",
            "\n",
            "Im większa stała ucząca, tym większe nasze kroki podczas minimalizacji. Możemy więc uczyć szybciej, ale istnieje ryzyko, że będziemy \"przeskakiwać\" minima. Mniejsza stała ucząca to wolniejszy, ale dokładniejszy trening. Jednak nie zawsze ona pozwala osiągnąć lepsze wyniki, bo może okazać się, że utkniemy w minimum lokalnym. Można także zmieniać stałą uczącą podczas treningu, co nazywa się **learning rate scheduling (LR scheduling)**. Obrazowo:\n",
            "\n",
            "![learning_rate](http://www.bdhammel.com/assets/learning-rate/lr-types.png)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "496qEjkVLAso"
         },
         "source": [
            "![interactive LR](http://cdn-images-1.medium.com/max/640/1*eeIvlwkMNG1wSmj3FR6M2g.gif)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "RYkyAHKzLAsp"
         },
         "source": [
            "Policzmy więc pochodną dla naszej funkcji kosztu MSE. Pochodną liczymy po parametrach naszego modelu, bo to właśnie ich chcemy dopasować tak, żeby koszt był jak najmniejszy:\n",
            "\n",
            "$$\\large\n",
            "MSE = \\frac{1}{N} \\sum_{i}^{N} (y_i - \\hat{y_i})^2\n",
            "$$\n",
            "\n",
            "W powyższym wzorze tylko $y_i$ jest zależny od $a$ oraz $b$. Możemy wykorzystać tu regułę łańcuchową (*chain rule*) i policzyć pochodne po naszych parametrach w sposób następujący:\n",
            "\n",
            "$$\\large\n",
            "\\frac{\\text{d} MSE}{\\text{d} a} = \\frac{1}{N} \\sum_{i}^{N} \\frac{\\text{d} (y_i - \\hat{y_i})^2}{\\text{d} \\hat{y_i}} \\frac{\\text{d} \\hat{y_i}}{\\text{d} a}\n",
            "$$\n",
            "\n",
            "$$\\large\n",
            "\\frac{\\text{d} MSE}{\\text{d} b} = \\frac{1}{N} \\sum_{i}^{N} \\frac{\\text{d} (y_i - \\hat{y_i})^2}{\\text{d} \\hat{y_i}} \\frac{\\text{d} \\hat{y_i}}{\\text{d} b}\n",
            "$$\n",
            "\n",
            "Policzmy te pochodne po kolei:\n",
            "\n",
            "$$\\large\n",
            "\\frac{\\text{d} (y_i - \\hat{y_i})^2}{\\text{d} \\hat{y_i}} = -2 \\cdot (y_i - \\hat{y_i})\n",
            "$$\n",
            "\n",
            "$$\\large\n",
            "\\frac{\\text{d} \\hat{y_i}}{\\text{d} a} = x_i\n",
            "$$\n",
            "\n",
            "$$\\large\n",
            "\\frac{\\text{d} \\hat{y_i}}{\\text{d} b} = 1\n",
            "$$\n",
            "\n",
            "Łącząc powyższe wyniki dostaniemy:\n",
            "\n",
            "$$\\large\n",
            "\\frac{\\text{d} MSE}{\\text{d} a} = \\frac{-2}{N} \\sum_{i}^{N} (y_i - \\hat{y_i}) \\cdot {x_i}\n",
            "$$\n",
            "\n",
            "$$\\large\n",
            "\\frac{\\text{d} MSE}{\\text{d} b} = \\frac{-2}{N} \\sum_{i}^{N} (y_i - \\hat{y_i})\n",
            "$$\n",
            "\n",
            "Aktualizacja parametrów wygląda tak:\n",
            "\n",
            "$$\\large\n",
            "a' = a - \\alpha * \\left( \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i) \\cdot x_i \\right)\n",
            "$$\n",
            "$$\\large\n",
            "b' = b - \\alpha * \\left( \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i) \\right)\n",
            "$$\n",
            "\n",
            "Liczymy więc pochodną funkcji kosztu, a potem za pomocą reguły łańcuchowej \"cofamy się\", dochodząc do tego, jak każdy z parametrów wpływa na błąd i w jaki sposób powinniśmy go zmienić. Nazywa się to **propagacją wsteczną (backpropagation)** i jest podstawowym mechanizmem umożliwiającym naukę sieci neuronowych za pomocą spadku wzdłuż gradientu. Więcej możesz o tym przeczytać [tutaj](https://cs231n.github.io/optimization-2/)."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "\n",
            "#### Zadanie 2 (1.0 punkt)\n",
            "\n",
            "Zaimplementuj funkcję realizującą jedną epokę treningową. Zauważ, że `x` oraz `y` są wektorami. Oblicz predykcję przy aktualnych parametrach oraz zaktualizuj je zgodnie z powyższymi wzorami."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "4qbdWOSULAsp",
            "outputId": "055607ae-87aa-470a-e6da-25682c82d470"
         },
         "outputs": [],
         "source": [
            "def optimize(\n",
            "    x: np.ndarray, y: np.ndarray, a: float, b: float, learning_rate: float = 0.1\n",
            "):\n",
            "    y_hat = a * x + b\n",
            "    errors = y - y_hat\n",
            "    n = len(x)\n",
            "\n",
            "    a = a - learning_rate * ( -2 / n) * np.sum(errors * x)\n",
            "    b = b - learning_rate * (-2 / n) * np.sum(errors)\n",
            "\n",
            "    return (a, b)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "step 0 loss:  0.1330225119404028\n",
                  "step 100 loss:  0.012673197778527677\n",
                  "step 200 loss:  0.010257153540857817\n",
                  "step 300 loss:  0.0100948037549359\n",
                  "step 400 loss:  0.010083894412889118\n",
                  "step 500 loss:  0.010083161342973332\n",
                  "step 600 loss:  0.010083112083219709\n",
                  "step 700 loss:  0.010083108773135261\n",
                  "step 800 loss:  0.010083108550709076\n",
                  "step 900 loss:  0.01008310853576281\n",
                  "final loss: 0.010083108534760455\n"
               ]
            }
         ],
         "source": [
            "for i in range(1000):\n",
            "    loss = mse(y, a * x + b)\n",
            "    a, b = optimize(x, y, a, b)\n",
            "    if i % 100 == 0:\n",
            "        print(f\"step {i} loss: \", loss)\n",
            "\n",
            "print(\"final loss:\", loss)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/",
               "height": 282
            },
            "id": "xOgRcPC1LAsq",
            "outputId": "85b0b3e4-aa0d-467a-d8ff-5f01be17b243",
            "scrolled": false
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[<matplotlib.lines.Line2D at 0x219fe95e610>]"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 640x480 with 1 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "plt.scatter(x, y)\n",
            "plt.plot(x, a * x + b, color=\"g\", linewidth=4)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "vOr2fWYpLAsq"
         },
         "source": [
            "Udało ci się wytrenować swoją pierwszą sieć neuronową. Czemu? Otóż neuron to po prostu wektor parametrów, a zwykle robimy iloczyn skalarny tych parametrów z wejściem. Dodatkowo na wyjście nakłada się **funkcję aktywacji (activation function)**, która przekształca wyjście. Tutaj takiej nie było, a właściwie była to po prostu funkcja identyczności.\n",
            "\n",
            "Oczywiście w praktyce korzystamy z odpowiedniego frameworka, który w szczególności:\n",
            "- ułatwia budowanie sieci, np. ma gotowe klasy dla warstw neuronów\n",
            "- ma zaimplementowane funkcje kosztu oraz ich pochodne\n",
            "- sam różniczkuje ze względu na odpowiednie parametry i aktualizuje je odpowiednio podczas treningu\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "NJBYJabuLAsr"
         },
         "source": [
            "## Wprowadzenie do PyTorcha"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "EB-99XqhLAsr"
         },
         "source": [
            "PyTorch to w gruncie rzeczy narzędzie do algebry liniowej z [automatycznym rożniczkowaniem](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html), z możliwością przyspieszenia obliczeń z pomocą GPU. Na tych fundamentach zbudowany jest pełny framework do uczenia głębokiego. Można spotkać się ze stwierdzenie, że PyTorch to NumPy + GPU + opcjonalne różniczkowanie, co jest całkiem celne. Plus można łatwo debugować printem :)\n",
            "\n",
            "PyTorch używa dynamicznego grafu obliczeń, który sami definiujemy w kodzie. Takie podejście jest bardzo wygodne, elastyczne i pozwala na łatwe eksperymentowanie. Odbywa się to potencjalnie kosztem wydajności, ponieważ pozostawia kwestię optymalizacji programiście. Więcej na ten temat dla zainteresowanych na końcu laboratorium.\n",
            "\n",
            "Samo API PyTorcha bardzo przypomina Numpy'a, a podstawowym obiektem jest `Tensor`, klasa reprezentująca tensory dowolnego wymiaru. Dodatkowo niektóre tensory będą miały automatycznie obliczony gradient. Co ważne, tensor jest na pewnym urządzeniu, CPU lub GPU, a przenosić między nimi trzeba explicite.\n",
            "\n",
            "Najważniejsze moduły:\n",
            "- `torch` - podstawowe klasy oraz funkcje, np. `Tensor`, `from_numpy()`\n",
            "- `torch.nn` - klasy związane z sieciami neuronowymi, np. `Linear`, `Sigmoid`\n",
            "- `torch.optim` - wszystko związane z optymalizacją, głównie spadkiem wzdłuż gradientu"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {
            "id": "FwuIt8S-LAss"
         },
         "outputs": [],
         "source": [
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "bfCiUFXULAss",
            "outputId": "83f6231d-ecc4-461a-b758-fdc4bc2a88a4"
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([1.4154, 1.8855, 1.2668, 1.9454, 1.3595, 1.6473, 1.9778, 1.1298, 1.7165,\n",
                  "        1.4596])\n",
                  "tensor([0.4154, 0.8855, 0.2668, 0.9454, 0.3595, 0.6473, 0.9778, 0.1298, 0.7165,\n",
                  "        0.4596])\n",
                  "tensor(5.8036)\n"
               ]
            }
         ],
         "source": [
            "ones = torch.ones(10)\n",
            "noise = torch.ones(10) * torch.rand(10)\n",
            "\n",
            "# elementwise sum\n",
            "print(ones + noise)\n",
            "\n",
            "# elementwise multiplication\n",
            "print(ones * noise)\n",
            "\n",
            "# dot product\n",
            "print(ones @ noise)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "_x, _y = x, y"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {
            "id": "ynNd_kD0LAst"
         },
         "outputs": [],
         "source": [
            "# beware - shares memory with original Numpy array!\n",
            "# very fast, but modifications are visible to original variable\n",
            "x = torch.from_numpy(_x)\n",
            "y = torch.from_numpy(_y)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "W9kkxczELAsu"
         },
         "source": [
            "Jeżeli dla stworzonych przez nas tensorów chcemy śledzić operacje i obliczać gradient, to musimy oznaczyć `requires_grad=True`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "8HtZL-KfLAsu",
            "outputId": "47c6d930-5678-452a-95bc-227935138b40"
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(tensor([0.0464], requires_grad=True), tensor([0.8258], requires_grad=True))"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "a = torch.rand(1, requires_grad=True)\n",
            "b = torch.rand(1, requires_grad=True)\n",
            "a, b"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "Nl1guWZ_LAsv"
         },
         "source": [
            "PyTorch zawiera większość powszechnie używanych funkcji kosztu, np. MSE. Mogą być one używane na 2 sposoby, z czego pierwszy jest popularniejszy:\n",
            "- jako klasy wywoływalne z modułu `torch.nn`\n",
            "- jako funkcje z modułu `torch.nn.functional`\n",
            "\n",
            "Po wykonaniu poniższego kodu widzimy, że zwraca on nam tensor z dodatkowymi atrybutami. Co ważne, jest to skalar (0-wymiarowy tensor), bo potrzebujemy zwyczajnej liczby do obliczania propagacji wstecznych (pochodnych czątkowych)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "tensor(0.2003, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "mse = nn.MSELoss()\n",
            "mse(y, a * x + b)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "vS35r49nLAsw"
         },
         "source": [
            "Atrybutu `grad_fn` nie używamy wprost, bo korzysta z niego w środku PyTorch, ale widać, że tensor jest \"świadomy\", że liczy się na nim pochodną. Możemy natomiast skorzystać z atrybutu `grad`, który zawiera faktyczny gradient. Zanim go jednak dostaniemy, to trzeba powiedzieć PyTorchowi, żeby policzył gradient. Służy do tego metoda `.backward()`, wywoływana na obiekcie zwracanym przez funkcję kosztu."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {
            "id": "Qb7l6Xg1LAsx"
         },
         "outputs": [],
         "source": [
            "loss = mse(y, a * x + b)\n",
            "loss.backward()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "6LfQbLVoLAsx",
            "outputId": "d5b87fb7-d284-423c-f467-b677384b2f67"
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([0.1860])\n"
               ]
            }
         ],
         "source": [
            "print(a.grad)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "Kdf1iweELAsy"
         },
         "source": [
            "Ważne jest, że PyTorch nie liczy za każdym razem nowego gradientu, tylko dodaje go do istniejącego, czyli go akumuluje. Jest to przydatne w niektórych sieciach neuronowych, ale zazwyczaj trzeba go zerować. Jeżeli tego nie zrobimy, to dostaniemy coraz większe gradienty.\n",
            "\n",
            "Do zerowania służy metoda `.zero_()`. W PyTorchu wszystkie metody modyfikujące tensor w miejscu mają `_` na końcu nazwy. Jest to dość niskopoziomowa operacja dla pojedynczych tensorów - zobaczymy za chwilę, jak to robić łatwiej dla całej sieci."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "DiCQZKJsLAsy",
            "outputId": "2f779622-480d-43fc-b9d0-a0e36ff4b28b"
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "tensor([0.3720])"
                  ]
               },
               "execution_count": 17,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "loss = mse(y, a * x + b)\n",
            "loss.backward()\n",
            "a.grad"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "xNC3Ag8uLAsz"
         },
         "source": [
            "Zobaczmy, jak wyglądałaby regresja liniowa, ale napisana w PyTorchu. Jest to oczywiście bardzo niskopoziomowa implementacja - za chwilę zobaczymy, jak to wygląda w praktyce."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "AKnxyeboLAsz",
            "outputId": "2f939474-901a-4773-9704-686a40ae6e8e"
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "step 0 loss:  tensor(0.2003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 100 loss:  tensor(0.0162, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 200 loss:  tensor(0.0105, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 300 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 400 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 500 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 600 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 700 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 800 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "step 900 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
                  "final loss: tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
               ]
            }
         ],
         "source": [
            "learning_rate = 0.1\n",
            "for i in range(1000):\n",
            "    loss = mse(y, a * x + b)\n",
            "\n",
            "    # compute gradients\n",
            "    loss.backward()\n",
            "\n",
            "    # update parameters\n",
            "    a.data -= learning_rate * a.grad\n",
            "    b.data -= learning_rate * b.grad\n",
            "\n",
            "    # zero gradients\n",
            "    a.grad.data.zero_()\n",
            "    b.grad.data.zero_()\n",
            "\n",
            "    if i % 100 == 0:\n",
            "        print(f\"step {i} loss: \", loss)\n",
            "\n",
            "print(\"final loss:\", loss)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "2DXNVhshmmI-"
         },
         "source": [
            "Trening modeli w PyTorchu jest dosyć schematyczny i najczęściej rozdziela się go na kilka bloków, dających razem **pętlę uczącą (training loop)**, powtarzaną w każdej epoce:\n",
            "1. Forward pass - obliczenie predykcji sieci\n",
            "2. Loss calculation\n",
            "3. Backpropagation - obliczenie pochodnych oraz zerowanie gradientów\n",
            "4. Optimalization - aktualizacja wag\n",
            "5. Other - ewaluacja na zbiorze walidacyjnym, logging etc."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "2etpw7TNLAs0",
            "outputId": "8ac35c12-6c70-41ec-bf57-414456fc3c96",
            "scrolled": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "step 0 loss: 0.4044\n",
                  "step 100 loss: 0.0122\n",
                  "step 200 loss: 0.0102\n",
                  "step 300 loss: 0.0101\n",
                  "step 400 loss: 0.0101\n",
                  "step 500 loss: 0.0101\n",
                  "step 600 loss: 0.0101\n",
                  "step 700 loss: 0.0101\n",
                  "step 800 loss: 0.0101\n",
                  "step 900 loss: 0.0101\n",
                  "final loss: tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
               ]
            }
         ],
         "source": [
            "# initialization\n",
            "learning_rate = 0.1\n",
            "a = torch.rand(1, requires_grad=True)\n",
            "b = torch.rand(1, requires_grad=True)\n",
            "optimizer = torch.optim.SGD([a, b], lr=learning_rate)\n",
            "best_loss = float(\"inf\")\n",
            "\n",
            "# training loop in each epoch\n",
            "for i in range(1000):\n",
            "    # forward pass\n",
            "    y_hat = a * x + b\n",
            "\n",
            "    # loss calculation\n",
            "    loss = mse(y, y_hat)\n",
            "\n",
            "    # backpropagation\n",
            "    loss.backward()\n",
            "\n",
            "    # optimization\n",
            "    optimizer.step()\n",
            "    optimizer.zero_grad()  # zeroes all gradients - very convenient!\n",
            "\n",
            "    if i % 100 == 0:\n",
            "        if loss < best_loss:\n",
            "            best_model = (a.clone(), b.clone())\n",
            "            best_loss = loss\n",
            "        print(f\"step {i} loss: {loss.item():.4f}\")\n",
            "\n",
            "print(\"final loss:\", loss)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Przejdziemy teraz do budowy sieci neuronowej do klasyfikacji. Typowo implementuje się ją po prostu jako sieć dla regresji, ale zwracającą tyle wyników, ile mamy klas, a potem aplikuje się na tym funkcję sigmoidalną (2 klasy) lub softmax (>2 klasy). W przypadku klasyfikacji binarnej zwraca się czasem tylko 1 wartość, przepuszczaną przez sigmoidę - wtedy wyjście z sieci to prawdopodobieństwo klasy pozytywnej.\n",
            "\n",
            "Funkcją kosztu zwykle jest **entropia krzyżowa (cross-entropy)**, stosowana też w klasycznej regresji logistycznej. Co ważne, sieci neuronowe, nawet tak proste, uczą się szybciej i stabilniej, gdy dane na wejściu (a przynajmniej zmienne numeryczne) są **ustandaryzowane (standardized)**. Operacja ta polega na odjęciu średniej i podzieleniu przez odchylenie standardowe (tzw. *Z-score transformation*).\n",
            "\n",
            "**Uwaga - PyTorch wymaga tensora klas będącego liczbami zmiennoprzecinkowymi!**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Zbiór danych"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Na tym laboratorium wykorzystamy zbiór [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult). Dotyczy on przewidywania na podstawie danych demograficznych, czy dany człowiek zarabia powyżej 50 tysięcy dolarów rocznie, czy też mniej. Jest to cenna informacja np. przy planowaniu kampanii marketingowych. Jak możesz się domyślić, zbiór pochodzi z czasów, kiedy inflacja była dużo niższa :)\n",
            "\n",
            "Poniżej znajduje się kod do ściągnięcia i preprocessingu zbioru. Nie musisz go dokładnie analizować."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "4DNsaZAnLAs0",
            "outputId": "70822008-530d-4173-deb9-8149a9fe5b41",
            "scrolled": true
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "--2023-11-25 22:56:18--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
                  "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
                  "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
                  "HTTP request sent, awaiting response... 200 OK\n",
                  "Length: unspecified\n",
                  "Saving to: 'adult.data.1'\n",
                  "\n",
                  "     0K .......... .......... .......... .......... ..........  145K\n",
                  "    50K .......... .......... .......... .......... ..........  546K\n",
                  "   100K .......... .......... .......... .......... ..........  591K\n",
                  "   150K .......... .......... .......... .......... .......... 15,5M\n",
                  "   200K .......... .......... .......... .......... ..........  293K\n",
                  "   250K .......... .......... .......... .......... .......... 16,8M\n",
                  "   300K .......... .......... .......... .......... ..........  598K\n",
                  "   350K .......... .......... .......... .......... .......... 15,2M\n",
                  "   400K .......... .......... .......... .......... .......... 22,5M\n",
                  "   450K .......... .......... .......... .......... ..........  611K\n",
                  "   500K .......... .......... .......... .......... .......... 19,3M\n",
                  "   550K .......... .......... .......... .......... .......... 24,4M\n",
                  "   600K .......... .......... .......... .......... .......... 21,9M\n",
                  "   650K .......... .......... .......... .......... .......... 1,95M\n",
                  "   700K .......... .......... .......... .......... ..........  868K\n",
                  "   750K .......... .......... .......... .......... .......... 29,0M\n",
                  "   800K .......... .......... .......... .......... .......... 37,4M\n",
                  "   850K .......... .......... .......... .......... .......... 37,5M\n",
                  "   900K .......... .......... .......... .......... .......... 26,5M\n",
                  "   950K .......... .......... .......... .......... ..........  659K\n",
                  "  1000K .......... .......... .......... .......... .......... 8,19M\n",
                  "  1050K .......... .......... .......... .......... .......... 25,5M\n",
                  "  1100K .......... .......... .......... .......... .......... 54,7M\n",
                  "  1150K .......... .......... .......... .......... .......... 35,5M\n",
                  "  1200K .......... .......... .......... .......... .......... 26,8M\n",
                  "  1250K .......... .......... .......... .......... .......... 37,7M\n",
                  "  1300K .......... .......... .......... .......... .......... 55,2M\n",
                  "  1350K .......... .......... .......... .......... .......... 2,67M\n",
                  "  1400K .......... .......... .......... .......... ..........  895K\n",
                  "  1450K .......... .......... .......... .......... .......... 6,53M\n",
                  "  1500K .......... .......... .......... .......... .......... 24,1M\n",
                  "  1550K .......... .......... .......... .......... .......... 21,7M\n",
                  "  1600K .......... .......... .......... .......... .......... 38,6M\n",
                  "  1650K .......... .......... .......... .......... .......... 16,0M\n",
                  "  1700K .......... .......... .......... .......... .......... 36,7M\n",
                  "  1750K .......... .......... .......... .......... .......... 27,6M\n",
                  "  1800K .......... .......... .......... .......... .......... 36,8M\n",
                  "  1850K .......... .......... .......... .......... .......... 39,6M\n",
                  "  1900K .......... .......... .......... .......... .......... 35,8M\n",
                  "  1950K .......... .......... .......... .......... ..........  818K\n",
                  "  2000K .......... .......... .......... .......... .......... 10,8M\n",
                  "  2050K .......... .......... .......... .......... .......... 23,9M\n",
                  "  2100K .......... .......... .......... .......... .......... 12,9M\n",
                  "  2150K .......... .......... .......... .......... .......... 46,5M\n",
                  "  2200K .......... .......... .......... .......... .......... 26,3M\n",
                  "  2250K .......... .......... .......... .......... .......... 72,5M\n",
                  "  2300K .......... .......... .......... .......... .......... 29,6M\n",
                  "  2350K .......... .......... .......... .......... .......... 15,0M\n",
                  "  2400K .......... .......... .......... .......... .......... 34,2M\n",
                  "  2450K .......... .......... .......... .......... .......... 23,0M\n",
                  "  2500K .......... .......... .......... .......... .......... 37,1M\n",
                  "  2550K .......... .......... .......... .......... .......... 30,9M\n",
                  "  2600K .......... .......... .......... .......... .......... 32,6M\n",
                  "  2650K .......... .......... .......... .......... .......... 41,2M\n",
                  "  2700K .......... .......... .......... .......... .......... 39,2M\n",
                  "  2750K .......... .......... .......... .......... .......... 19,3M\n",
                  "  2800K .......... .......... .......... .......... .......... 11,2M\n",
                  "  2850K .......... .......... .......... .......... ..........  928K\n",
                  "  2900K .......... .......... .......... .......... .......... 23,7M\n",
                  "  2950K .......... .......... .......... .......... .......... 8,21M\n",
                  "  3000K .......... .......... .......... .......... .......... 22,2M\n",
                  "  3050K .......... .......... .......... .......... .......... 17,6M\n",
                  "  3100K .......... .......... .......... .......... .......... 81,1M\n",
                  "  3150K .......... .......... .......... .......... .......... 42,3M\n",
                  "  3200K .......... .......... .......... .......... .......... 30,6M\n",
                  "  3250K .......... .......... .......... .......... .......... 43,5M\n",
                  "  3300K .......... .......... .......... .......... .......... 31,9M\n",
                  "  3350K .......... .......... .......... .......... .......... 20,7M\n",
                  "  3400K .......... .......... .......... .......... .......... 39,7M\n",
                  "  3450K .......... .......... .......... .......... .......... 42,5M\n",
                  "  3500K .......... .......... .......... .......... .......... 15,1M\n",
                  "  3550K .......... .......... .......... .......... .......... 20,4M\n",
                  "  3600K .......... .......... .......... .......... .......... 38,2M\n",
                  "  3650K .......... .......... .......... .......... .......... 21,7M\n",
                  "  3700K .......... .......... .......... .......... .......... 38,2M\n",
                  "  3750K .......... .......... .......... .......... .......... 60,6M\n",
                  "  3800K .......... .......... .......... .......... .......... 62,2M\n",
                  "  3850K .......... .......... .......... .                     76,8M=1,3s\n",
                  "\n",
                  "2023-11-25 22:56:21 (2,83 MB/s) - 'adult.data.1' saved [3974305]\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "array([' <=50K', ' >50K'], dtype=object)"
                  ]
               },
               "execution_count": 21,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import pandas as pd\n",
            "\n",
            "\n",
            "columns = [\n",
            "    \"age\",\n",
            "    \"workclass\",\n",
            "    \"fnlwgt\",\n",
            "    \"education\",\n",
            "    \"education-num\",\n",
            "    \"marital-status\",\n",
            "    \"occupation\",\n",
            "    \"relationship\",\n",
            "    \"race\",\n",
            "    \"sex\",\n",
            "    \"capital-gain\",\n",
            "    \"capital-loss\",\n",
            "    \"hours-per-week\",\n",
            "    \"native-country\",\n",
            "    \"wage\"\n",
            "]\n",
            "\n",
            "\"\"\"\n",
            "age: continuous.\n",
            "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
            "fnlwgt: continuous.\n",
            "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
            "education-num: continuous.\n",
            "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
            "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
            "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
            "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
            "sex: Female, Male.\n",
            "capital-gain: continuous.\n",
            "capital-loss: continuous.f\n",
            "hours-per-week: continuous.\n",
            "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
            "\"\"\"\n",
            "\n",
            "df = pd.read_csv(\"adult.data\", header=None, names=columns)\n",
            "df.wage.unique()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [],
         "source": [
            "# attribution: https://www.kaggle.com/code/royshih23/topic7-classification-in-python\n",
            "df['education'].replace('Preschool', 'dropout',inplace=True)\n",
            "df['education'].replace('10th', 'dropout',inplace=True)\n",
            "df['education'].replace('11th', 'dropout',inplace=True)\n",
            "df['education'].replace('12th', 'dropout',inplace=True)\n",
            "df['education'].replace('1st-4th', 'dropout',inplace=True)\n",
            "df['education'].replace('5th-6th', 'dropout',inplace=True)\n",
            "df['education'].replace('7th-8th', 'dropout',inplace=True)\n",
            "df['education'].replace('9th', 'dropout',inplace=True)\n",
            "df['education'].replace('HS-Grad', 'HighGrad',inplace=True)\n",
            "df['education'].replace('HS-grad', 'HighGrad',inplace=True)\n",
            "df['education'].replace('Some-college', 'CommunityCollege',inplace=True)\n",
            "df['education'].replace('Assoc-acdm', 'CommunityCollege',inplace=True)\n",
            "df['education'].replace('Assoc-voc', 'CommunityCollege',inplace=True)\n",
            "df['education'].replace('Bachelors', 'Bachelors',inplace=True)\n",
            "df['education'].replace('Masters', 'Masters',inplace=True)\n",
            "df['education'].replace('Prof-school', 'Masters',inplace=True)\n",
            "df['education'].replace('Doctorate', 'Doctorate',inplace=True)\n",
            "\n",
            "df['marital-status'].replace('Never-married', 'NotMarried',inplace=True)\n",
            "df['marital-status'].replace(['Married-AF-spouse'], 'Married',inplace=True)\n",
            "df['marital-status'].replace(['Married-civ-spouse'], 'Married',inplace=True)\n",
            "df['marital-status'].replace(['Married-spouse-absent'], 'NotMarried',inplace=True)\n",
            "df['marital-status'].replace(['Separated'], 'Separated',inplace=True)\n",
            "df['marital-status'].replace(['Divorced'], 'Separated',inplace=True)\n",
            "df['marital-status'].replace(['Widowed'], 'Widowed',inplace=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "LiOxs_6mLAs1",
            "outputId": "c95418cf-2632-41d0-de0a-9caf109de113",
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
            "\n",
            "\n",
            "X = df.copy()\n",
            "y = (X.pop(\"wage\") == ' >50K').astype(int).values\n",
            "\n",
            "train_valid_size = 0.2\n",
            "\n",
            "X_train, X_test, y_train, y_test = train_test_split(\n",
            "    X, y, \n",
            "    test_size=train_valid_size, \n",
            "    random_state=0, \n",
            "    shuffle=True, \n",
            "    stratify=y\n",
            ")\n",
            "X_train, X_valid, y_train, y_valid = train_test_split(\n",
            "    X_train, y_train, \n",
            "    test_size=train_valid_size, \n",
            "    random_state=0, \n",
            "    shuffle=True, \n",
            "    stratify=y_train\n",
            ")\n",
            "\n",
            "continuous_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
            "continuous_X_train = X_train[continuous_cols]\n",
            "categorical_X_train = X_train.loc[:, ~X_train.columns.isin(continuous_cols)]\n",
            "\n",
            "continuous_X_valid = X_valid[continuous_cols]\n",
            "categorical_X_valid = X_valid.loc[:, ~X_valid.columns.isin(continuous_cols)]\n",
            "\n",
            "continuous_X_test = X_test[continuous_cols]\n",
            "categorical_X_test = X_test.loc[:, ~X_test.columns.isin(continuous_cols)]\n",
            "\n",
            "categorical_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
            "continuous_scaler = StandardScaler() #MinMaxScaler(feature_range=(-1, 1))\n",
            "\n",
            "categorical_encoder.fit(categorical_X_train)\n",
            "continuous_scaler.fit(continuous_X_train)\n",
            "\n",
            "continuous_X_train = continuous_scaler.transform(continuous_X_train)\n",
            "continuous_X_valid = continuous_scaler.transform(continuous_X_valid)\n",
            "continuous_X_test = continuous_scaler.transform(continuous_X_test)\n",
            "\n",
            "categorical_X_train = categorical_encoder.transform(categorical_X_train)\n",
            "categorical_X_valid = categorical_encoder.transform(categorical_X_valid)\n",
            "categorical_X_test = categorical_encoder.transform(categorical_X_test)\n",
            "\n",
            "X_train = np.concatenate([continuous_X_train, categorical_X_train], axis=1)\n",
            "X_valid = np.concatenate([continuous_X_valid, categorical_X_valid], axis=1)\n",
            "X_test = np.concatenate([continuous_X_test, categorical_X_test], axis=1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 171,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(torch.Size([20838, 108]), torch.Size([20838, 1]))"
                  ]
               },
               "execution_count": 171,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "X_train.shape, y_train.shape"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Uwaga co do typów - PyTorchu wszystko w sieci neuronowej musi być typu `float32`. W szczególności trzeba uważać na konwersje z Numpy'a, który używa domyślnie typu `float64`. Może ci się przydać metoda `.float()`.\n",
            "\n",
            "Uwaga co do kształtów wyjścia - wejścia do `nn.BCELoss` muszą być tego samego kształtu. Może ci się przydać metoda `.squeeze()` lub `.unsqueeze()`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {
            "id": "qfRA3xEoLAs1"
         },
         "outputs": [],
         "source": [
            "X_train = torch.from_numpy(X_train).float()\n",
            "y_train = torch.from_numpy(y_train).float().unsqueeze(-1)\n",
            "\n",
            "X_valid = torch.from_numpy(X_valid).float()\n",
            "y_valid = torch.from_numpy(y_valid).float().unsqueeze(-1)\n",
            "\n",
            "X_test = torch.from_numpy(X_test).float()\n",
            "y_test = torch.from_numpy(y_test).float().unsqueeze(-1)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Podobnie jak w laboratorium 2, mamy tu do czynienia z klasyfikacją niezbalansowaną:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 640x480 with 1 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "y_pos_perc = 100 * y_train.sum().item() / len(y_train)\n",
            "y_neg_perc = 100 - y_pos_perc\n",
            "\n",
            "plt.title(\"Class percentages\")\n",
            "plt.bar([\"<50k\", \">=50k\"], [y_neg_perc, y_pos_perc])\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "W związku z powyższym będziemy używać odpowiednich metryk, czyli AUROC, precyzji i czułości."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "XLexWff-LAs0"
         },
         "source": [
            "#### Zadanie 3 (1.0 punkt)\n",
            "\n",
            "Zaimplementuj regresję logistyczną dla tego zbioru danych, używając PyTorcha. Dane wejściowe zostały dla ciebie przygotowane w komórkach poniżej.\n",
            "\n",
            "Sama sieć składa się z 2 elementów:\n",
            "- warstwa liniowa `nn.Linear`, przekształcająca wektor wejściowy na 1 wyjście - logit\n",
            "- aktywacja sigmoidalna `nn.Sigmoid`, przekształcająca logit na prawdopodobieństwo klasy pozytywnej\n",
            "\n",
            "Użyj binarnej entropii krzyżowej `nn.BCELoss` jako funkcji kosztu. Użyj optymalizatora SGD ze stałą uczącą `1e-3`. Trenuj przez 3000 epok. Pamiętaj, aby przekazać do optymalizatora `torch.optim.SGD` parametry sieci (metoda `.parameters()`). Dopisz logowanie kosztu raz na 100 epok."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 174,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "NbABKz5-LAs2",
            "outputId": "086dc0f3-0184-4072-9fd3-275b60dee2e4",
            "scrolled": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 100/3000\tLoss: 0.6276997923851013\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 200/3000\tLoss: 0.6031697392463684\n",
                  "Epoch: 300/3000\tLoss: 0.5826571583747864\n",
                  "Epoch: 400/3000\tLoss: 0.5653572082519531\n",
                  "Epoch: 500/3000\tLoss: 0.5506343245506287\n",
                  "Epoch: 600/3000\tLoss: 0.5379889011383057\n",
                  "Epoch: 700/3000\tLoss: 0.5270274877548218\n",
                  "Epoch: 800/3000\tLoss: 0.5174400806427002\n",
                  "Epoch: 900/3000\tLoss: 0.508981466293335\n",
                  "Epoch: 1000/3000\tLoss: 0.5014569759368896\n",
                  "Epoch: 1100/3000\tLoss: 0.4947112500667572\n",
                  "Epoch: 1200/3000\tLoss: 0.48861974477767944\n",
                  "Epoch: 1300/3000\tLoss: 0.483082115650177\n",
                  "Epoch: 1400/3000\tLoss: 0.47801676392555237\n",
                  "Epoch: 1500/3000\tLoss: 0.4733572006225586\n",
                  "Epoch: 1600/3000\tLoss: 0.4690491259098053\n",
                  "Epoch: 1700/3000\tLoss: 0.46504727005958557\n",
                  "Epoch: 1800/3000\tLoss: 0.46131429076194763\n",
                  "Epoch: 1900/3000\tLoss: 0.4578188955783844\n",
                  "Epoch: 2000/3000\tLoss: 0.4545348584651947\n",
                  "Epoch: 2100/3000\tLoss: 0.45143988728523254\n",
                  "Epoch: 2200/3000\tLoss: 0.4485150873661041\n",
                  "Epoch: 2300/3000\tLoss: 0.44574421644210815\n",
                  "Epoch: 2400/3000\tLoss: 0.4431132972240448\n",
                  "Epoch: 2500/3000\tLoss: 0.44061020016670227\n",
                  "Epoch: 2600/3000\tLoss: 0.4382243752479553\n",
                  "Epoch: 2700/3000\tLoss: 0.43594664335250854\n",
                  "Epoch: 2800/3000\tLoss: 0.43376874923706055\n",
                  "Epoch: 2900/3000\tLoss: 0.4316834509372711\n",
                  "Epoch: 3000/3000\tLoss: 0.4296844005584717\n",
                  "Best loss: 0.4296844005584717\n"
               ]
            }
         ],
         "source": [
            "learning_rate = 1e-3\n",
            "\n",
            "input_size = X_train.shape[1]\n",
            "\n",
            "model = nn.Linear(input_size, 1)\n",
            "activation = nn.Sigmoid()\n",
            "optimizer = optim.SGD(lr=learning_rate, params=model.parameters())\n",
            "loss_fn = nn.BCELoss()\n",
            "\n",
            "epochs = 3000\n",
            "\n",
            "best_loss = float('inf')\n",
            "\n",
            "model.train()\n",
            "\n",
            "for epoch in range(1, epochs + 1):\n",
            "    outputs = activation(model(X_train))\n",
            "    loss = loss_fn(outputs, y_train)\n",
            "    \n",
            "    optimizer.zero_grad()\n",
            "    loss.backward()\n",
            "    optimizer.step()\n",
            "\n",
            "    if best_loss > loss:\n",
            "        best_loss = loss\n",
            "\n",
            "    if epoch % 100 == 0:\n",
            "        print(f\"Epoch: {epoch}/{epochs}\\tLoss: {loss.item()}\")\n",
            "\n",
            "print(f\"Best loss: {best_loss.item()}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Teraz trzeba sprawdzić, jak poszło naszej sieci. W PyTorchu sieć pracuje zawsze w jednym z dwóch trybów: treningowym lub ewaluacyjnym (predykcyjnym). Ten drugi wyłącza niektóre mechanizmy, które są używane tylko podczas treningu, w szczególności regularyzację dropout. Do przełączania służą metody modelu `.train()` i `.eval()`.\n",
            "\n",
            "Dodatkowo podczas liczenia predykcji dobrze jest wyłączyć liczenie gradientów, bo nie będą potrzebne, a oszczędza to czas i pamięć. Używa się do tego menadżera kontekstu `with torch.no_grad():`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 175,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "zH37zDX4LAs2",
            "outputId": "b1f93309-6f04-4ffc-b0ca-08d0a32120a0",
            "scrolled": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "AUROC: 86.04%\n"
               ]
            }
         ],
         "source": [
            "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support, roc_auc_score\n",
            "\n",
            "model.eval()\n",
            "with torch.no_grad():\n",
            "    y_score = activation(model(X_test))\n",
            "\n",
            "auroc = roc_auc_score(y_test, y_score)\n",
            "print(f\"AUROC: {100 * auroc:.2f}%\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Jest to całkiem dobry wynik, a może być jeszcze lepszy. Sprawdźmy dla pewności jeszcze inne metryki: precyzję, recall oraz F1-score. Dodatkowo narysujemy krzywą precision-recall, czyli jak zmieniają się te metryki w zależności od przyjętego progu (threshold) prawdopodobieństwa, powyżej którego przyjmujemy klasę pozytywną. Taką krzywą należy rysować na zbiorze walidacyjnym, bo później chcemy wykorzystać tę informację do doboru progu, a nie chcemy mieć wycieku danych testowych (data leakage).\n",
            "\n",
            "Poniżej zaimplementowano także funkcję `get_optimal_threshold()`, która sprawdza, dla którego progu uzyskujemy maksymalny F1-score, i zwraca indeks oraz wartość optymalnego progu. Przyda ci się ona w dalszej części laboratorium."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 57,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.metrics import PrecisionRecallDisplay\n",
            "\n",
            "\n",
            "def get_optimal_threshold(\n",
            "    precisions: np.array, \n",
            "    recalls: np.array, \n",
            "    thresholds: np.array\n",
            ") -> Tuple[int, float]:\n",
            "    \n",
            "    numerator = 2 * precisions * recalls\n",
            "    denominator = precisions + recalls\n",
            "    f1_scores = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator != 0)\n",
            "    \n",
            "    optimal_idx = np.argmax(f1_scores)\n",
            "    optimal_threshold = thresholds[optimal_idx]\n",
            "    \n",
            "    return optimal_idx, optimal_threshold\n",
            "\n",
            "\n",
            "def plot_precision_recall_curve(y_true, y_pred_score) -> None:\n",
            "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_score)\n",
            "    optimal_idx, optimal_threshold = get_optimal_threshold(precisions, recalls, thresholds)\n",
            "\n",
            "    disp = PrecisionRecallDisplay(precisions, recalls)\n",
            "    disp.plot()\n",
            "    plt.title(f\"Precision-recall curve (opt. thresh.: {optimal_threshold:.4f})\")\n",
            "    plt.axvline(recalls[optimal_idx], color=\"green\", linestyle=\"-.\")\n",
            "    plt.axhline(precisions[optimal_idx], color=\"green\", linestyle=\"-.\")\n",
            "    plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 58,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 640x480 with 1 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "model.eval()\n",
            "with torch.no_grad():\n",
            "    y_pred_valid_score = activation(model(X_valid))\n",
            "\n",
            "plot_precision_recall_curve(y_valid, y_pred_valid_score)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "vfQPIUQ_LAs2"
         },
         "source": [
            "Jak widać, chociaż AUROC jest wysokie, to dla optymalnego F1-score recall nie jest zbyt wysoki, a precyzja jest już dość niska. Być może wynik uda się poprawić, używając modelu o większej pojemności - pełnej, głębokiej sieci neuronowej."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Sieci neuronowe"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "YP298w6Cq7T6"
         },
         "source": [
            "Wszystko zaczęło się od inspirowanych biologią [sztucznych neuronów](https://en.wikipedia.org/wiki/Artificial_neuron), których próbowano użyć do symulacji mózgu. Naukowcy szybko odeszli od tego podejścia (sam problem modelowania okazał się też znacznie trudniejszy, niż sądzono), zamiast tego używając neuronów jako jednostek reprezentującą dowolną funkcję parametryczną $f(x, \\Theta)$. Każdy neuron jest zatem bardzo elastyczny, bo jedyne wymagania to funkcja różniczkowalna, a mamy do tego wektor parametrów $\\Theta$.\n",
            "\n",
            "W praktyce najczęściej można spotkać się z kilkoma rodzinami sieci neuronowych:\n",
            "1. Perceptrony wielowarstwowe (*MultiLayer Perceptron*, MLP) - najbardziej podobne do powyższego opisu, niezbędne do klasyfikacji i regresji\n",
            "2. Konwolucyjne (*Convolutional Neural Networks*, CNNs) - do przetwarzania danych z zależnościami przestrzennymi, np. obrazów czy dźwięku\n",
            "3. Rekurencyjne (*Recurrent Neural Networks*, RNNs) - do przetwarzania danych z zależnościami sekwencyjnymi, np. szeregi czasowe, oraz kiedyś do języka naturalnego\n",
            "4. Transformacyjne (*Transformers*), oparte o mechanizm atencji (*attention*) - do przetwarzania języka naturalnego (NLP), z którego wyparły RNNs, a coraz częściej także do wszelkich innych danych, np. obrazów, dźwięku\n",
            "5. Grafowe (*Graph Neural Networks*, GNNS) - do przetwarzania grafów\n",
            "\n",
            "Na tym laboratorium skupimy się na najprostszej architekturze, czyli MLP. Jest ona powszechnie łączona z wszelkimi innymi architekturami, bo pozwala dokonywać klasyfikacji i regresji. Przykładowo, klasyfikacja obrazów to zwykle CNN + MLP, klasyfikacja tekstów to transformer + MLP, a regresja na grafach to GNN + MLP.\n",
            "\n",
            "Dodatkowo, pomimo prostoty MLP są bardzo potężne - udowodniono, że perceptrony (ich powszechna nazwa) są [uniwersalnym aproksymatorem](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208), będącym w stanie przybliżyć dowolną funkcję z odpowiednio małym błędem, zakładając wystarczającą wielkość warstw sieci. Szczególne ich wersje potrafią nawet [reprezentować drzewa decyzyjne](https://www.youtube.com/watch?v=_okxGdHM5b8).\n",
            "\n",
            "Dla zainteresowanych polecamy [doskonałą książkę \"Dive into Deep Learning\", z implementacjami w PyTorchu](https://d2l.ai/chapter_multilayer-perceptrons/index.html), [klasyczną książkę \"Deep Learning Book\"](https://www.deeplearningbook.org/contents/mlp.html), oraz [ten filmik](https://www.youtube.com/watch?v=BFHrIxKcLjA), jeśli zastanawiałeś/-aś się, czemu używamy deep learning, a nie naprzykład (wide?) learning. (aka. czemu staramy się budować głębokie sieci, a nie płytkie za to szerokie)"
         ]
      },
      {
         "attachments": {
            "1_x-3NGQv0pRIab8xDT-f_Hg.png": {
               "image/png": "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"
            }
         },
         "cell_type": "markdown",
         "metadata": {
            "id": "S_ZjoGBU5upj"
         },
         "source": [
            "### Sieci MLP\n",
            "\n",
            "Dla przypomnienia, na wejściu mamy punkty ze zbioru treningowego, czyli $d$-wymiarowe wektory. W klasyfikacji chcemy znaleźć granicę decyzyjną, czyli krzywą, która oddzieli od siebie klasy. W wejściowej przestrzeni może być to trudne, bo chmury punktów z poszczególnych klas mogą być ze sobą dość pomieszane. Pamiętajmy też, że regresja logistyczna jest klasyfikatorem liniowym, czyli w danej przestrzeni potrafi oddzielić punkty tylko linią prostą.\n",
            "\n",
            "Sieć MLP składa się z warstw. Każda z nich dokonuje nieliniowego przekształcenia przestrzeni (można o tym myśleć jak o składaniu przestrzeni jakąś prostą/łamaną), tak, aby w finalnej przestrzeni nasze punkty były możliwie liniowo separowalne. Wtedy ostatnia warstwa z sigmoidą będzie potrafiła je rozdzielić od siebie.\n",
            "\n",
            "![1_x-3NGQv0pRIab8xDT-f_Hg.png](attachment:1_x-3NGQv0pRIab8xDT-f_Hg.png)\n",
            "\n",
            "Poszczególne neurony składają się z iloczynu skalarnego wejść z wagami neuronu, oraz nieliniowej funkcji aktywacji. W PyTorchu są to osobne obiekty - `nn.Linear` oraz np. `nn.Sigmoid`. Funkcja aktywacji przyjmuje wynik iloczynu skalarnego i przekształca go, aby sprawdzić, jak mocno reaguje neuron na dane wejście. Musi być nieliniowa z dwóch powodów. Po pierwsze, tylko nieliniowe przekształcenia są na tyle potężne, żeby umożliwić liniową separację danych w ostatniej warstwie. Po drugie, liniowe przekształcenia zwyczajnie nie działają. Aby zrozumieć czemu, trzeba zobaczyć, co matematycznie oznacza sieć MLP.\n",
            "\n",
            "![perceptron](https://www.saedsayad.com/images/Perceptron_bkp_1.png)\n",
            "\n",
            "Zapisane matematycznie MLP to:\n",
            "\n",
            "$\\large\n",
            "h_1 = f_1(x) \\\\\n",
            "h_2 = f_2(h_1) \\\\\n",
            "h_3 = f_3(h_2) \\\\\n",
            "... \\\\\n",
            "h_n = f_n(h_{n-1})\n",
            "$\n",
            "\n",
            "gdzie $x$ to wejście $f_i$ to funkcja aktywacji $i$-tej warstwy, a $h_i$ to wyjście $i$-tej warstwy, nazywane **ukrytą reprezentacją (hidden representation)**, lub *latent representation*. Nazwa bierze się z tego, że w środku sieci wyciągamy cechy i wzorce w danych, które nie są widoczne na pierwszy rzut oka na wejściu.\n",
            "\n",
            "Załóżmy, że uczymy się na danych $x$ o jednym wymiarze (dla uproszczenia wzorów) oraz nie mamy funkcji aktywacji, czyli wykorzystujemy tak naprawdę aktywację liniową $f(x) = x$. Zobaczmy jak będą wyglądać dane przechodząc przez kolejne warstwy:\n",
            "\n",
            "$\\large\n",
            "h_1 = f_1(xw_1) = xw_1 \\\\\n",
            "h_2 = f_2(h_1w_2) = xw_1w_2 \\\\\n",
            "... \\\\\n",
            "h_n = f_n(h_{n-1}w_n) = xw_1w_2...w_n\n",
            "$\n",
            "\n",
            "gdzie $w_i$ to jest parametr $i$-tej warstwy sieci, $x$ to są dane (w naszym przypadku jedna liczba) wejściowa, a $h_i$ to wyjście $i$-tej warstwy.\n",
            "\n",
            "Jak widać, taka sieć o $n$ warstwach jest równoważna sieci o jednej warstwie z parametrem $w = w_1w_2...w_n$. Wynika to z tego, że złożenie funkcji liniowych jest także funkcją liniową - patrz notatki z algebry :)\n",
            "\n",
            "Jeżeli natomiast użyjemy nieliniowej funkcji aktywacji, często oznaczanej jako $\\sigma$, to wszystko będzie działać. Co ważne, ostatnia warstwa, dająca wyjście sieci, ma zwykle inną aktywację od warstw wewnątrz sieci, bo też ma inne zadanie - zwrócić wartość dla klasyfikacji lub regresji. Na wyjściu korzysta się z funkcji liniowej (regresja), sigmoidalnej (klasyfikacja binarna) lub softmax (klasyfikacja wieloklasowa).\n",
            "\n",
            "Wewnątrz sieci używano kiedyś sigmoidy oraz tangensa hiperbolicznego `tanh`, ale okazało się to nieefektywne przy uczeniu głębokich sieci o wielu warstwach. Nowoczesne sieci korzystają zwykle z funkcji ReLU (*rectified linear unit*), która jest zaskakująco prosta: $ReLU(x) = \\max(0, x)$. Okazało się, że bardzo dobrze nadaje się do treningu nawet bardzo głębokich sieci neuronowych. Nowsze funkcje aktywacji są głównie modyfikacjami ReLU.\n",
            "\n",
            "![relu](https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### MLP w PyTorchu\n",
            "\n",
            "Warstwę neuronów w MLP nazywa się warstwą gęstą (*dense layer*) lub warstwą w pełni połączoną (*fully-connected layer*), i taki opis oznacza zwykle same neurony oraz funkcję aktywacji. PyTorch, jak już widzieliśmy, definiuje osobno transformację liniową oraz aktywację, a więc jedna warstwa składa się de facto z 2 obiektów, wywoływanych jeden po drugim. Inne frameworki, szczególnie wysokopoziomowe (np. Keras) łączą to często w jeden obiekt.\n",
            "\n",
            "MLP składa się zatem z sekwencji obiektów, które potem wywołuje się jeden po drugim, gdzie wyjście poprzedniego to wejście kolejnego. Ale nie można tutaj używać Pythonowych list! Z perspektywy PyTorcha to wtedy niezależne obiekty i nie zostanie wtedy przekazany między nimi gradient. Trzeba tutaj skorzystać z `nn.Sequential`, aby tworzyć taki pipeline.\n",
            "\n",
            "Rozmiary wejścia i wyjścia dla każdej warstwy trzeba w PyTorchu podawać explicite. Jest to po pierwsze edukacyjne, a po drugie często ułatwia wnioskowanie o działaniu sieci oraz jej debugowanie - mamy jasno podane, czego oczekujemy. Niektóre frameworki (np. Keras) obliczają to automatycznie.\n",
            "\n",
            "Co ważne, ostatnia warstwa zwykle nie ma funkcji aktywacji. Wynika to z tego, że obliczanie wielu funkcji kosztu (np. entropii krzyżowej) na aktywacjach jest często niestabilne numerycznie. Z tego powodu PyTorch oferuje funkcje kosztu zawierające w środku aktywację dla ostatniej warstwy, a ich implementacje są stabilne numerycznie. Przykładowo, `nn.BCELoss` przyjmuje wejście z zaaplikowanymi już aktywacjami, ale może skutkować under/overflow, natomiast `nn.BCEWithLogitsLoss` przyjmuje wejście bez aktywacji, a w środku ma specjalną implementację łączącą binarną entropię krzyżową z aktywacją sigmoidalną. Oczywiście w związku z tym aby dokonać potem predykcji w praktyce, trzeba pamiętać o użyciu funkcji aktywacji. Często korzysta się przy tym z funkcji z modułu `torch.nn.functional`, które są w tym wypadku nieco wygodniejsze od klas wywoływalnych z `torch.nn`.\n",
            "\n",
            "Całe sieci w PyTorchu tworzy się jako klasy dziedziczące po `nn.Module`. Co ważne, obiekty, z których tworzymy sieć, np. `nn.Linear`, także dziedziczą po tej klasie. Pozwala to na bardzo modułową budowę kodu, zgodną z zasadami OOP. W konstruktorze najpierw trzeba zawsze wywołać konstruktor rodzica - `super().__init__()`, a później tworzy się potrzebne obiekty i zapisuje jako atrybuty. Każdy atrybut dziedziczący po `nn.Module` lub `nn.Parameter` jest uważany za taki, który zawiera parametry sieci, a więc przy wywołaniu metody `parameters()` - parametry z tych atrybutów pojawią się w liście wszystkich parametrów. Musimy też zdefiniować metodę `forward()`, która przyjmuje tensor `x` i zwraca wynik. Typowo ta metoda po prostu używa obiektów zdefiniowanych w konstruktorze.\n",
            "\n",
            "\n",
            "**UWAGA: nigdy w normalnych warunkach się nie woła metody `forward` ręcznie**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "J8niDgExAMDO"
         },
         "source": [
            "#### Zadanie 4 (0.5 punktu)\n",
            "\n",
            "Uzupełnij implementację 3-warstwowej sieci MLP. Użyj rozmiarów:\n",
            "* pierwsza warstwa: input_size x 256\n",
            "* druga warstwa: 256 x 128\n",
            "* trzecia warstwa: 128 x 1\n",
            "\n",
            "Użyj funkcji aktywacji ReLU.\n",
            "\n",
            "Przydatne klasy:\n",
            "- `nn.Sequential`\n",
            "- `nn.Linear`\n",
            "- `nn.ReLU`"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 91,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torch import sigmoid\n",
            "\n",
            "\n",
            "class MLP(nn.Module):\n",
            "    def __init__(self, input_size: int):\n",
            "        super().__init__()\n",
            "\n",
            "        self.transitions = nn.Sequential(            \n",
            "            nn.Linear(input_size, 256),\n",
            "            nn.ReLU(),\n",
            "            nn.Linear(256, 128),\n",
            "            nn.ReLU(),\n",
            "            nn.Linear(128, 1),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        return self.transitions(x)\n",
            "\n",
            "    def predict_proba(self, x):\n",
            "        return sigmoid(self(x))\n",
            "    \n",
            "    def predict(self, x, threshold: float = 0.5):\n",
            "        y_pred_score = self.predict_proba(x)\n",
            "        return (y_pred_score > threshold).to(torch.int32)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 92,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7079\n",
                  "Epoch 200 train loss: 0.6790\n",
                  "Epoch 400 train loss: 0.6542\n",
                  "Epoch 600 train loss: 0.6325\n",
                  "Epoch 800 train loss: 0.6134\n",
                  "Epoch 1000 train loss: 0.5965\n",
                  "Epoch 1200 train loss: 0.5816\n",
                  "Epoch 1400 train loss: 0.5684\n",
                  "Epoch 1600 train loss: 0.5569\n",
                  "Epoch 1800 train loss: 0.5467\n",
                  "final loss: 0.5378\n"
               ]
            }
         ],
         "source": [
            "learning_rate = 1e-3\n",
            "model = MLP(input_size=X_train.shape[1])\n",
            "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
            "\n",
            "# note that we are using loss function with sigmoid built in\n",
            "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
            "num_epochs = 2000\n",
            "evaluation_steps = 200\n",
            "\n",
            "for i in range(num_epochs):\n",
            "    y_pred = model(X_train)\n",
            "    loss = loss_fn(y_pred, y_train)\n",
            "    loss.backward()\n",
            "\n",
            "    optimizer.step()\n",
            "    optimizer.zero_grad()\n",
            "\n",
            "    if i % evaluation_steps == 0:\n",
            "        print(f\"Epoch {i} train loss: {loss.item():.4f}\")\n",
            "\n",
            "print(f\"final loss: {loss.item():.4f}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 94,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "LP5GSup24dXU",
            "outputId": "05f332c4-5d94-41f6-f85b-17793d3c4b49"
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "AUROC: 81.59%\n"
               ]
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 640x480 with 1 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "model.eval()\n",
            "with torch.no_grad():\n",
            "    # positive class probabilities\n",
            "    y_pred_valid_score = model.predict_proba(X_valid)\n",
            "    y_pred_test_score = model.predict_proba(X_test)\n",
            "\n",
            "auroc = roc_auc_score(y_test, y_pred_test_score)\n",
            "print(f\"AUROC: {100 * auroc:.2f}%\")\n",
            "\n",
            "plot_precision_recall_curve(y_valid, y_pred_valid_score)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "AUROC jest podobne, a precision i recall spadły - wypadamy wręcz gorzej od regresji liniowej! Skoro dodaliśmy więcej warstw, to może pojemność modelu jest teraz za duża i trzeba by go zregularyzować?\n",
            "\n",
            "Sieci neuronowe bardzo łatwo przeuczają, bo są bardzo elastycznymi i pojemnymi modelami. Dlatego mają wiele różnych rodzajów regularyzacji, których używa się razem. Co ciekawe, udowodniono eksperymentalnie, że zbyt duże sieci z mocną regularyzacją działają lepiej niż mniejsze sieci, odpowiedniego rozmiaru, za to ze słabszą regularyzacją.\n",
            "\n",
            "Pierwszy rodzaj regularyzacji to znana nam już **regularyzacja L2**, czyli penalizacja zbyt dużych wag. W kontekście sieci neuronowych nazywa się też ją czasem *weight decay*. W PyTorchu dodaje się ją jako argument do optymalizatora.\n",
            "\n",
            "Regularyzacja specyficzna dla sieci neuronowych to **dropout**. Polega on na losowym wyłączaniu zadanego procenta neuronów podczas treningu. Pomimo prostoty okazała się niesamowicie skuteczna, szczególnie w treningu bardzo głębokich sieci. Co ważne, jest to mechanizm używany tylko podczas treningu - w trakcie predykcji za pomocą sieci wyłącza się ten mechanizm i dokonuje normalnie predykcji całą siecią. Podejście to można potraktować jak ensemble learning, podobny do lasów losowych - wyłączając losowe części sieci, w każdej iteracji trenujemy nieco inną sieć, co odpowiada uśrednianiu predykcji różnych algorytmów. Typowo stosuje się dość mocny dropout, rzędu 25-50%. W PyTorchu implementuje go warstwa `nn.Dropout`, aplikowana zazwyczaj po funkcji aktywacji.\n",
            "\n",
            "Ostatni, a być może najważniejszy rodzaj regularyzacji to **wczesny stop (early stopping)**. W każdym kroku mocniej dostosowujemy terenową sieć do zbioru treningowego, a więc zbyt długi trening będzie skutkował przeuczeniem. W metodzie wczesnego stopu używamy wydzielonego zbioru walidacyjnego (pojedynczego, metoda holdout), sprawdzając co określoną liczbę epok wynik na tym zbiorze. Jeżeli nie uzyskamy wyniku lepszego od najlepszego dotychczas uzyskanego przez określoną liczbę epok, to przerywamy trening. Okres, przez który czekamy na uzyskanie lepszego wyniku, to cierpliwość (*patience*). Im mniejsze, tym mocniejszy jest ten rodzaj regularyzacji, ale trzeba z tym uważać, bo łatwo jest przesadzić i zbyt szybko przerywać trening. Niektóre implementacje uwzględniają tzw. *grace period*, czyli gwarantowaną minimalną liczbę epok, przez którą będziemy trenować sieć, niezależnie od wybranej cierpliwości.\n",
            "\n",
            "Dodatkowo ryzyko przeuczenia można zmniejszyć, używając mniejszej stałej uczącej."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Zadanie 5 (1.5 punktu)\n",
            "\n",
            "Zaimplementuj funkcję `evaluate_model()`, obliczającą metryki na zbiorze testowym:\n",
            "- wartość funkcji kosztu (loss)\n",
            "- AUROC\n",
            "- optymalny próg\n",
            "- F1-score przy optymalnym progu\n",
            "- precyzję oraz recall dla optymalnego progu\n",
            "\n",
            "Jeżeli podana jest wartość argumentu `threshold`, to użyj jej do zamiany prawdopodobieństw na twarde predykcje. W przeciwnym razie użyj funkcji `get_optimal_threshold` i oblicz optymalną wartość progu.\n",
            "\n",
            "Pamiętaj o przełączeniu modelu w tryb ewaluacji oraz o wyłączeniu obliczania gradientów."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 104,
         "metadata": {},
         "outputs": [],
         "source": [
            "from typing import Optional\n",
            "\n",
            "from sklearn.metrics import precision_score, recall_score, f1_score\n",
            "from torch import sigmoid\n",
            "\n",
            "def evaluate_model(\n",
            "    model: nn.Module, \n",
            "    X: torch.Tensor, \n",
            "    y: torch.Tensor, \n",
            "    loss_fn: nn.Module,\n",
            "    threshold: Optional[float] = None\n",
            ") -> Dict[str, float]:\n",
            "    # implement me!\n",
            "\n",
            "\n",
            "    results = {}\n",
            "\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        y_pred = model.predict_proba(X)\n",
            "\n",
            "    ### LOSS ###\n",
            "    results['loss'] = loss_fn(y_pred, y )\n",
            "    \n",
            "    ### AUROC ###\n",
            "    results['AUROC'] = roc_auc_score(y, y_pred)\n",
            "    \n",
            "    ### OPTIMAL VALUES ###\n",
            "    precisions, recalls, thresholds = precision_recall_curve(y, y_pred)\n",
            "    optimal_idx, optimal_threshold = get_optimal_threshold(precisions, recalls, thresholds)\n",
            "\n",
            "    optimal_precision = precisions[optimal_idx]\n",
            "    optimal_recall = recalls[optimal_idx]\n",
            "    optimal_f1 = 2 * optimal_precision * optimal_recall / (optimal_precision + optimal_recall)\n",
            "    \n",
            "    results['threshold'] = optimal_threshold\n",
            "    results['F1-score'] = optimal_f1\n",
            "    results['precision'] = precisions[optimal_idx]\n",
            "    results['recall'] = recalls[optimal_idx]\n",
            "\n",
            "    ### PASSED THRESHOLD ####    \n",
            "    if threshold is not None:\n",
            "        with torch.no_grad():\n",
            "            y_pred = model.predict(X, threshold)\n",
            "\n",
            "        results['threshold'] = threshold\n",
            "        results['F1-score'] = f1_score(y, y_pred)\n",
            "        results['precision'] = precision_score(y, y_pred)\n",
            "        results['recall'] = recall_score(y, y_pred)\n",
            "    \n",
            "    return results\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Zadanie 6 (0.5 punktu)\n",
            "\n",
            "Zaimplementuj 3-warstwową sieć MLP z dropout (50%). Rozmiary warstw ukrytych mają wynosić 256 i 128."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 95,
         "metadata": {},
         "outputs": [],
         "source": [
            "class RegularizedMLP(nn.Module):\n",
            "    def __init__(self, input_size: int, dropout_p: float = 0.5):\n",
            "        super().__init__()\n",
            "\n",
            "        self.transitions = nn.Sequential(            \n",
            "            nn.Linear(input_size, 256),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "\n",
            "            nn.Linear(256, 128),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "\n",
            "            nn.Linear(128, 1),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        return self.transitions(x)\n",
            "\n",
            "    def predict_proba(self, x):\n",
            "        return sigmoid(self(x))\n",
            "    \n",
            "    def predict(self, x, threshold: float = 0.5):\n",
            "        y_pred_score = self.predict_proba(x)\n",
            "        return (y_pred_score > threshold).to(torch.int32)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "rEk9azaULAsz"
         },
         "source": [
            "Opisaliśmy wcześniej podstawowy optymalizator w sieciach neuronowych - spadek wzdłuż gradientu. Jednak wymaga on użycia całego zbioru danych, aby obliczyć gradient, co jest często niewykonalne przez rozmiar zbioru. Dlatego wymyślono **stochastyczny spadek wzdłuż gradientu (stochastic gradient descent, SGD)**, w którym używamy 1 przykładu naraz, liczymy gradient tylko po nim i aktualizujemy parametry. Jest to oczywiście dość grube przybliżenie gradientu, ale pozwala robić szybko dużo małych kroków. Kompromisem, którego używa się w praktyce, jest **minibatch gradient descent**, czyli używanie batchy np. 32, 64 czy 128 przykładów.\n",
            "\n",
            "Rzadko wspominanym, a ważnym faktem jest także to, że stochastyczność metody optymalizacji jest sama w sobie też [metodą regularyzacji](https://arxiv.org/abs/2101.12176), a więc `batch_size` to także hiperparametr.\n",
            "\n",
            "Obecnie najpopularniejszą odmianą SGD jest [Adam](https://arxiv.org/abs/1412.6980), gdyż uczy on szybko sieć oraz daje bardzo dobre wyniki nawet przy niekoniecznie idealnie dobranych hiperparametrach. W PyTorchu najlepiej korzystać z jego implementacji `AdamW`, która jest nieco lepsza niż implementacja `Adam`. Jest to zasadniczo zawsze wybór domyślny przy treningu współczesnych sieci neuronowych.\n",
            "\n",
            "Na razie użyjemy jednak minibatch SGD."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Poniżej znajduje się implementacja prostej klasy dziedziczącej po `Dataset` - tak w PyTorchu implementuje się własne zbiory danych. Użycie takich klas umożliwia użycie klas ładujących dane (`DataLoader`), które z kolei pozwalają łatwo ładować batche danych. Trzeba w takiej klasie zaimplementować metody:\n",
            "- `__len__` - zwraca ilość punktów w zbiorze\n",
            "- `__getitem__` - zwraca przykład ze zbioru pod danym indeksem oraz jego klasę\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 96,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torch.utils.data import Dataset\n",
            "\n",
            "\n",
            "class MyDataset(Dataset):\n",
            "    def __init__(self, data, y):\n",
            "        super().__init__()\n",
            "        \n",
            "        self.data = data\n",
            "        self.y = y\n",
            "    \n",
            "    def __len__(self):\n",
            "        return self.data.shape[0]\n",
            "    \n",
            "    def __getitem__(self, idx):\n",
            "        return self.data[idx], self.y[idx]\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Zadanie 7 (1.5 punktu)\n",
            "\n",
            "Zaimplementuj pętlę treningowo-walidacyjną dla sieci neuronowej. Wykorzystaj podane wartości hiperparametrów do treningu (stała ucząca, prawdopodobieństwo dropoutu, regularyzacja L2, rozmiar batcha, maksymalna liczba epok). Użyj optymalizatora SGD.\n",
            "\n",
            "Dodatkowo zaimplementuj regularyzację przez early stopping. Sprawdzaj co epokę wynik na zbiorze walidacyjnym. Użyj podanej wartości patience, a jako metryki po prostu wartości funkcji kosztu. Może się tutaj przydać zaimplementowana funkcja `evaluate_model()`.\n",
            "\n",
            "Pamiętaj o tym, aby przechowywać najlepszy dotychczasowy wynik walidacyjny oraz najlepszy dotychczasowy model. Zapamiętaj też optymalny próg do klasyfikacji dla najlepszego modelu."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 97,
         "metadata": {},
         "outputs": [],
         "source": [
            "from copy import deepcopy\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "\n",
            "\n",
            "learning_rate = 1e-3\n",
            "dropout_p = 0.5\n",
            "l2_reg = 1e-4\n",
            "batch_size = 128\n",
            "max_epochs = 300\n",
            "\n",
            "early_stopping_patience = 4"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 105,
         "metadata": {
            "scrolled": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.6561, eval loss 0.8409299850463867\n",
                  "Epoch 1 train loss: 0.6428, eval loss 0.8347378969192505\n",
                  "Epoch 2 train loss: 0.6273, eval loss 0.8289541602134705\n",
                  "Epoch 3 train loss: 0.6176, eval loss 0.8235336542129517\n",
                  "Epoch 4 train loss: 0.6047, eval loss 0.8184210658073425\n",
                  "Epoch 5 train loss: 0.5915, eval loss 0.8135877251625061\n",
                  "Epoch 6 train loss: 0.5868, eval loss 0.8090202808380127\n",
                  "Epoch 7 train loss: 0.5759, eval loss 0.8047022223472595\n",
                  "Epoch 8 train loss: 0.5633, eval loss 0.800606369972229\n",
                  "Epoch 9 train loss: 0.5559, eval loss 0.7967148423194885\n",
                  "Epoch 10 train loss: 0.5538, eval loss 0.7930673360824585\n",
                  "Epoch 11 train loss: 0.5467, eval loss 0.7895961403846741\n",
                  "Epoch 12 train loss: 0.5477, eval loss 0.7863551378250122\n",
                  "Epoch 13 train loss: 0.5487, eval loss 0.7832545638084412\n",
                  "Epoch 14 train loss: 0.5344, eval loss 0.7803651094436646\n",
                  "Epoch 15 train loss: 0.5408, eval loss 0.7776179909706116\n",
                  "Epoch 16 train loss: 0.5271, eval loss 0.775051474571228\n",
                  "Epoch 17 train loss: 0.5202, eval loss 0.7726078629493713\n",
                  "Epoch 18 train loss: 0.5216, eval loss 0.7702898979187012\n",
                  "Epoch 19 train loss: 0.5180, eval loss 0.7681267857551575\n",
                  "Epoch 20 train loss: 0.5199, eval loss 0.7660524845123291\n",
                  "Epoch 21 train loss: 0.5221, eval loss 0.7641136050224304\n",
                  "Epoch 22 train loss: 0.5123, eval loss 0.762246310710907\n",
                  "Epoch 23 train loss: 0.5158, eval loss 0.7605046629905701\n",
                  "Epoch 24 train loss: 0.5142, eval loss 0.7588704824447632\n",
                  "Epoch 25 train loss: 0.5098, eval loss 0.7572818398475647\n",
                  "Epoch 26 train loss: 0.4946, eval loss 0.7557525038719177\n",
                  "Epoch 27 train loss: 0.4909, eval loss 0.7542656660079956\n",
                  "Epoch 28 train loss: 0.4980, eval loss 0.7527868747711182\n",
                  "Epoch 29 train loss: 0.4870, eval loss 0.7513915300369263\n",
                  "Epoch 30 train loss: 0.4985, eval loss 0.7501153349876404\n",
                  "Epoch 31 train loss: 0.4844, eval loss 0.748813807964325\n",
                  "Epoch 32 train loss: 0.4875, eval loss 0.7475222945213318\n",
                  "Epoch 33 train loss: 0.4839, eval loss 0.7462928295135498\n",
                  "Epoch 34 train loss: 0.4799, eval loss 0.745082437992096\n",
                  "Epoch 35 train loss: 0.4729, eval loss 0.743872344493866\n",
                  "Epoch 36 train loss: 0.4772, eval loss 0.7427512407302856\n",
                  "Epoch 37 train loss: 0.4587, eval loss 0.741654634475708\n",
                  "Epoch 38 train loss: 0.4683, eval loss 0.740588903427124\n",
                  "Epoch 39 train loss: 0.4546, eval loss 0.7394770979881287\n",
                  "Epoch 40 train loss: 0.4707, eval loss 0.7384206056594849\n",
                  "Epoch 41 train loss: 0.4547, eval loss 0.7374183535575867\n",
                  "Epoch 42 train loss: 0.4643, eval loss 0.7364249229431152\n",
                  "Epoch 43 train loss: 0.4492, eval loss 0.7354336380958557\n",
                  "Epoch 44 train loss: 0.4486, eval loss 0.7344078421592712\n",
                  "Epoch 45 train loss: 0.4715, eval loss 0.733426034450531\n",
                  "Epoch 46 train loss: 0.4487, eval loss 0.732463002204895\n",
                  "Epoch 47 train loss: 0.4550, eval loss 0.7315363883972168\n",
                  "Epoch 48 train loss: 0.4443, eval loss 0.7307389974594116\n",
                  "Epoch 49 train loss: 0.4264, eval loss 0.7298889756202698\n",
                  "Epoch 50 train loss: 0.4430, eval loss 0.7290868163108826\n",
                  "Epoch 51 train loss: 0.4401, eval loss 0.7282419800758362\n",
                  "Epoch 52 train loss: 0.4479, eval loss 0.7274570465087891\n",
                  "Epoch 53 train loss: 0.4382, eval loss 0.7266631722450256\n",
                  "Epoch 54 train loss: 0.4411, eval loss 0.725945770740509\n",
                  "Epoch 55 train loss: 0.4404, eval loss 0.725200355052948\n",
                  "Epoch 56 train loss: 0.4370, eval loss 0.7244736552238464\n",
                  "Epoch 57 train loss: 0.4198, eval loss 0.7238161563873291\n",
                  "Epoch 58 train loss: 0.4274, eval loss 0.723158061504364\n",
                  "Epoch 59 train loss: 0.4232, eval loss 0.7224931120872498\n",
                  "Epoch 60 train loss: 0.4127, eval loss 0.7218281030654907\n",
                  "Epoch 61 train loss: 0.4388, eval loss 0.7212294936180115\n",
                  "Epoch 62 train loss: 0.4194, eval loss 0.7206520438194275\n",
                  "Epoch 63 train loss: 0.4268, eval loss 0.7200978994369507\n",
                  "Epoch 64 train loss: 0.4219, eval loss 0.7194838523864746\n",
                  "Epoch 65 train loss: 0.4114, eval loss 0.7189707159996033\n",
                  "Epoch 66 train loss: 0.4288, eval loss 0.7184216380119324\n",
                  "Epoch 67 train loss: 0.4360, eval loss 0.7179437875747681\n",
                  "Epoch 68 train loss: 0.4130, eval loss 0.7173982262611389\n",
                  "Epoch 69 train loss: 0.4309, eval loss 0.7169532179832458\n",
                  "Epoch 70 train loss: 0.4191, eval loss 0.716463565826416\n",
                  "Epoch 71 train loss: 0.4135, eval loss 0.7159793972969055\n",
                  "Epoch 72 train loss: 0.4146, eval loss 0.7154910564422607\n",
                  "Epoch 73 train loss: 0.4013, eval loss 0.7150589227676392\n",
                  "Epoch 74 train loss: 0.4334, eval loss 0.7146172523498535\n",
                  "Epoch 75 train loss: 0.4073, eval loss 0.7141782641410828\n",
                  "Epoch 76 train loss: 0.4077, eval loss 0.7137504816055298\n",
                  "Epoch 77 train loss: 0.4089, eval loss 0.7133535146713257\n",
                  "Epoch 78 train loss: 0.4117, eval loss 0.7129895687103271\n",
                  "Epoch 79 train loss: 0.4110, eval loss 0.7126367092132568\n",
                  "Epoch 80 train loss: 0.3972, eval loss 0.7122700810432434\n",
                  "Epoch 81 train loss: 0.3827, eval loss 0.7118843793869019\n",
                  "Epoch 82 train loss: 0.4118, eval loss 0.7115022540092468\n",
                  "Epoch 83 train loss: 0.4104, eval loss 0.7111331820487976\n",
                  "Epoch 84 train loss: 0.4483, eval loss 0.7107885479927063\n",
                  "Epoch 85 train loss: 0.3891, eval loss 0.7104740738868713\n",
                  "Epoch 86 train loss: 0.3773, eval loss 0.7101176381111145\n",
                  "Epoch 87 train loss: 0.4279, eval loss 0.7098321318626404\n",
                  "Epoch 88 train loss: 0.4035, eval loss 0.7095593214035034\n",
                  "Epoch 89 train loss: 0.4172, eval loss 0.7092218995094299\n",
                  "Epoch 90 train loss: 0.4035, eval loss 0.7089887857437134\n",
                  "Epoch 91 train loss: 0.4103, eval loss 0.7087026238441467\n",
                  "Epoch 92 train loss: 0.4172, eval loss 0.7082858085632324\n",
                  "Epoch 93 train loss: 0.4131, eval loss 0.7081155776977539\n",
                  "Epoch 94 train loss: 0.4055, eval loss 0.7078969478607178\n",
                  "Epoch 95 train loss: 0.3892, eval loss 0.7076750993728638\n",
                  "Epoch 96 train loss: 0.4234, eval loss 0.7073931694030762\n",
                  "Epoch 97 train loss: 0.4054, eval loss 0.7071532607078552\n",
                  "Epoch 98 train loss: 0.3909, eval loss 0.7069347500801086\n",
                  "Epoch 99 train loss: 0.3986, eval loss 0.706795334815979\n",
                  "Epoch 100 train loss: 0.3852, eval loss 0.7065563201904297\n",
                  "Epoch 101 train loss: 0.4149, eval loss 0.7063311338424683\n",
                  "Epoch 102 train loss: 0.3924, eval loss 0.7060286998748779\n",
                  "Epoch 103 train loss: 0.4049, eval loss 0.7058963179588318\n",
                  "Epoch 104 train loss: 0.3815, eval loss 0.7057217955589294\n",
                  "Epoch 105 train loss: 0.3709, eval loss 0.7055555582046509\n",
                  "Epoch 106 train loss: 0.3919, eval loss 0.7053222060203552\n",
                  "Epoch 107 train loss: 0.3907, eval loss 0.7052744626998901\n",
                  "Epoch 108 train loss: 0.3980, eval loss 0.705079197883606\n",
                  "Epoch 109 train loss: 0.3888, eval loss 0.7048582434654236\n",
                  "Epoch 110 train loss: 0.3937, eval loss 0.7046995162963867\n",
                  "Epoch 111 train loss: 0.4140, eval loss 0.7045410871505737\n",
                  "Epoch 112 train loss: 0.4010, eval loss 0.7042675614356995\n",
                  "Epoch 113 train loss: 0.3945, eval loss 0.7040615677833557\n",
                  "Epoch 114 train loss: 0.3924, eval loss 0.7038549184799194\n",
                  "Epoch 115 train loss: 0.3781, eval loss 0.7038099765777588\n",
                  "Epoch 116 train loss: 0.4092, eval loss 0.7035799622535706\n",
                  "Epoch 117 train loss: 0.3958, eval loss 0.7034440040588379\n",
                  "Epoch 118 train loss: 0.3943, eval loss 0.703405499458313\n",
                  "Epoch 119 train loss: 0.3858, eval loss 0.703220784664154\n",
                  "Epoch 120 train loss: 0.3864, eval loss 0.703070878982544\n",
                  "Epoch 121 train loss: 0.3814, eval loss 0.7028730511665344\n",
                  "Epoch 122 train loss: 0.4085, eval loss 0.7028163075447083\n",
                  "Epoch 123 train loss: 0.4213, eval loss 0.7027440667152405\n",
                  "Epoch 124 train loss: 0.3959, eval loss 0.7026003003120422\n",
                  "Epoch 125 train loss: 0.3735, eval loss 0.7024908065795898\n",
                  "Epoch 126 train loss: 0.4050, eval loss 0.7023977041244507\n",
                  "Epoch 127 train loss: 0.4365, eval loss 0.7023091316223145\n",
                  "Epoch 128 train loss: 0.3616, eval loss 0.7022541165351868\n",
                  "Epoch 129 train loss: 0.4093, eval loss 0.7021613121032715\n",
                  "Epoch 130 train loss: 0.3911, eval loss 0.7020716071128845\n",
                  "Epoch 131 train loss: 0.3772, eval loss 0.7019233107566833\n",
                  "Epoch 132 train loss: 0.3841, eval loss 0.7017985582351685\n",
                  "Epoch 133 train loss: 0.3895, eval loss 0.7016856670379639\n",
                  "Epoch 134 train loss: 0.3753, eval loss 0.7015910744667053\n",
                  "Epoch 135 train loss: 0.3659, eval loss 0.7014528512954712\n",
                  "Epoch 136 train loss: 0.3547, eval loss 0.701321005821228\n",
                  "Epoch 137 train loss: 0.3673, eval loss 0.7012854218482971\n",
                  "Epoch 138 train loss: 0.4117, eval loss 0.7012841701507568\n",
                  "Epoch 139 train loss: 0.3933, eval loss 0.7011747360229492\n",
                  "Epoch 140 train loss: 0.3890, eval loss 0.7010185718536377\n",
                  "Epoch 141 train loss: 0.3743, eval loss 0.7008830308914185\n",
                  "Epoch 142 train loss: 0.3987, eval loss 0.700796365737915\n",
                  "Epoch 143 train loss: 0.3966, eval loss 0.7007476091384888\n",
                  "Epoch 144 train loss: 0.3866, eval loss 0.7006731629371643\n",
                  "Epoch 145 train loss: 0.3881, eval loss 0.7005882263183594\n",
                  "Epoch 146 train loss: 0.4075, eval loss 0.7004398703575134\n",
                  "Epoch 147 train loss: 0.3704, eval loss 0.7003726363182068\n",
                  "Epoch 148 train loss: 0.3895, eval loss 0.700339674949646\n",
                  "Epoch 149 train loss: 0.4096, eval loss 0.7002452611923218\n",
                  "Epoch 150 train loss: 0.3738, eval loss 0.7001515030860901\n",
                  "Epoch 151 train loss: 0.3552, eval loss 0.7000755667686462\n",
                  "Epoch 152 train loss: 0.4122, eval loss 0.7000067234039307\n",
                  "Epoch 153 train loss: 0.3868, eval loss 0.6999034881591797\n",
                  "Epoch 154 train loss: 0.3783, eval loss 0.699785590171814\n",
                  "Epoch 155 train loss: 0.3960, eval loss 0.6997497081756592\n",
                  "Epoch 156 train loss: 0.3443, eval loss 0.6996443271636963\n",
                  "Epoch 157 train loss: 0.3924, eval loss 0.6995765566825867\n",
                  "Epoch 158 train loss: 0.3675, eval loss 0.6994516253471375\n",
                  "Epoch 159 train loss: 0.3970, eval loss 0.6994103789329529\n",
                  "Epoch 160 train loss: 0.3726, eval loss 0.6993390917778015\n",
                  "Epoch 161 train loss: 0.3789, eval loss 0.6992347836494446\n",
                  "Epoch 162 train loss: 0.3636, eval loss 0.6992636322975159\n",
                  "Epoch 163 train loss: 0.3696, eval loss 0.6992079019546509\n",
                  "Epoch 164 train loss: 0.4027, eval loss 0.6990893483161926\n",
                  "Epoch 165 train loss: 0.3764, eval loss 0.6989359855651855\n",
                  "Epoch 166 train loss: 0.3867, eval loss 0.6988809108734131\n",
                  "Epoch 167 train loss: 0.3803, eval loss 0.6987667679786682\n",
                  "Epoch 168 train loss: 0.4091, eval loss 0.6986765265464783\n",
                  "Epoch 169 train loss: 0.3699, eval loss 0.6987003087997437\n",
                  "Epoch 170 train loss: 0.3600, eval loss 0.698591411113739\n",
                  "Epoch 171 train loss: 0.3756, eval loss 0.6985532641410828\n",
                  "Epoch 172 train loss: 0.3769, eval loss 0.6984434723854065\n",
                  "Epoch 173 train loss: 0.3813, eval loss 0.698372483253479\n",
                  "Epoch 174 train loss: 0.4015, eval loss 0.6983578205108643\n",
                  "Epoch 175 train loss: 0.4137, eval loss 0.6983200907707214\n",
                  "Epoch 176 train loss: 0.3641, eval loss 0.6982592344284058\n",
                  "Epoch 177 train loss: 0.3707, eval loss 0.6981958150863647\n",
                  "Epoch 178 train loss: 0.3809, eval loss 0.6980910897254944\n",
                  "Epoch 179 train loss: 0.3671, eval loss 0.698129415512085\n",
                  "Epoch 180 train loss: 0.3833, eval loss 0.6981008648872375\n",
                  "Epoch 181 train loss: 0.4258, eval loss 0.6980197429656982\n",
                  "Epoch 182 train loss: 0.3800, eval loss 0.6979659795761108\n",
                  "Epoch 183 train loss: 0.3858, eval loss 0.6979085206985474\n",
                  "Epoch 184 train loss: 0.3585, eval loss 0.697880744934082\n",
                  "Epoch 185 train loss: 0.4210, eval loss 0.6977826952934265\n",
                  "Epoch 186 train loss: 0.3541, eval loss 0.697681725025177\n",
                  "Epoch 187 train loss: 0.4049, eval loss 0.6977058053016663\n",
                  "Epoch 188 train loss: 0.3724, eval loss 0.6976835131645203\n",
                  "Epoch 189 train loss: 0.3760, eval loss 0.6975659728050232\n",
                  "Epoch 190 train loss: 0.3701, eval loss 0.6974442005157471\n",
                  "Epoch 191 train loss: 0.3996, eval loss 0.697385847568512\n",
                  "Epoch 192 train loss: 0.3832, eval loss 0.697351336479187\n",
                  "Epoch 193 train loss: 0.3858, eval loss 0.697209894657135\n",
                  "Epoch 194 train loss: 0.3764, eval loss 0.6971180438995361\n",
                  "Epoch 195 train loss: 0.3741, eval loss 0.6970224976539612\n",
                  "Epoch 196 train loss: 0.3615, eval loss 0.696982741355896\n",
                  "Epoch 197 train loss: 0.3752, eval loss 0.6969619393348694\n",
                  "Epoch 198 train loss: 0.3657, eval loss 0.6969533562660217\n",
                  "Epoch 199 train loss: 0.3589, eval loss 0.6968921422958374\n",
                  "Epoch 200 train loss: 0.3456, eval loss 0.6968883275985718\n",
                  "Epoch 201 train loss: 0.3544, eval loss 0.6967845559120178\n",
                  "Epoch 202 train loss: 0.3629, eval loss 0.6966912746429443\n",
                  "Epoch 203 train loss: 0.3659, eval loss 0.6967068910598755\n",
                  "Epoch 204 train loss: 0.3788, eval loss 0.6966730952262878\n",
                  "Epoch 205 train loss: 0.3902, eval loss 0.6966158747673035\n",
                  "Epoch 206 train loss: 0.3666, eval loss 0.6965816020965576\n",
                  "Epoch 207 train loss: 0.3931, eval loss 0.6965628862380981\n",
                  "Epoch 208 train loss: 0.3835, eval loss 0.6964662671089172\n",
                  "Epoch 209 train loss: 0.3602, eval loss 0.696416437625885\n",
                  "Epoch 210 train loss: 0.3617, eval loss 0.696419358253479\n",
                  "Epoch 211 train loss: 0.3796, eval loss 0.6963728666305542\n",
                  "Epoch 212 train loss: 0.3925, eval loss 0.6963205933570862\n",
                  "Epoch 213 train loss: 0.3519, eval loss 0.6962488293647766\n",
                  "Epoch 214 train loss: 0.3505, eval loss 0.6961947083473206\n",
                  "Epoch 215 train loss: 0.3862, eval loss 0.6961609125137329\n",
                  "Epoch 216 train loss: 0.3551, eval loss 0.6960450410842896\n",
                  "Epoch 217 train loss: 0.3787, eval loss 0.6959863901138306\n",
                  "Epoch 218 train loss: 0.3570, eval loss 0.6959823369979858\n",
                  "Epoch 219 train loss: 0.3597, eval loss 0.69593346118927\n",
                  "Epoch 220 train loss: 0.3660, eval loss 0.6958901882171631\n",
                  "Epoch 221 train loss: 0.3850, eval loss 0.6958471536636353\n",
                  "Epoch 222 train loss: 0.3689, eval loss 0.6958526968955994\n",
                  "Epoch 223 train loss: 0.3589, eval loss 0.6958857178688049\n",
                  "Epoch 224 train loss: 0.3686, eval loss 0.6957935094833374\n",
                  "Epoch 225 train loss: 0.3991, eval loss 0.695766270160675\n",
                  "Epoch 226 train loss: 0.3735, eval loss 0.6957316398620605\n",
                  "Epoch 227 train loss: 0.3463, eval loss 0.6956485509872437\n",
                  "Epoch 228 train loss: 0.3516, eval loss 0.6955118775367737\n",
                  "Epoch 229 train loss: 0.3890, eval loss 0.695419192314148\n",
                  "Epoch 230 train loss: 0.4000, eval loss 0.6954576969146729\n",
                  "Epoch 231 train loss: 0.3797, eval loss 0.6954153776168823\n",
                  "Epoch 232 train loss: 0.4223, eval loss 0.6953117847442627\n",
                  "Epoch 233 train loss: 0.3948, eval loss 0.6952800750732422\n",
                  "Epoch 234 train loss: 0.3749, eval loss 0.6952531337738037\n",
                  "Epoch 235 train loss: 0.3811, eval loss 0.695278525352478\n",
                  "Epoch 236 train loss: 0.3912, eval loss 0.6951786875724792\n",
                  "Epoch 237 train loss: 0.3819, eval loss 0.6951428055763245\n",
                  "Epoch 238 train loss: 0.3937, eval loss 0.6950604915618896\n",
                  "Epoch 239 train loss: 0.3887, eval loss 0.6950553059577942\n",
                  "Epoch 240 train loss: 0.3748, eval loss 0.6949936747550964\n",
                  "Epoch 241 train loss: 0.3433, eval loss 0.6949865221977234\n",
                  "Epoch 242 train loss: 0.3714, eval loss 0.6949794888496399\n",
                  "Epoch 243 train loss: 0.3680, eval loss 0.694897472858429\n",
                  "Epoch 244 train loss: 0.3899, eval loss 0.6948021650314331\n",
                  "Epoch 245 train loss: 0.3500, eval loss 0.6947320103645325\n",
                  "Epoch 246 train loss: 0.3436, eval loss 0.6947509050369263\n",
                  "Epoch 247 train loss: 0.3601, eval loss 0.6947082877159119\n",
                  "Epoch 248 train loss: 0.3812, eval loss 0.6946653723716736\n",
                  "Epoch 249 train loss: 0.3780, eval loss 0.6946221590042114\n",
                  "Epoch 250 train loss: 0.3813, eval loss 0.694658637046814\n",
                  "Epoch 251 train loss: 0.3572, eval loss 0.6946781873703003\n",
                  "Epoch 252 train loss: 0.3986, eval loss 0.6946726441383362\n",
                  "Epoch 253 train loss: 0.3843, eval loss 0.6946136355400085\n",
                  "Epoch 254 train loss: 0.3488, eval loss 0.6945663094520569\n",
                  "Epoch 255 train loss: 0.3814, eval loss 0.6945155262947083\n",
                  "Epoch 256 train loss: 0.3526, eval loss 0.6944079399108887\n",
                  "Epoch 257 train loss: 0.3913, eval loss 0.6943414807319641\n",
                  "Epoch 258 train loss: 0.3666, eval loss 0.6942801475524902\n",
                  "Epoch 259 train loss: 0.3973, eval loss 0.6942387223243713\n",
                  "Epoch 260 train loss: 0.3425, eval loss 0.694188117980957\n",
                  "Epoch 261 train loss: 0.3755, eval loss 0.6941350698471069\n",
                  "Epoch 262 train loss: 0.3769, eval loss 0.6940568089485168\n",
                  "Epoch 263 train loss: 0.3704, eval loss 0.6940329670906067\n",
                  "Epoch 264 train loss: 0.3752, eval loss 0.6940045356750488\n",
                  "Epoch 265 train loss: 0.3669, eval loss 0.6940960884094238\n",
                  "Epoch 266 train loss: 0.3621, eval loss 0.694042980670929\n",
                  "Epoch 267 train loss: 0.3674, eval loss 0.6940310597419739\n",
                  "Epoch 268 train loss: 0.3838, eval loss 0.6939846277236938\n",
                  "Epoch 269 train loss: 0.3813, eval loss 0.6939203143119812\n",
                  "Epoch 270 train loss: 0.3620, eval loss 0.6938971281051636\n",
                  "Epoch 271 train loss: 0.3738, eval loss 0.6938096284866333\n",
                  "Epoch 272 train loss: 0.3375, eval loss 0.6937267184257507\n",
                  "Epoch 273 train loss: 0.3506, eval loss 0.6936832070350647\n",
                  "Epoch 274 train loss: 0.3598, eval loss 0.6937447190284729\n",
                  "Epoch 275 train loss: 0.3728, eval loss 0.6937463283538818\n",
                  "Epoch 276 train loss: 0.3630, eval loss 0.6936777234077454\n",
                  "Epoch 277 train loss: 0.3646, eval loss 0.6936774849891663\n",
                  "Epoch 278 train loss: 0.3661, eval loss 0.6935334205627441\n",
                  "Epoch 279 train loss: 0.3692, eval loss 0.6935674548149109\n",
                  "Epoch 280 train loss: 0.3239, eval loss 0.693548321723938\n",
                  "Epoch 281 train loss: 0.3739, eval loss 0.6935914754867554\n"
               ]
            }
         ],
         "source": [
            "model = RegularizedMLP(\n",
            "    input_size=X_train.shape[1], \n",
            "    dropout_p=dropout_p\n",
            ")\n",
            "optimizer = torch.optim.SGD(\n",
            "    model.parameters(), \n",
            "    lr=learning_rate, \n",
            "    weight_decay=l2_reg\n",
            ")\n",
            "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
            "\n",
            "train_dataset = MyDataset(X_train, y_train)\n",
            "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
            "\n",
            "steps_without_improvement = 0\n",
            "\n",
            "best_val_loss = np.inf\n",
            "best_model = None\n",
            "best_threshold = None\n",
            "\n",
            "for epoch_num in range(max_epochs):\n",
            "    model.train()\n",
            "\n",
            "    # note that we are using DataLoader to get batches\n",
            "    for X_batch, y_batch in train_dataloader:\n",
            "        # model training\n",
            "        outputs = model(X_batch)\n",
            "        loss = loss_fn(outputs, y_batch)\n",
            "\n",
            "        optimizer.zero_grad()\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "\n",
            "    # model evaluation, early stopping\n",
            "    \n",
            "    model.eval()\n",
            "\n",
            "    valid_metrics = evaluate_model(model, X_valid, y_valid, loss_fn)\n",
            "\n",
            "    if best_val_loss > valid_metrics['loss']:\n",
            "        steps_without_improvement = 0\n",
            "\n",
            "        best_model = deepcopy(model)\n",
            "        best_val_loss = valid_metrics['loss']\n",
            "        best_threshold = valid_metrics['threshold']\n",
            "\n",
            "    else:\n",
            "        steps_without_improvement += 1\n",
            "        \n",
            "        if steps_without_improvement == early_stopping_patience:\n",
            "            break\n",
            "    \n",
            "    print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 106,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "AUROC: 90.09%\n",
                  "F1: 68.32%\n",
                  "Precision: 62.96%\n",
                  "Recall: 74.68%\n"
               ]
            }
         ],
         "source": [
            "test_metrics = evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n",
            "\n",
            "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
            "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
            "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
            "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Wyniki wyglądają już dużo lepiej.\n",
            "\n",
            "Na koniec laboratorium dołożymy do naszego modelu jeszcze 3 powrzechnie używane techniki, które są bardzo proste, a pozwalają często ulepszyć wynik modelu.\n",
            "\n",
            "Pierwszą z nich są **warstwy normalizacji (normalization layers)**. Powstały one początkowo z założeniem, że przez przekształcenia przestrzeni dokonywane przez sieć zmienia się rozkład prawdopodobieństw pomiędzy warstwami, czyli tzw. *internal covariate shift*. Później okazało się, że zastosowanie takiej normalizacji wygładza powierzchnię funkcji kosztu, co ułatwia i przyspiesza optymalizację. Najpowszechniej używaną normalizacją jest **batch normalization (batch norm)**.\n",
            "\n",
            "Drugim ulepszeniem jest dodanie **wag klas (class weights)**. Mamy do czynienia z problemem klasyfikacji niezbalansowanej, więc klasa mniejszościowa, ważniejsza dla nas, powinna dostać większą wagę. Implementuje się to trywialnie prosto - po prostu mnożymy wartość funkcji kosztu dla danego przykładu przez wagę dla prawdziwej klasy tego przykładu. Praktycznie każdy klasyfikator operujący na jakiejś ważonej funkcji może działać w ten sposób, nie tylko sieci neuronowe.\n",
            "\n",
            "Ostatnim ulepszeniem jest zamiana SGD na optymalizator Adam, a konkretnie na optymalizator `AdamW`. Jest to przykład **optymalizatora adaptacyjnego (adaptive optimizer)**, który potrafi zaadaptować stałą uczącą dla każdego parametru z osobna w trakcie treningu. Wykorzystuje do tego gradienty - w uproszczeniu, im większa wariancja gradientu, tym mniejsze kroki w tym kierunku robimy."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Zadanie 8 (0.5 punktu)\n",
            "\n",
            "Zaimplementuj model `NormalizingMLP`, o takiej samej strukturze jak `RegularizedMLP`, ale dodatkowo z warstwami `BatchNorm1d` pomiędzy warstwami `Linear` oraz `ReLU`.\n",
            "\n",
            "Za pomocą funkcji `compute_class_weight()` oblicz wagi dla poszczególnych klas. Użyj opcji `\"balanced\"`. Przekaż do funkcji kosztu wagę klasy pozytywnej (pamiętaj, aby zamienić ją na tensor).\n",
            "\n",
            "Zamień używany optymalizator na `AdamW`.\n",
            "\n",
            "Na koniec skopiuj resztę kodu do treningu z poprzedniego zadania, wytrenuj sieć i oblicz wyniki na zbiorze testowym."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 111,
         "metadata": {},
         "outputs": [],
         "source": [
            "class NormalizingMLP(nn.Module):\n",
            "    def __init__(self, input_size: int, dropout_p: float = 0.5):\n",
            "        super().__init__()\n",
            "\n",
            "        self.transitions = nn.Sequential(            \n",
            "            nn.Linear(input_size, 256),\n",
            "            nn.BatchNorm1d(256),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "\n",
            "            nn.Linear(256, 128),\n",
            "            nn.BatchNorm1d(128),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "\n",
            "            nn.Linear(128, 1),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        return self.transitions(x)\n",
            "\n",
            "    def predict_proba(self, x):\n",
            "        return sigmoid(self(x))\n",
            "    \n",
            "    def predict(self, x, threshold: float = 0.5):\n",
            "        y_pred_score = self.predict_proba(x)\n",
            "        return (y_pred_score > threshold).to(torch.int32)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 117,
         "metadata": {},
         "outputs": [],
         "source": [
            "# define all the hyperparameters\n",
            "# your_code\n",
            "\n",
            "from copy import deepcopy\n",
            "from sklearn.utils import compute_class_weight\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "\n",
            "\n",
            "learning_rate = 1e-3\n",
            "dropout_p = 0.5\n",
            "l2_reg = 1e-4\n",
            "batch_size = 128\n",
            "max_epochs = 300\n",
            "\n",
            "early_stopping_patience = 4\n",
            "\n",
            "weights = compute_class_weight(\n",
            "    \"balanced\",\n",
            "    classes = np.unique(y),\n",
            "    y = y\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 118,
         "metadata": {
            "scrolled": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7869, eval loss 1.44856595993042\n",
                  "Epoch 1 train loss: 0.7464, eval loss 1.4412180185317993\n",
                  "Epoch 2 train loss: 0.7870, eval loss 1.437193512916565\n",
                  "Epoch 3 train loss: 0.7217, eval loss 1.4344581365585327\n",
                  "Epoch 4 train loss: 0.7118, eval loss 1.4328137636184692\n",
                  "Epoch 5 train loss: 0.7708, eval loss 1.4321856498718262\n",
                  "Epoch 6 train loss: 0.7051, eval loss 1.430405855178833\n",
                  "Epoch 7 train loss: 0.7059, eval loss 1.4293659925460815\n",
                  "Epoch 8 train loss: 0.6812, eval loss 1.4294514656066895\n",
                  "Epoch 9 train loss: 0.7394, eval loss 1.430424690246582\n",
                  "Epoch 10 train loss: 0.7216, eval loss 1.4290835857391357\n",
                  "Epoch 11 train loss: 0.6852, eval loss 1.4301484823226929\n",
                  "Epoch 12 train loss: 0.7425, eval loss 1.4270368814468384\n",
                  "Epoch 13 train loss: 0.6844, eval loss 1.4269318580627441\n",
                  "Epoch 14 train loss: 0.6379, eval loss 1.4273426532745361\n",
                  "Epoch 15 train loss: 0.6733, eval loss 1.4252169132232666\n",
                  "Epoch 16 train loss: 0.6584, eval loss 1.4285341501235962\n",
                  "Epoch 17 train loss: 0.7431, eval loss 1.424997329711914\n",
                  "Epoch 18 train loss: 0.7770, eval loss 1.4259560108184814\n",
                  "Epoch 19 train loss: 0.6831, eval loss 1.4249225854873657\n",
                  "Epoch 20 train loss: 0.6499, eval loss 1.4277982711791992\n",
                  "Epoch 21 train loss: 0.6947, eval loss 1.4284847974777222\n",
                  "Epoch 22 train loss: 0.6292, eval loss 1.4268122911453247\n"
               ]
            }
         ],
         "source": [
            "# training loop\n",
            "# your_code\n",
            "\n",
            "\n",
            "model = NormalizingMLP(\n",
            "    input_size=X_train.shape[1], \n",
            "    dropout_p=dropout_p\n",
            ")\n",
            "\n",
            "optimizer = torch.optim.AdamW(\n",
            "    model.parameters(), \n",
            "    lr=learning_rate, \n",
            "    weight_decay=l2_reg\n",
            ")\n",
            "\n",
            "loss_fn = torch.nn.BCEWithLogitsLoss(\n",
            "    weight = torch.from_numpy(weights)[1]\n",
            ")\n",
            "\n",
            "train_dataset = MyDataset(X_train, y_train)\n",
            "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
            "\n",
            "steps_without_improvement = 0\n",
            "\n",
            "best_val_loss = np.inf\n",
            "best_model = None\n",
            "best_threshold = None\n",
            "\n",
            "for epoch_num in range(max_epochs):\n",
            "    model.train()\n",
            "\n",
            "    # note that we are using DataLoader to get batches\n",
            "    for X_batch, y_batch in train_dataloader:\n",
            "        # model training\n",
            "        outputs = model(X_batch)\n",
            "        loss = loss_fn(outputs, y_batch)\n",
            "\n",
            "        optimizer.zero_grad()\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "\n",
            "    # model evaluation, early stopping\n",
            "    \n",
            "    model.eval()\n",
            "\n",
            "    valid_metrics = evaluate_model(model, X_valid, y_valid, loss_fn)\n",
            "\n",
            "    if best_val_loss > valid_metrics['loss']:\n",
            "        steps_without_improvement = 0\n",
            "\n",
            "        best_model = deepcopy(model)\n",
            "        best_val_loss = valid_metrics['loss']\n",
            "        best_threshold = valid_metrics['threshold']\n",
            "\n",
            "    else:\n",
            "        steps_without_improvement += 1\n",
            "        \n",
            "        if steps_without_improvement == early_stopping_patience:\n",
            "            break\n",
            "    \n",
            "    print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 119,
         "metadata": {
            "scrolled": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "AUROC: 90.73%\n",
                  "F1: 69.41%\n",
                  "Precision: 64.84%\n",
                  "Recall: 74.68%\n"
               ]
            }
         ],
         "source": [
            "test_metrics = evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n",
            "\n",
            "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
            "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
            "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
            "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "XyoRnHT4GFR9"
         },
         "source": [
            "## Akceleracja sprzętowa (dla zainteresowanych)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Jak wcześniej wspominaliśmy, użycie akceleracji sprzętowej, czyli po prostu GPU do obliczeń, jest bardzo efektywne w przypadku sieci neuronowych. Karty graficzne bardzo efektywnie mnożą macierze, a sieci neuronowe to, jak można było się przekonać, dużo mnożenia macierzy.\n",
            "\n",
            "W PyTorchu jest to dosyć łatwe, ale trzeba robić to explicite. Służy do tego metoda `.to()`, która przenosi tensory między CPU i GPU. Poniżej przykład, jak to się robi (oczywiście trzeba mieć skonfigurowane GPU, żeby działało):"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import time \n",
            "\n",
            "\n",
            "class CudaMLP(nn.Module):\n",
            "    def __init__(self, input_size: int, dropout_p: float = 0.5):\n",
            "        super().__init__()\n",
            "\n",
            "        self.mlp = nn.Sequential(\n",
            "            nn.Linear(input_size, 512),\n",
            "            nn.BatchNorm1d(512),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "            nn.Linear(512, 256),\n",
            "            nn.BatchNorm1d(256),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "            nn.Linear(256, 256),\n",
            "            nn.BatchNorm1d(256),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "            nn.Linear(256, 128),\n",
            "            nn.BatchNorm1d(128),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "            nn.Linear(128, 1),\n",
            "        )\n",
            "    \n",
            "    def forward(self, x):\n",
            "        return self.mlp(x)\n",
            "\n",
            "    def predict_proba(self, x):\n",
            "        return sigmoid(self(x))\n",
            "    \n",
            "    def predict(self, x, threshold: float = 0.5):\n",
            "        y_pred_score = self.predict_proba(x)\n",
            "        return (y_pred_score > threshold).to(torch.int32)\n",
            "\n",
            "\n",
            "model = CudaMLP(X_train.shape[1]).to('cuda')\n",
            "\n",
            "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
            "\n",
            "# note that we are using loss function with sigmoid built in\n",
            "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(weights)[1].to('cuda'))\n",
            "\n",
            "step_counter = 0\n",
            "time_from_eval = time.time()\n",
            "for epoch_id in range(30):\n",
            "    for batch_x, batch_y in train_dataloader:\n",
            "        batch_x = batch_x.to('cuda')\n",
            "        batch_y = batch_y.to('cuda')\n",
            "        \n",
            "        loss = loss_fn(model(batch_x), batch_y)\n",
            "        loss.backward()\n",
            "\n",
            "        optimizer.step()\n",
            "        optimizer.zero_grad()\n",
            "        \n",
            "        if step_counter % evaluation_steps == 0:\n",
            "            print(f\"Epoch {epoch_id} train loss: {loss.item():.4f}, time: {time.time() - time_from_eval}\")\n",
            "            time_from_eval = time.time()\n",
            "\n",
            "        step_counter += 1\n",
            "\n",
            "test_res = evaluate_model(model.to('cpu'), X_test, y_test, loss_fn.to('cpu'), threshold=0.5)\n",
            "\n",
            "print(f\"AUROC: {100 * test_res['AUROC']:.2f}%\")\n",
            "print(f\"F1: {100 * test_res['F1-score']:.2f}%\")\n",
            "print(test_res)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Co prawda ten model nie będzie tak dobry jak ten z laboratorium, ale zwróć uwagę, o ile jest większy, a przy tym szybszy.\n",
            "\n",
            "Dla zainteresowanych polecamy [tę serie artykułów](https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Zadanie dla chętnych"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Jak widzieliśmy, sieci neuronowe mają bardzo dużo hiperparametrów. Przeszukiwanie ich grid search'em jest więc niewykonalne, a chociaż random search by działał, to potrzebowałby wielu iteracji, co też jest kosztowne obliczeniowo.\n",
            "\n",
            "Zaimplementuj inteligentne przeszukiwanie przestrzeni hiperparametrów za pomocą biblioteki [Optuna](https://optuna.org/). Implementuje ona między innymi algorytm Tree Parzen Estimator (TPE), należący do grupy algorytmów typu Bayesian search. Typowo osiągają one bardzo dobre wyniki, a właściwie zawsze lepsze od przeszukiwania losowego. Do tego wystarcza im często niewielka liczba kroków.\n",
            "\n",
            "Zaimplementuj 3-warstwową sieć MLP, gdzie pierwsza warstwa ma rozmiar ukryty N, a druga N // 2. Ucz ją optymalizatorem Adam przez maksymalnie 300 epok z cierpliwością 10.\n",
            "\n",
            "Przeszukaj wybrane zakresy dla hiperparametrów:\n",
            "- rozmiar warstw ukrytych (N)\n",
            "- stała ucząca\n",
            "- batch size\n",
            "- siła regularyzacji L2\n",
            "- prawdopodobieństwo dropoutu\n",
            "\n",
            "Wykorzystaj przynajmniej 30 iteracji. Następnie przełącz algorytm na losowy (Optuna także jego implementuje), wykonaj 30 iteracji i porównaj jakość wyników.\n",
            "\n",
            "Przydatne materiały:\n",
            "- [Optuna code examples - PyTorch](https://optuna.org/#code_examples)\n",
            "- [Auto-Tuning Hyperparameters with Optuna and PyTorch](https://www.youtube.com/watch?v=P6NwZVl8ttc)\n",
            "- [Hyperparameter Tuning of Neural Networks with Optuna and PyTorch](https://towardsdatascience.com/hyperparameter-tuning-of-neural-networks-with-optuna-and-pytorch-22e179efc837)\n",
            "- [Using Optuna to Optimize PyTorch Hyperparameters](https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 126,
         "metadata": {},
         "outputs": [],
         "source": [
            "class OptimizedMLP(nn.Module):\n",
            "    def __init__(self, input_size: int, dropout_p: float, hidden_layer_size: int):\n",
            "        super().__init__()\n",
            "\n",
            "        n = hidden_layer_size\n",
            "        n_half = hidden_layer_size // 2\n",
            "\n",
            "        self.transitions = nn.Sequential(            \n",
            "            nn.Linear(input_size, n),\n",
            "            nn.BatchNorm1d(n),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "            \n",
            "            nn.Linear(n, n_half),\n",
            "            nn.BatchNorm1d(n_half),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_p),\n",
            "            \n",
            "            nn.Linear(n_half, 1),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        return self.transitions(x)\n",
            "\n",
            "    def predict_proba(self, x):\n",
            "        return sigmoid(self(x))\n",
            "    \n",
            "    def predict(self, x, threshold: float = 0.5):\n",
            "        y_pred_score = self.predict_proba(x)\n",
            "        return (y_pred_score > threshold).to(torch.int32)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 162,
         "metadata": {},
         "outputs": [],
         "source": [
            "max_epochs = 150"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 163,
         "metadata": {},
         "outputs": [],
         "source": [
            "import optuna\n",
            "\n",
            "def define_model(trial):\n",
            "    hidden_layers_size = trial.suggest_int(\n",
            "        'hidden_layers_size', \n",
            "        X_train.shape[1] // 2, \n",
            "        2 * X_train.shape[1],\n",
            "        log = True\n",
            "    )\n",
            "\n",
            "    dropout_p = trial.suggest_float(\n",
            "        'dropout_p',\n",
            "        0.25,\n",
            "        0.5,\n",
            "        log = True\n",
            "    )\n",
            "\n",
            "    return OptimizedMLP(\n",
            "        input_size=X_train.shape[1], \n",
            "        dropout_p = dropout_p,\n",
            "        hidden_layer_size = hidden_layers_size\n",
            "    )\n",
            "\n",
            "def train(model, optimizer, loss_fn, batch_size):\n",
            "    global max_epochs, X_train, y_train, X_valid, y_valid\n",
            "    train_dataset = MyDataset(X_train, y_train)\n",
            "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
            "\n",
            "    steps_without_improvement = 0\n",
            "\n",
            "    best_val_loss = np.inf\n",
            "    best_model = None\n",
            "    best_threshold = None\n",
            "\n",
            "    for epoch_num in range(max_epochs):\n",
            "        model.train()\n",
            "\n",
            "        # note that we are using DataLoader to get batches\n",
            "        for X_batch, y_batch in train_dataloader:\n",
            "            # model training\n",
            "            outputs = model(X_batch)\n",
            "            loss = loss_fn(outputs, y_batch)\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        # model evaluation, early stopping\n",
            "        \n",
            "        model.eval()\n",
            "\n",
            "        valid_metrics = evaluate_model(model, X_valid, y_valid, loss_fn)\n",
            "\n",
            "        if best_val_loss > valid_metrics['loss']:\n",
            "            steps_without_improvement = 0\n",
            "\n",
            "            best_model = deepcopy(model)\n",
            "            best_val_loss = valid_metrics['loss']\n",
            "            best_threshold = valid_metrics['threshold']\n",
            "\n",
            "        else:\n",
            "            steps_without_improvement += 1\n",
            "            \n",
            "            if steps_without_improvement == early_stopping_patience:\n",
            "                break\n",
            "\n",
            "        print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")\n",
            "\n",
            "    return best_model, best_val_loss, best_threshold\n",
            "\n",
            "def train_optuna(model, optimizer, loss_fn, batch_size):\n",
            "    _, loss, _ = train(model, optimizer, loss_fn, batch_size)\n",
            "    return loss\n",
            "\n",
            "def objective(trial):\n",
            "    model = define_model(trial)\n",
            "\n",
            "    learning_rate = trial.suggest_float(\n",
            "        'learning_rate', \n",
            "        1e-6, \n",
            "        1e-1, \n",
            "        log = True\n",
            "    )\n",
            "\n",
            "    batch_size = trial.suggest_int(\n",
            "        'batch_size',\n",
            "        128,\n",
            "        512,\n",
            "        log = True\n",
            "    )\n",
            "\n",
            "    l2_reg = trial.suggest_float(\n",
            "        'l2_reg',\n",
            "        1e-5,\n",
            "        1e-3,\n",
            "        log = True\n",
            "    )\n",
            "\n",
            "    optimizer = torch.optim.AdamW(\n",
            "        model.parameters(), \n",
            "        lr=learning_rate, \n",
            "        weight_decay=l2_reg\n",
            "    )\n",
            "\n",
            "    weights = compute_class_weight(\n",
            "        \"balanced\",\n",
            "        classes = np.unique(y),\n",
            "        y = y\n",
            "    )\n",
            "\n",
            "    loss_fn = torch.nn.BCEWithLogitsLoss(\n",
            "        weight = torch.from_numpy(weights)[1]\n",
            "    )\n",
            "\n",
            "    return train_optuna(model, optimizer, loss_fn, batch_size)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 164,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:38:50,215] A new study created in memory with name: no-name-f220af00-cc84-4850-9f86-164e31e35988\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.6165, eval loss 1.4268232583999634\n",
                  "Epoch 1 train loss: 0.3124, eval loss 1.4239012002944946\n",
                  "Epoch 2 train loss: 0.3173, eval loss 1.4223229885101318\n",
                  "Epoch 3 train loss: 0.4300, eval loss 1.4196330308914185\n",
                  "Epoch 4 train loss: 0.3357, eval loss 1.42090904712677\n",
                  "Epoch 5 train loss: 0.3804, eval loss 1.4262138605117798\n",
                  "Epoch 6 train loss: 0.2801, eval loss 1.4227888584136963\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:38:55,868] Trial 0 finished with value: 1.4196330308914185 and parameters: {'hidden_layers_size': 185, 'dropout_p': 0.4068804799715802, 'learning_rate': 0.010978991823991393, 'batch_size': 336, 'l2_reg': 1.34641142059497e-05}. Best is trial 0 with value: 1.4196330308914185.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4006, eval loss 1.8089185953140259\n",
                  "Epoch 1 train loss: 1.3063, eval loss 1.8107423782348633\n",
                  "Epoch 2 train loss: 1.5072, eval loss 1.8080438375473022\n",
                  "Epoch 3 train loss: 1.2947, eval loss 1.8089221715927124\n",
                  "Epoch 4 train loss: 1.5810, eval loss 1.8088653087615967\n",
                  "Epoch 5 train loss: 1.2184, eval loss 1.803401231765747\n",
                  "Epoch 6 train loss: 1.5646, eval loss 1.8060569763183594\n",
                  "Epoch 7 train loss: 1.5708, eval loss 1.8051623106002808\n",
                  "Epoch 8 train loss: 1.2308, eval loss 1.805528163909912\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:39:04,094] Trial 1 finished with value: 1.803401231765747 and parameters: {'hidden_layers_size': 87, 'dropout_p': 0.489907983890005, 'learning_rate': 1.5461728272636294e-06, 'batch_size': 217, 'l2_reg': 3.202440992612983e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.0667, eval loss 1.639102816581726\n",
                  "Epoch 1 train loss: 0.9433, eval loss 1.5832183361053467\n",
                  "Epoch 2 train loss: 0.8702, eval loss 1.5496737957000732\n",
                  "Epoch 3 train loss: 0.7843, eval loss 1.5267010927200317\n",
                  "Epoch 4 train loss: 0.7905, eval loss 1.5078661441802979\n",
                  "Epoch 5 train loss: 0.7655, eval loss 1.4926570653915405\n",
                  "Epoch 6 train loss: 0.7302, eval loss 1.4817637205123901\n",
                  "Epoch 7 train loss: 0.7016, eval loss 1.4728914499282837\n",
                  "Epoch 8 train loss: 0.6590, eval loss 1.4652535915374756\n",
                  "Epoch 9 train loss: 0.6883, eval loss 1.4596012830734253\n",
                  "Epoch 10 train loss: 0.6854, eval loss 1.454770803451538\n",
                  "Epoch 11 train loss: 0.6550, eval loss 1.4515832662582397\n",
                  "Epoch 12 train loss: 0.6722, eval loss 1.4480564594268799\n",
                  "Epoch 13 train loss: 0.6607, eval loss 1.4452193975448608\n",
                  "Epoch 14 train loss: 0.6588, eval loss 1.4436118602752686\n",
                  "Epoch 15 train loss: 0.6329, eval loss 1.442274808883667\n",
                  "Epoch 16 train loss: 0.6586, eval loss 1.441723108291626\n",
                  "Epoch 17 train loss: 0.6220, eval loss 1.4393420219421387\n",
                  "Epoch 18 train loss: 0.6148, eval loss 1.4382113218307495\n",
                  "Epoch 19 train loss: 0.6167, eval loss 1.4390060901641846\n",
                  "Epoch 20 train loss: 0.6166, eval loss 1.4370743036270142\n",
                  "Epoch 21 train loss: 0.6414, eval loss 1.4359771013259888\n",
                  "Epoch 22 train loss: 0.6174, eval loss 1.4367042779922485\n",
                  "Epoch 23 train loss: 0.6275, eval loss 1.434579610824585\n",
                  "Epoch 24 train loss: 0.6210, eval loss 1.434201717376709\n",
                  "Epoch 25 train loss: 0.5864, eval loss 1.4344539642333984\n",
                  "Epoch 26 train loss: 0.6058, eval loss 1.4329931735992432\n",
                  "Epoch 27 train loss: 0.5602, eval loss 1.4331417083740234\n",
                  "Epoch 28 train loss: 0.5961, eval loss 1.433791160583496\n",
                  "Epoch 29 train loss: 0.6109, eval loss 1.4332058429718018\n",
                  "Epoch 30 train loss: 0.5723, eval loss 1.4322681427001953\n",
                  "Epoch 31 train loss: 0.5948, eval loss 1.4325464963912964\n",
                  "Epoch 32 train loss: 0.5785, eval loss 1.4316482543945312\n",
                  "Epoch 33 train loss: 0.5881, eval loss 1.431742548942566\n",
                  "Epoch 34 train loss: 0.5957, eval loss 1.432404637336731\n",
                  "Epoch 35 train loss: 0.6269, eval loss 1.4318515062332153\n",
                  "Epoch 36 train loss: 0.5890, eval loss 1.431180477142334\n",
                  "Epoch 37 train loss: 0.5957, eval loss 1.431395411491394\n",
                  "Epoch 38 train loss: 0.6278, eval loss 1.4301395416259766\n",
                  "Epoch 39 train loss: 0.5612, eval loss 1.4311411380767822\n",
                  "Epoch 40 train loss: 0.5937, eval loss 1.430301308631897\n",
                  "Epoch 41 train loss: 0.6031, eval loss 1.430803656578064\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:39:24,705] Trial 2 finished with value: 1.4301395416259766 and parameters: {'hidden_layers_size': 65, 'dropout_p': 0.2828372333521446, 'learning_rate': 0.0003532557894056364, 'batch_size': 480, 'l2_reg': 0.0009952730340921808}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.0391, eval loss 1.5901604890823364\n",
                  "Epoch 1 train loss: 1.0020, eval loss 1.5303393602371216\n",
                  "Epoch 2 train loss: 0.8757, eval loss 1.4990736246109009\n",
                  "Epoch 3 train loss: 0.7983, eval loss 1.481130838394165\n",
                  "Epoch 4 train loss: 0.8153, eval loss 1.4686423540115356\n",
                  "Epoch 5 train loss: 0.7615, eval loss 1.4606677293777466\n",
                  "Epoch 6 train loss: 0.7651, eval loss 1.4554810523986816\n",
                  "Epoch 7 train loss: 0.6799, eval loss 1.451284646987915\n",
                  "Epoch 8 train loss: 0.6614, eval loss 1.4473800659179688\n",
                  "Epoch 9 train loss: 0.6153, eval loss 1.444901466369629\n",
                  "Epoch 10 train loss: 0.6636, eval loss 1.4433597326278687\n",
                  "Epoch 11 train loss: 0.5631, eval loss 1.4416804313659668\n",
                  "Epoch 12 train loss: 0.5281, eval loss 1.4404723644256592\n",
                  "Epoch 13 train loss: 0.6120, eval loss 1.4387907981872559\n",
                  "Epoch 14 train loss: 0.6434, eval loss 1.4377154111862183\n",
                  "Epoch 15 train loss: 0.5691, eval loss 1.4362729787826538\n",
                  "Epoch 16 train loss: 0.5504, eval loss 1.4362157583236694\n",
                  "Epoch 17 train loss: 0.5421, eval loss 1.435915231704712\n",
                  "Epoch 18 train loss: 0.5252, eval loss 1.4360144138336182\n",
                  "Epoch 19 train loss: 0.5927, eval loss 1.4343713521957397\n",
                  "Epoch 20 train loss: 0.5174, eval loss 1.4345661401748657\n",
                  "Epoch 21 train loss: 0.5311, eval loss 1.4346139430999756\n",
                  "Epoch 22 train loss: 0.5057, eval loss 1.4338819980621338\n",
                  "Epoch 23 train loss: 0.5412, eval loss 1.4336930513381958\n",
                  "Epoch 24 train loss: 0.5581, eval loss 1.4332866668701172\n",
                  "Epoch 25 train loss: 0.4556, eval loss 1.4320528507232666\n",
                  "Epoch 26 train loss: 0.4795, eval loss 1.4322551488876343\n",
                  "Epoch 27 train loss: 0.3712, eval loss 1.4306191205978394\n",
                  "Epoch 28 train loss: 0.5136, eval loss 1.4311975240707397\n",
                  "Epoch 29 train loss: 0.4568, eval loss 1.4318902492523193\n",
                  "Epoch 30 train loss: 0.4184, eval loss 1.431145429611206\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:39:50,437] Trial 3 finished with value: 1.4306191205978394 and parameters: {'hidden_layers_size': 189, 'dropout_p': 0.335446125087379, 'learning_rate': 0.00018634702171506481, 'batch_size': 260, 'l2_reg': 6.065164275108682e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7101, eval loss 1.4648549556732178\n",
                  "Epoch 1 train loss: 0.6525, eval loss 1.4420949220657349\n",
                  "Epoch 2 train loss: 0.6222, eval loss 1.4417181015014648\n",
                  "Epoch 3 train loss: 0.6030, eval loss 1.4371815919876099\n",
                  "Epoch 4 train loss: 0.5852, eval loss 1.437233328819275\n",
                  "Epoch 5 train loss: 0.5928, eval loss 1.4368410110473633\n",
                  "Epoch 6 train loss: 0.5767, eval loss 1.436415672302246\n",
                  "Epoch 7 train loss: 0.6206, eval loss 1.434099555015564\n",
                  "Epoch 8 train loss: 0.5776, eval loss 1.4362107515335083\n",
                  "Epoch 9 train loss: 0.5927, eval loss 1.4324151277542114\n",
                  "Epoch 10 train loss: 0.5776, eval loss 1.433035135269165\n",
                  "Epoch 11 train loss: 0.5652, eval loss 1.4331684112548828\n",
                  "Epoch 12 train loss: 0.5829, eval loss 1.433249831199646\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:39:58,782] Trial 4 finished with value: 1.4324151277542114 and parameters: {'hidden_layers_size': 111, 'dropout_p': 0.3002039437138742, 'learning_rate': 0.0029727208396297736, 'batch_size': 479, 'l2_reg': 0.0004831397700544172}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.2200, eval loss 1.4228744506835938\n",
                  "Epoch 1 train loss: 1.5240, eval loss 1.424627661705017\n",
                  "Epoch 2 train loss: 0.5862, eval loss 1.4265384674072266\n",
                  "Epoch 3 train loss: 0.7615, eval loss 1.4238454103469849\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:40:02,430] Trial 5 finished with value: 1.4228744506835938 and parameters: {'hidden_layers_size': 100, 'dropout_p': 0.30626568843058916, 'learning_rate': 0.01473230680293548, 'batch_size': 251, 'l2_reg': 2.3166256377940516e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.6699, eval loss 1.8016992807388306\n",
                  "Epoch 1 train loss: 1.6497, eval loss 1.7856029272079468\n",
                  "Epoch 2 train loss: 1.5298, eval loss 1.7725859880447388\n",
                  "Epoch 3 train loss: 1.4894, eval loss 1.759587049484253\n",
                  "Epoch 4 train loss: 1.5360, eval loss 1.7461634874343872\n",
                  "Epoch 5 train loss: 1.4696, eval loss 1.735628366470337\n",
                  "Epoch 6 train loss: 1.4066, eval loss 1.7246735095977783\n",
                  "Epoch 7 train loss: 1.4880, eval loss 1.7139297723770142\n",
                  "Epoch 8 train loss: 1.3360, eval loss 1.7058454751968384\n",
                  "Epoch 9 train loss: 1.3030, eval loss 1.6972993612289429\n",
                  "Epoch 10 train loss: 1.2669, eval loss 1.688965082168579\n",
                  "Epoch 11 train loss: 1.2152, eval loss 1.6797707080841064\n",
                  "Epoch 12 train loss: 1.2954, eval loss 1.6718412637710571\n",
                  "Epoch 13 train loss: 1.3042, eval loss 1.6656986474990845\n",
                  "Epoch 14 train loss: 1.2156, eval loss 1.6593745946884155\n",
                  "Epoch 15 train loss: 1.1645, eval loss 1.6524603366851807\n",
                  "Epoch 16 train loss: 1.1380, eval loss 1.6486958265304565\n",
                  "Epoch 17 train loss: 1.1759, eval loss 1.6407350301742554\n",
                  "Epoch 18 train loss: 1.1640, eval loss 1.634558916091919\n",
                  "Epoch 19 train loss: 1.0818, eval loss 1.6285488605499268\n",
                  "Epoch 20 train loss: 1.1409, eval loss 1.622857928276062\n",
                  "Epoch 21 train loss: 1.0949, eval loss 1.618492603302002\n",
                  "Epoch 22 train loss: 1.0803, eval loss 1.6117292642593384\n",
                  "Epoch 23 train loss: 1.0637, eval loss 1.609265923500061\n",
                  "Epoch 24 train loss: 1.0803, eval loss 1.6050031185150146\n",
                  "Epoch 25 train loss: 1.0090, eval loss 1.6005512475967407\n",
                  "Epoch 26 train loss: 1.0586, eval loss 1.5938721895217896\n",
                  "Epoch 27 train loss: 0.9790, eval loss 1.5909181833267212\n",
                  "Epoch 28 train loss: 0.9927, eval loss 1.587903380393982\n",
                  "Epoch 29 train loss: 0.9975, eval loss 1.583985447883606\n",
                  "Epoch 30 train loss: 0.9600, eval loss 1.5794833898544312\n",
                  "Epoch 31 train loss: 0.9684, eval loss 1.5754896402359009\n",
                  "Epoch 32 train loss: 0.9574, eval loss 1.5719006061553955\n",
                  "Epoch 33 train loss: 0.9508, eval loss 1.567445993423462\n",
                  "Epoch 34 train loss: 0.9923, eval loss 1.5647518634796143\n",
                  "Epoch 35 train loss: 0.9737, eval loss 1.5604335069656372\n",
                  "Epoch 36 train loss: 0.8961, eval loss 1.5573360919952393\n",
                  "Epoch 37 train loss: 0.9108, eval loss 1.5550050735473633\n",
                  "Epoch 38 train loss: 0.9401, eval loss 1.5506829023361206\n",
                  "Epoch 39 train loss: 0.9177, eval loss 1.547379493713379\n",
                  "Epoch 40 train loss: 0.9231, eval loss 1.545825719833374\n",
                  "Epoch 41 train loss: 0.8827, eval loss 1.5431911945343018\n",
                  "Epoch 42 train loss: 0.9013, eval loss 1.5384799242019653\n",
                  "Epoch 43 train loss: 0.8497, eval loss 1.5374996662139893\n",
                  "Epoch 44 train loss: 0.8911, eval loss 1.5331547260284424\n",
                  "Epoch 45 train loss: 0.9135, eval loss 1.531075358390808\n",
                  "Epoch 46 train loss: 0.9234, eval loss 1.528848648071289\n",
                  "Epoch 47 train loss: 0.8796, eval loss 1.5257123708724976\n",
                  "Epoch 48 train loss: 0.8745, eval loss 1.5234793424606323\n",
                  "Epoch 49 train loss: 0.8856, eval loss 1.52005136013031\n",
                  "Epoch 50 train loss: 0.8124, eval loss 1.5172702074050903\n",
                  "Epoch 51 train loss: 0.8890, eval loss 1.516763687133789\n",
                  "Epoch 52 train loss: 0.7924, eval loss 1.5149588584899902\n",
                  "Epoch 53 train loss: 0.8400, eval loss 1.512819528579712\n",
                  "Epoch 54 train loss: 0.8502, eval loss 1.5106505155563354\n",
                  "Epoch 55 train loss: 0.8398, eval loss 1.5081615447998047\n",
                  "Epoch 56 train loss: 0.9005, eval loss 1.5058271884918213\n",
                  "Epoch 57 train loss: 0.7846, eval loss 1.5043561458587646\n",
                  "Epoch 58 train loss: 0.7754, eval loss 1.5018864870071411\n",
                  "Epoch 59 train loss: 0.7727, eval loss 1.5000813007354736\n",
                  "Epoch 60 train loss: 0.7975, eval loss 1.49947988986969\n",
                  "Epoch 61 train loss: 0.7536, eval loss 1.4966392517089844\n",
                  "Epoch 62 train loss: 0.7543, eval loss 1.4946093559265137\n",
                  "Epoch 63 train loss: 0.7961, eval loss 1.4929084777832031\n",
                  "Epoch 64 train loss: 0.8572, eval loss 1.491731882095337\n",
                  "Epoch 65 train loss: 0.7491, eval loss 1.489554762840271\n",
                  "Epoch 66 train loss: 0.7930, eval loss 1.4880799055099487\n",
                  "Epoch 67 train loss: 0.7957, eval loss 1.4872444868087769\n",
                  "Epoch 68 train loss: 0.7758, eval loss 1.4841591119766235\n",
                  "Epoch 69 train loss: 0.7258, eval loss 1.4841735363006592\n",
                  "Epoch 70 train loss: 0.7812, eval loss 1.4828404188156128\n",
                  "Epoch 71 train loss: 0.7518, eval loss 1.480738639831543\n",
                  "Epoch 72 train loss: 0.7439, eval loss 1.480164885520935\n",
                  "Epoch 73 train loss: 0.7682, eval loss 1.4779590368270874\n",
                  "Epoch 74 train loss: 0.7093, eval loss 1.4767645597457886\n",
                  "Epoch 75 train loss: 0.7767, eval loss 1.4758574962615967\n",
                  "Epoch 76 train loss: 0.7658, eval loss 1.474073052406311\n",
                  "Epoch 77 train loss: 0.7690, eval loss 1.4729076623916626\n",
                  "Epoch 78 train loss: 0.8140, eval loss 1.4731075763702393\n",
                  "Epoch 79 train loss: 0.7225, eval loss 1.4719288349151611\n",
                  "Epoch 80 train loss: 0.7645, eval loss 1.4698455333709717\n",
                  "Epoch 81 train loss: 0.7477, eval loss 1.4690333604812622\n",
                  "Epoch 82 train loss: 0.7604, eval loss 1.4681764841079712\n",
                  "Epoch 83 train loss: 0.6761, eval loss 1.4683641195297241\n",
                  "Epoch 84 train loss: 0.7650, eval loss 1.467284917831421\n",
                  "Epoch 85 train loss: 0.8347, eval loss 1.466341257095337\n",
                  "Epoch 86 train loss: 0.8314, eval loss 1.4636174440383911\n",
                  "Epoch 87 train loss: 0.7387, eval loss 1.4640828371047974\n",
                  "Epoch 88 train loss: 0.7881, eval loss 1.463499665260315\n",
                  "Epoch 89 train loss: 0.7606, eval loss 1.462046504020691\n",
                  "Epoch 90 train loss: 0.8032, eval loss 1.4616669416427612\n",
                  "Epoch 91 train loss: 0.7557, eval loss 1.4607808589935303\n",
                  "Epoch 92 train loss: 0.7102, eval loss 1.4612168073654175\n",
                  "Epoch 93 train loss: 0.7275, eval loss 1.4596590995788574\n",
                  "Epoch 94 train loss: 0.7650, eval loss 1.459149956703186\n",
                  "Epoch 95 train loss: 0.7271, eval loss 1.4587626457214355\n",
                  "Epoch 96 train loss: 0.7520, eval loss 1.4579203128814697\n",
                  "Epoch 97 train loss: 0.8317, eval loss 1.4561989307403564\n",
                  "Epoch 98 train loss: 0.7345, eval loss 1.4564491510391235\n",
                  "Epoch 99 train loss: 0.6612, eval loss 1.4558125734329224\n",
                  "Epoch 100 train loss: 0.6806, eval loss 1.4550824165344238\n",
                  "Epoch 101 train loss: 0.8046, eval loss 1.4551901817321777\n",
                  "Epoch 102 train loss: 0.7548, eval loss 1.4549497365951538\n",
                  "Epoch 103 train loss: 0.7179, eval loss 1.4545493125915527\n",
                  "Epoch 104 train loss: 0.7267, eval loss 1.4537991285324097\n",
                  "Epoch 105 train loss: 0.7203, eval loss 1.4534190893173218\n",
                  "Epoch 106 train loss: 0.6919, eval loss 1.4522143602371216\n",
                  "Epoch 107 train loss: 0.6610, eval loss 1.451969861984253\n",
                  "Epoch 108 train loss: 0.7182, eval loss 1.4516774415969849\n",
                  "Epoch 109 train loss: 0.6822, eval loss 1.4513895511627197\n",
                  "Epoch 110 train loss: 0.6786, eval loss 1.4515290260314941\n",
                  "Epoch 111 train loss: 0.6311, eval loss 1.4504663944244385\n",
                  "Epoch 112 train loss: 0.7588, eval loss 1.4511898756027222\n",
                  "Epoch 113 train loss: 0.7873, eval loss 1.4506075382232666\n",
                  "Epoch 114 train loss: 0.7808, eval loss 1.4493201971054077\n",
                  "Epoch 115 train loss: 0.7201, eval loss 1.4499626159667969\n",
                  "Epoch 116 train loss: 0.7099, eval loss 1.4492477178573608\n",
                  "Epoch 117 train loss: 0.7099, eval loss 1.4484987258911133\n",
                  "Epoch 118 train loss: 0.6836, eval loss 1.448036789894104\n",
                  "Epoch 119 train loss: 0.7526, eval loss 1.4487088918685913\n",
                  "Epoch 120 train loss: 0.6991, eval loss 1.4479246139526367\n",
                  "Epoch 121 train loss: 0.6833, eval loss 1.4469679594039917\n",
                  "Epoch 122 train loss: 0.8528, eval loss 1.4476804733276367\n",
                  "Epoch 123 train loss: 0.7155, eval loss 1.4475998878479004\n",
                  "Epoch 124 train loss: 0.7075, eval loss 1.4464120864868164\n",
                  "Epoch 125 train loss: 0.7285, eval loss 1.4462188482284546\n",
                  "Epoch 126 train loss: 0.7296, eval loss 1.446033000946045\n",
                  "Epoch 127 train loss: 0.6743, eval loss 1.446428894996643\n",
                  "Epoch 128 train loss: 0.7866, eval loss 1.445650577545166\n",
                  "Epoch 129 train loss: 0.7161, eval loss 1.445623517036438\n",
                  "Epoch 130 train loss: 0.7108, eval loss 1.44606351852417\n",
                  "Epoch 131 train loss: 0.7697, eval loss 1.4451706409454346\n",
                  "Epoch 132 train loss: 0.7739, eval loss 1.4458459615707397\n",
                  "Epoch 133 train loss: 0.6980, eval loss 1.4458417892456055\n",
                  "Epoch 134 train loss: 0.7386, eval loss 1.4452126026153564\n",
                  "Epoch 135 train loss: 0.6964, eval loss 1.4445562362670898\n",
                  "Epoch 136 train loss: 0.7420, eval loss 1.4444639682769775\n",
                  "Epoch 137 train loss: 0.7708, eval loss 1.4445425271987915\n",
                  "Epoch 138 train loss: 0.6912, eval loss 1.4447767734527588\n",
                  "Epoch 139 train loss: 0.7671, eval loss 1.443927526473999\n",
                  "Epoch 140 train loss: 0.7424, eval loss 1.443878412246704\n",
                  "Epoch 141 train loss: 0.6911, eval loss 1.4440687894821167\n",
                  "Epoch 142 train loss: 0.6826, eval loss 1.443906307220459\n",
                  "Epoch 143 train loss: 0.6962, eval loss 1.4429848194122314\n",
                  "Epoch 144 train loss: 0.7127, eval loss 1.4434078931808472\n",
                  "Epoch 145 train loss: 0.7231, eval loss 1.442967176437378\n",
                  "Epoch 146 train loss: 0.7263, eval loss 1.4430404901504517\n",
                  "Epoch 147 train loss: 0.7582, eval loss 1.4426188468933105\n",
                  "Epoch 148 train loss: 0.6586, eval loss 1.4420490264892578\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:42:11,962] Trial 6 finished with value: 1.4420490264892578 and parameters: {'hidden_layers_size': 84, 'dropout_p': 0.355192902719554, 'learning_rate': 1.1856994355524345e-05, 'batch_size': 147, 'l2_reg': 1.1533970697156625e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.7004, eval loss 1.4422955513000488\n",
                  "Epoch 0 train loss: 0.7592, eval loss 1.4392565488815308\n",
                  "Epoch 1 train loss: 0.7097, eval loss 1.4312855005264282\n",
                  "Epoch 2 train loss: 0.6643, eval loss 1.427862286567688\n",
                  "Epoch 3 train loss: 0.6484, eval loss 1.4265140295028687\n",
                  "Epoch 4 train loss: 0.6531, eval loss 1.4262874126434326\n",
                  "Epoch 5 train loss: 0.7326, eval loss 1.4252033233642578\n",
                  "Epoch 6 train loss: 0.6856, eval loss 1.4260015487670898\n",
                  "Epoch 7 train loss: 0.6636, eval loss 1.4266576766967773\n",
                  "Epoch 8 train loss: 0.6856, eval loss 1.425023078918457\n",
                  "Epoch 9 train loss: 0.5903, eval loss 1.4265354871749878\n",
                  "Epoch 10 train loss: 0.6423, eval loss 1.4254704713821411\n",
                  "Epoch 11 train loss: 0.6942, eval loss 1.4237393140792847\n",
                  "Epoch 12 train loss: 0.6342, eval loss 1.4245840311050415\n",
                  "Epoch 13 train loss: 0.5949, eval loss 1.4243565797805786\n",
                  "Epoch 14 train loss: 0.6283, eval loss 1.4231367111206055\n",
                  "Epoch 15 train loss: 0.6178, eval loss 1.4227485656738281\n",
                  "Epoch 16 train loss: 0.7032, eval loss 1.4238309860229492\n",
                  "Epoch 17 train loss: 0.5609, eval loss 1.4228121042251587\n",
                  "Epoch 18 train loss: 0.6001, eval loss 1.4235873222351074\n",
                  "Epoch 19 train loss: 0.6500, eval loss 1.4222842454910278\n",
                  "Epoch 20 train loss: 0.6411, eval loss 1.4223191738128662\n",
                  "Epoch 21 train loss: 0.6135, eval loss 1.42316734790802\n",
                  "Epoch 22 train loss: 0.5872, eval loss 1.422120213508606\n",
                  "Epoch 23 train loss: 0.5283, eval loss 1.4248380661010742\n",
                  "Epoch 24 train loss: 0.4965, eval loss 1.4259033203125\n",
                  "Epoch 25 train loss: 0.5722, eval loss 1.4222320318222046\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:42:43,105] Trial 7 finished with value: 1.422120213508606 and parameters: {'hidden_layers_size': 135, 'dropout_p': 0.2877508594801334, 'learning_rate': 0.0012921275464269674, 'batch_size': 134, 'l2_reg': 0.00010354605429446324}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.6664, eval loss 1.4368209838867188\n",
                  "Epoch 1 train loss: 0.6912, eval loss 1.4383376836776733\n",
                  "Epoch 2 train loss: 0.6330, eval loss 1.439626932144165\n",
                  "Epoch 3 train loss: 0.6815, eval loss 1.4426182508468628\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:42:46,498] Trial 8 finished with value: 1.4368209838867188 and parameters: {'hidden_layers_size': 57, 'dropout_p': 0.39520546062488676, 'learning_rate': 0.04739320270422495, 'batch_size': 299, 'l2_reg': 0.00027050363398514217}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7169, eval loss 1.437097430229187\n",
                  "Epoch 1 train loss: 0.6942, eval loss 1.4335131645202637\n",
                  "Epoch 2 train loss: 0.6211, eval loss 1.4369632005691528\n",
                  "Epoch 3 train loss: 0.5548, eval loss 1.432682752609253\n",
                  "Epoch 4 train loss: 0.4627, eval loss 1.4263136386871338\n",
                  "Epoch 5 train loss: 0.4237, eval loss 1.4300023317337036\n",
                  "Epoch 6 train loss: 0.4951, eval loss 1.4350719451904297\n",
                  "Epoch 7 train loss: 0.4578, eval loss 1.4267882108688354\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:42:52,393] Trial 9 finished with value: 1.4263136386871338 and parameters: {'hidden_layers_size': 54, 'dropout_p': 0.27146235362128895, 'learning_rate': 0.033780962695344606, 'batch_size': 416, 'l2_reg': 0.0005398007559151257}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.6318, eval loss 1.7973930835723877\n",
                  "Epoch 1 train loss: 1.6002, eval loss 1.796926736831665\n",
                  "Epoch 2 train loss: 1.6157, eval loss 1.7940939664840698\n",
                  "Epoch 3 train loss: 1.5813, eval loss 1.7941765785217285\n",
                  "Epoch 4 train loss: 1.6756, eval loss 1.7914142608642578\n",
                  "Epoch 5 train loss: 1.6057, eval loss 1.792375087738037\n",
                  "Epoch 6 train loss: 1.6146, eval loss 1.7913492918014526\n",
                  "Epoch 7 train loss: 1.5977, eval loss 1.7900804281234741\n",
                  "Epoch 8 train loss: 1.5971, eval loss 1.7896268367767334\n",
                  "Epoch 9 train loss: 1.5961, eval loss 1.7872806787490845\n",
                  "Epoch 10 train loss: 1.5491, eval loss 1.7876583337783813\n",
                  "Epoch 11 train loss: 1.5600, eval loss 1.786561131477356\n",
                  "Epoch 12 train loss: 1.5939, eval loss 1.7874130010604858\n",
                  "Epoch 13 train loss: 1.5272, eval loss 1.7850030660629272\n",
                  "Epoch 14 train loss: 1.5095, eval loss 1.7855640649795532\n",
                  "Epoch 15 train loss: 1.5599, eval loss 1.78235924243927\n",
                  "Epoch 16 train loss: 1.6034, eval loss 1.7819253206253052\n",
                  "Epoch 17 train loss: 1.4992, eval loss 1.7817376852035522\n",
                  "Epoch 18 train loss: 1.5742, eval loss 1.7811191082000732\n",
                  "Epoch 19 train loss: 1.5720, eval loss 1.7803540229797363\n",
                  "Epoch 20 train loss: 1.5188, eval loss 1.7784830331802368\n",
                  "Epoch 21 train loss: 1.6284, eval loss 1.7795604467391968\n",
                  "Epoch 22 train loss: 1.5497, eval loss 1.7764678001403809\n",
                  "Epoch 23 train loss: 1.5303, eval loss 1.778303861618042\n",
                  "Epoch 24 train loss: 1.4943, eval loss 1.7758839130401611\n",
                  "Epoch 25 train loss: 1.4996, eval loss 1.7742345333099365\n",
                  "Epoch 26 train loss: 1.5342, eval loss 1.7746669054031372\n",
                  "Epoch 27 train loss: 1.4758, eval loss 1.7741087675094604\n",
                  "Epoch 28 train loss: 1.4156, eval loss 1.772607684135437\n",
                  "Epoch 29 train loss: 1.5492, eval loss 1.7711381912231445\n",
                  "Epoch 30 train loss: 1.5378, eval loss 1.7713180780410767\n",
                  "Epoch 31 train loss: 1.5496, eval loss 1.7711883783340454\n",
                  "Epoch 32 train loss: 1.4927, eval loss 1.7704815864562988\n",
                  "Epoch 33 train loss: 1.5266, eval loss 1.7678653001785278\n",
                  "Epoch 34 train loss: 1.5215, eval loss 1.7679048776626587\n",
                  "Epoch 35 train loss: 1.5421, eval loss 1.7671693563461304\n",
                  "Epoch 36 train loss: 1.5486, eval loss 1.7651704549789429\n",
                  "Epoch 37 train loss: 1.5192, eval loss 1.765358805656433\n",
                  "Epoch 38 train loss: 1.4675, eval loss 1.7650904655456543\n",
                  "Epoch 39 train loss: 1.5190, eval loss 1.7628147602081299\n",
                  "Epoch 40 train loss: 1.5934, eval loss 1.7614825963974\n",
                  "Epoch 41 train loss: 1.4542, eval loss 1.7622405290603638\n",
                  "Epoch 42 train loss: 1.4441, eval loss 1.7608944177627563\n",
                  "Epoch 43 train loss: 1.4920, eval loss 1.760528326034546\n",
                  "Epoch 44 train loss: 1.5279, eval loss 1.7580983638763428\n",
                  "Epoch 45 train loss: 1.5449, eval loss 1.7592554092407227\n",
                  "Epoch 46 train loss: 1.3923, eval loss 1.759053349494934\n",
                  "Epoch 47 train loss: 1.4729, eval loss 1.7577433586120605\n",
                  "Epoch 48 train loss: 1.5021, eval loss 1.756988525390625\n",
                  "Epoch 49 train loss: 1.4489, eval loss 1.7551144361495972\n",
                  "Epoch 50 train loss: 1.4601, eval loss 1.7563542127609253\n",
                  "Epoch 51 train loss: 1.4878, eval loss 1.7548794746398926\n",
                  "Epoch 52 train loss: 1.4664, eval loss 1.7538247108459473\n",
                  "Epoch 53 train loss: 1.4402, eval loss 1.7525420188903809\n",
                  "Epoch 54 train loss: 1.4746, eval loss 1.7524384260177612\n",
                  "Epoch 55 train loss: 1.4872, eval loss 1.7505831718444824\n",
                  "Epoch 56 train loss: 1.4231, eval loss 1.749868631362915\n",
                  "Epoch 57 train loss: 1.4093, eval loss 1.7489101886749268\n",
                  "Epoch 58 train loss: 1.4167, eval loss 1.7483218908309937\n",
                  "Epoch 59 train loss: 1.4568, eval loss 1.7466069459915161\n",
                  "Epoch 60 train loss: 1.3651, eval loss 1.746770977973938\n",
                  "Epoch 61 train loss: 1.4520, eval loss 1.7469158172607422\n",
                  "Epoch 62 train loss: 1.3957, eval loss 1.7458343505859375\n",
                  "Epoch 63 train loss: 1.4053, eval loss 1.7469652891159058\n",
                  "Epoch 64 train loss: 1.4768, eval loss 1.7457165718078613\n",
                  "Epoch 65 train loss: 1.4467, eval loss 1.7443095445632935\n",
                  "Epoch 66 train loss: 1.4575, eval loss 1.7419847249984741\n",
                  "Epoch 67 train loss: 1.4021, eval loss 1.7421622276306152\n",
                  "Epoch 68 train loss: 1.4200, eval loss 1.740753173828125\n",
                  "Epoch 69 train loss: 1.4054, eval loss 1.7415751218795776\n",
                  "Epoch 70 train loss: 1.3760, eval loss 1.74049711227417\n",
                  "Epoch 71 train loss: 1.4248, eval loss 1.7410472631454468\n",
                  "Epoch 72 train loss: 1.4468, eval loss 1.739323377609253\n",
                  "Epoch 73 train loss: 1.3298, eval loss 1.7385668754577637\n",
                  "Epoch 74 train loss: 1.4285, eval loss 1.736103892326355\n",
                  "Epoch 75 train loss: 1.3471, eval loss 1.736605167388916\n",
                  "Epoch 76 train loss: 1.3971, eval loss 1.737483263015747\n",
                  "Epoch 77 train loss: 1.3560, eval loss 1.7349069118499756\n",
                  "Epoch 78 train loss: 1.3214, eval loss 1.7355058193206787\n",
                  "Epoch 79 train loss: 1.4128, eval loss 1.7343909740447998\n",
                  "Epoch 80 train loss: 1.3944, eval loss 1.7330044507980347\n",
                  "Epoch 81 train loss: 1.3977, eval loss 1.7321697473526\n",
                  "Epoch 82 train loss: 1.3650, eval loss 1.7324614524841309\n",
                  "Epoch 83 train loss: 1.3644, eval loss 1.7293366193771362\n",
                  "Epoch 84 train loss: 1.3753, eval loss 1.7310136556625366\n",
                  "Epoch 85 train loss: 1.3253, eval loss 1.7307064533233643\n",
                  "Epoch 86 train loss: 1.3737, eval loss 1.7284531593322754\n",
                  "Epoch 87 train loss: 1.3638, eval loss 1.7275105714797974\n",
                  "Epoch 88 train loss: 1.3547, eval loss 1.7277889251708984\n",
                  "Epoch 89 train loss: 1.3452, eval loss 1.727121114730835\n",
                  "Epoch 90 train loss: 1.4130, eval loss 1.7255934476852417\n",
                  "Epoch 91 train loss: 1.3998, eval loss 1.7256163358688354\n",
                  "Epoch 92 train loss: 1.3902, eval loss 1.7251936197280884\n",
                  "Epoch 93 train loss: 1.3286, eval loss 1.7231608629226685\n",
                  "Epoch 94 train loss: 1.2983, eval loss 1.7228481769561768\n",
                  "Epoch 95 train loss: 1.3632, eval loss 1.7228000164031982\n",
                  "Epoch 96 train loss: 1.3349, eval loss 1.7226454019546509\n",
                  "Epoch 97 train loss: 1.4063, eval loss 1.7206428050994873\n",
                  "Epoch 98 train loss: 1.3292, eval loss 1.720682144165039\n",
                  "Epoch 99 train loss: 1.3508, eval loss 1.7198140621185303\n",
                  "Epoch 100 train loss: 1.3305, eval loss 1.7198797464370728\n",
                  "Epoch 101 train loss: 1.3111, eval loss 1.7182456254959106\n",
                  "Epoch 102 train loss: 1.3268, eval loss 1.7169724702835083\n",
                  "Epoch 103 train loss: 1.3537, eval loss 1.7180862426757812\n",
                  "Epoch 104 train loss: 1.2488, eval loss 1.7180230617523193\n",
                  "Epoch 105 train loss: 1.3810, eval loss 1.7174699306488037\n",
                  "Epoch 106 train loss: 1.3200, eval loss 1.7149995565414429\n",
                  "Epoch 107 train loss: 1.3049, eval loss 1.713407039642334\n",
                  "Epoch 108 train loss: 1.3186, eval loss 1.713971734046936\n",
                  "Epoch 109 train loss: 1.2934, eval loss 1.7134974002838135\n",
                  "Epoch 110 train loss: 1.2997, eval loss 1.7138559818267822\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:44:28,580] Trial 10 finished with value: 1.713407039642334 and parameters: {'hidden_layers_size': 78, 'dropout_p': 0.4910729724376275, 'learning_rate': 1.1621568817979394e-06, 'batch_size': 185, 'l2_reg': 3.8426680434817866e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.3505, eval loss 1.7458510398864746\n",
                  "Epoch 1 train loss: 1.4506, eval loss 1.7476296424865723\n",
                  "Epoch 2 train loss: 1.6246, eval loss 1.7434031963348389\n",
                  "Epoch 3 train loss: 1.3902, eval loss 1.7433992624282837\n",
                  "Epoch 4 train loss: 1.2796, eval loss 1.742552399635315\n",
                  "Epoch 5 train loss: 1.5304, eval loss 1.7440767288208008\n",
                  "Epoch 6 train loss: 1.3807, eval loss 1.7423486709594727\n",
                  "Epoch 7 train loss: 1.4468, eval loss 1.7437301874160767\n",
                  "Epoch 8 train loss: 1.3772, eval loss 1.7398512363433838\n",
                  "Epoch 9 train loss: 1.3458, eval loss 1.7365080118179321\n",
                  "Epoch 10 train loss: 1.4339, eval loss 1.738708257675171\n",
                  "Epoch 11 train loss: 1.4199, eval loss 1.73784601688385\n",
                  "Epoch 12 train loss: 1.3294, eval loss 1.7347745895385742\n",
                  "Epoch 13 train loss: 1.3824, eval loss 1.7365102767944336\n",
                  "Epoch 14 train loss: 1.3253, eval loss 1.7370673418045044\n",
                  "Epoch 15 train loss: 1.4182, eval loss 1.7333968877792358\n",
                  "Epoch 16 train loss: 1.4185, eval loss 1.7358115911483765\n",
                  "Epoch 17 train loss: 1.3776, eval loss 1.7329570055007935\n",
                  "Epoch 18 train loss: 1.5551, eval loss 1.73142409324646\n",
                  "Epoch 19 train loss: 1.3110, eval loss 1.730339765548706\n",
                  "Epoch 20 train loss: 1.5562, eval loss 1.7303712368011475\n",
                  "Epoch 21 train loss: 1.2186, eval loss 1.7308940887451172\n",
                  "Epoch 22 train loss: 1.2656, eval loss 1.7297781705856323\n",
                  "Epoch 23 train loss: 1.2852, eval loss 1.7293438911437988\n",
                  "Epoch 24 train loss: 1.3581, eval loss 1.7263926267623901\n",
                  "Epoch 25 train loss: 1.4185, eval loss 1.7252845764160156\n",
                  "Epoch 26 train loss: 1.3750, eval loss 1.7269313335418701\n",
                  "Epoch 27 train loss: 1.4366, eval loss 1.7244364023208618\n",
                  "Epoch 28 train loss: 1.2807, eval loss 1.724387764930725\n",
                  "Epoch 29 train loss: 1.3855, eval loss 1.7210564613342285\n",
                  "Epoch 30 train loss: 1.3996, eval loss 1.7229435443878174\n",
                  "Epoch 31 train loss: 1.3354, eval loss 1.7207221984863281\n",
                  "Epoch 32 train loss: 1.3122, eval loss 1.7196489572525024\n",
                  "Epoch 33 train loss: 1.3369, eval loss 1.718311071395874\n",
                  "Epoch 34 train loss: 1.3726, eval loss 1.717880129814148\n",
                  "Epoch 35 train loss: 1.3721, eval loss 1.7201381921768188\n",
                  "Epoch 36 train loss: 1.3861, eval loss 1.7154780626296997\n",
                  "Epoch 37 train loss: 1.4021, eval loss 1.717442274093628\n",
                  "Epoch 38 train loss: 1.2420, eval loss 1.7137306928634644\n",
                  "Epoch 39 train loss: 1.4700, eval loss 1.716137409210205\n",
                  "Epoch 40 train loss: 1.2427, eval loss 1.7144205570220947\n",
                  "Epoch 41 train loss: 1.2774, eval loss 1.7137333154678345\n",
                  "Epoch 42 train loss: 1.2292, eval loss 1.7122368812561035\n",
                  "Epoch 43 train loss: 1.2016, eval loss 1.7116039991378784\n",
                  "Epoch 44 train loss: 1.2712, eval loss 1.711971402168274\n",
                  "Epoch 45 train loss: 1.2904, eval loss 1.7107570171356201\n",
                  "Epoch 46 train loss: 1.2438, eval loss 1.7108920812606812\n",
                  "Epoch 47 train loss: 1.3075, eval loss 1.7105425596237183\n",
                  "Epoch 48 train loss: 1.2767, eval loss 1.7091178894042969\n",
                  "Epoch 49 train loss: 1.3638, eval loss 1.7075575590133667\n",
                  "Epoch 50 train loss: 1.3835, eval loss 1.705471396446228\n",
                  "Epoch 51 train loss: 1.2842, eval loss 1.7054766416549683\n",
                  "Epoch 52 train loss: 1.2966, eval loss 1.7026890516281128\n",
                  "Epoch 53 train loss: 1.1752, eval loss 1.7045233249664307\n",
                  "Epoch 54 train loss: 1.3497, eval loss 1.70432710647583\n",
                  "Epoch 55 train loss: 1.3437, eval loss 1.7040495872497559\n",
                  "Epoch 56 train loss: 1.3222, eval loss 1.7019009590148926\n",
                  "Epoch 57 train loss: 1.2845, eval loss 1.701688289642334\n",
                  "Epoch 58 train loss: 1.3356, eval loss 1.7043287754058838\n",
                  "Epoch 59 train loss: 1.3652, eval loss 1.69980788230896\n",
                  "Epoch 60 train loss: 1.2849, eval loss 1.7001556158065796\n",
                  "Epoch 61 train loss: 1.2543, eval loss 1.6975685358047485\n",
                  "Epoch 62 train loss: 1.4672, eval loss 1.698020100593567\n",
                  "Epoch 63 train loss: 1.4363, eval loss 1.6979811191558838\n",
                  "Epoch 64 train loss: 1.2959, eval loss 1.7002378702163696\n",
                  "Epoch 65 train loss: 1.2403, eval loss 1.6966832876205444\n",
                  "Epoch 66 train loss: 1.1722, eval loss 1.6946477890014648\n",
                  "Epoch 67 train loss: 1.2036, eval loss 1.69508695602417\n",
                  "Epoch 68 train loss: 1.2053, eval loss 1.693541407585144\n",
                  "Epoch 69 train loss: 1.2420, eval loss 1.6958141326904297\n",
                  "Epoch 70 train loss: 1.2227, eval loss 1.692574143409729\n",
                  "Epoch 71 train loss: 1.2002, eval loss 1.6918610334396362\n",
                  "Epoch 72 train loss: 1.2226, eval loss 1.691811442375183\n",
                  "Epoch 73 train loss: 1.1923, eval loss 1.6906547546386719\n",
                  "Epoch 74 train loss: 1.1325, eval loss 1.6919276714324951\n",
                  "Epoch 75 train loss: 1.2392, eval loss 1.6914095878601074\n",
                  "Epoch 76 train loss: 1.3771, eval loss 1.6875888109207153\n",
                  "Epoch 77 train loss: 1.2845, eval loss 1.6868525743484497\n",
                  "Epoch 78 train loss: 1.2786, eval loss 1.6871377229690552\n",
                  "Epoch 79 train loss: 1.3281, eval loss 1.6858446598052979\n",
                  "Epoch 80 train loss: 1.2435, eval loss 1.685321569442749\n",
                  "Epoch 81 train loss: 1.1295, eval loss 1.6863970756530762\n",
                  "Epoch 82 train loss: 1.2788, eval loss 1.6862276792526245\n",
                  "Epoch 83 train loss: 1.2405, eval loss 1.684199333190918\n",
                  "Epoch 84 train loss: 1.1968, eval loss 1.6829135417938232\n",
                  "Epoch 85 train loss: 1.3198, eval loss 1.683470606803894\n",
                  "Epoch 86 train loss: 1.3817, eval loss 1.6802082061767578\n",
                  "Epoch 87 train loss: 1.2060, eval loss 1.6802618503570557\n",
                  "Epoch 88 train loss: 1.3056, eval loss 1.6804393529891968\n",
                  "Epoch 89 train loss: 1.2474, eval loss 1.679996371269226\n",
                  "Epoch 90 train loss: 1.1881, eval loss 1.6810623407363892\n",
                  "Epoch 91 train loss: 1.1684, eval loss 1.6768361330032349\n",
                  "Epoch 92 train loss: 1.2729, eval loss 1.6786175966262817\n",
                  "Epoch 93 train loss: 1.1260, eval loss 1.6776829957962036\n",
                  "Epoch 94 train loss: 1.1712, eval loss 1.6760671138763428\n",
                  "Epoch 95 train loss: 1.3693, eval loss 1.6771034002304077\n",
                  "Epoch 96 train loss: 1.1372, eval loss 1.674038290977478\n",
                  "Epoch 97 train loss: 1.1997, eval loss 1.6748460531234741\n",
                  "Epoch 98 train loss: 1.2216, eval loss 1.6716227531433105\n",
                  "Epoch 99 train loss: 1.1177, eval loss 1.6759041547775269\n",
                  "Epoch 100 train loss: 1.1722, eval loss 1.674620270729065\n",
                  "Epoch 101 train loss: 1.2552, eval loss 1.6731897592544556\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:45:39,739] Trial 11 finished with value: 1.6716227531433105 and parameters: {'hidden_layers_size': 79, 'dropout_p': 0.4995540527585774, 'learning_rate': 1.0013372175504955e-06, 'batch_size': 189, 'l2_reg': 5.62465192894945e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4351, eval loss 1.761287808418274\n",
                  "Epoch 1 train loss: 1.4471, eval loss 1.761607050895691\n",
                  "Epoch 2 train loss: 1.4363, eval loss 1.7590199708938599\n",
                  "Epoch 3 train loss: 1.4894, eval loss 1.757539987564087\n",
                  "Epoch 4 train loss: 1.4315, eval loss 1.7598501443862915\n",
                  "Epoch 5 train loss: 1.4426, eval loss 1.7580764293670654\n",
                  "Epoch 6 train loss: 1.4760, eval loss 1.7573615312576294\n",
                  "Epoch 7 train loss: 1.4877, eval loss 1.7588859796524048\n",
                  "Epoch 8 train loss: 1.5256, eval loss 1.7556606531143188\n",
                  "Epoch 9 train loss: 1.4199, eval loss 1.7555103302001953\n",
                  "Epoch 10 train loss: 1.5000, eval loss 1.7545783519744873\n",
                  "Epoch 11 train loss: 1.3404, eval loss 1.7536920309066772\n",
                  "Epoch 12 train loss: 1.3763, eval loss 1.7520778179168701\n",
                  "Epoch 13 train loss: 1.3986, eval loss 1.752812147140503\n",
                  "Epoch 14 train loss: 1.4000, eval loss 1.7498860359191895\n",
                  "Epoch 15 train loss: 1.3531, eval loss 1.7502676248550415\n",
                  "Epoch 16 train loss: 1.4340, eval loss 1.749920129776001\n",
                  "Epoch 17 train loss: 1.4970, eval loss 1.7505730390548706\n",
                  "Epoch 18 train loss: 1.3575, eval loss 1.74783456325531\n",
                  "Epoch 19 train loss: 1.4830, eval loss 1.7483608722686768\n",
                  "Epoch 20 train loss: 1.3039, eval loss 1.7467058897018433\n",
                  "Epoch 21 train loss: 1.4072, eval loss 1.7474030256271362\n",
                  "Epoch 22 train loss: 1.4720, eval loss 1.7461462020874023\n",
                  "Epoch 23 train loss: 1.4379, eval loss 1.7448270320892334\n",
                  "Epoch 24 train loss: 1.4008, eval loss 1.743793249130249\n",
                  "Epoch 25 train loss: 1.3590, eval loss 1.7443376779556274\n",
                  "Epoch 26 train loss: 1.3005, eval loss 1.7442824840545654\n",
                  "Epoch 27 train loss: 1.3634, eval loss 1.7430570125579834\n",
                  "Epoch 28 train loss: 1.2442, eval loss 1.7428765296936035\n",
                  "Epoch 29 train loss: 1.4280, eval loss 1.7415416240692139\n",
                  "Epoch 30 train loss: 1.3837, eval loss 1.7418056726455688\n",
                  "Epoch 31 train loss: 1.4474, eval loss 1.7385265827178955\n",
                  "Epoch 32 train loss: 1.3989, eval loss 1.7394555807113647\n",
                  "Epoch 33 train loss: 1.2642, eval loss 1.7383168935775757\n",
                  "Epoch 34 train loss: 1.3049, eval loss 1.7371768951416016\n",
                  "Epoch 35 train loss: 1.3699, eval loss 1.737854242324829\n",
                  "Epoch 36 train loss: 1.3563, eval loss 1.7381001710891724\n",
                  "Epoch 37 train loss: 1.2987, eval loss 1.7356367111206055\n",
                  "Epoch 38 train loss: 1.3300, eval loss 1.7355055809020996\n",
                  "Epoch 39 train loss: 1.3311, eval loss 1.7339335680007935\n",
                  "Epoch 40 train loss: 1.3201, eval loss 1.7335104942321777\n",
                  "Epoch 41 train loss: 1.2952, eval loss 1.732022762298584\n",
                  "Epoch 42 train loss: 1.3087, eval loss 1.7319821119308472\n",
                  "Epoch 43 train loss: 1.2110, eval loss 1.7304447889328003\n",
                  "Epoch 44 train loss: 1.3069, eval loss 1.7315504550933838\n",
                  "Epoch 45 train loss: 1.3305, eval loss 1.7307186126708984\n",
                  "Epoch 46 train loss: 1.3635, eval loss 1.7305670976638794\n",
                  "Epoch 47 train loss: 1.3415, eval loss 1.730050802230835\n",
                  "Epoch 48 train loss: 1.4243, eval loss 1.7277729511260986\n",
                  "Epoch 49 train loss: 1.3559, eval loss 1.7278177738189697\n",
                  "Epoch 50 train loss: 1.2816, eval loss 1.7266758680343628\n",
                  "Epoch 51 train loss: 1.3194, eval loss 1.727379322052002\n",
                  "Epoch 52 train loss: 1.3032, eval loss 1.7240732908248901\n",
                  "Epoch 53 train loss: 1.2477, eval loss 1.72511625289917\n",
                  "Epoch 54 train loss: 1.3922, eval loss 1.724213719367981\n",
                  "Epoch 55 train loss: 1.3916, eval loss 1.724128246307373\n",
                  "Epoch 56 train loss: 1.2865, eval loss 1.7228811979293823\n",
                  "Epoch 57 train loss: 1.4285, eval loss 1.7206062078475952\n",
                  "Epoch 58 train loss: 1.2544, eval loss 1.7210521697998047\n",
                  "Epoch 59 train loss: 1.2823, eval loss 1.7197610139846802\n",
                  "Epoch 60 train loss: 1.2526, eval loss 1.720039963722229\n",
                  "Epoch 61 train loss: 1.2844, eval loss 1.719326138496399\n",
                  "Epoch 62 train loss: 1.3054, eval loss 1.7178844213485718\n",
                  "Epoch 63 train loss: 1.2517, eval loss 1.7170679569244385\n",
                  "Epoch 64 train loss: 1.2875, eval loss 1.7186298370361328\n",
                  "Epoch 65 train loss: 1.2543, eval loss 1.7184104919433594\n",
                  "Epoch 66 train loss: 1.3352, eval loss 1.716409683227539\n",
                  "Epoch 67 train loss: 1.2876, eval loss 1.7156418561935425\n",
                  "Epoch 68 train loss: 1.2509, eval loss 1.7154865264892578\n",
                  "Epoch 69 train loss: 1.2747, eval loss 1.714583396911621\n",
                  "Epoch 70 train loss: 1.3125, eval loss 1.7127432823181152\n",
                  "Epoch 71 train loss: 1.3916, eval loss 1.7133691310882568\n",
                  "Epoch 72 train loss: 1.3135, eval loss 1.7129219770431519\n",
                  "Epoch 73 train loss: 1.2665, eval loss 1.7120834589004517\n",
                  "Epoch 74 train loss: 1.2683, eval loss 1.7113864421844482\n",
                  "Epoch 75 train loss: 1.2960, eval loss 1.711060881614685\n",
                  "Epoch 76 train loss: 1.2527, eval loss 1.7090235948562622\n",
                  "Epoch 77 train loss: 1.2639, eval loss 1.7092739343643188\n",
                  "Epoch 78 train loss: 1.3089, eval loss 1.7080578804016113\n",
                  "Epoch 79 train loss: 1.2854, eval loss 1.7079075574874878\n",
                  "Epoch 80 train loss: 1.3227, eval loss 1.707171082496643\n",
                  "Epoch 81 train loss: 1.3429, eval loss 1.7074683904647827\n",
                  "Epoch 82 train loss: 1.3047, eval loss 1.7056899070739746\n",
                  "Epoch 83 train loss: 1.2550, eval loss 1.7067654132843018\n",
                  "Epoch 84 train loss: 1.2925, eval loss 1.7048020362854004\n",
                  "Epoch 85 train loss: 1.2673, eval loss 1.704369068145752\n",
                  "Epoch 86 train loss: 1.3661, eval loss 1.7038103342056274\n",
                  "Epoch 87 train loss: 1.2136, eval loss 1.7040363550186157\n",
                  "Epoch 88 train loss: 1.2812, eval loss 1.702325701713562\n",
                  "Epoch 89 train loss: 1.2227, eval loss 1.7008440494537354\n",
                  "Epoch 90 train loss: 1.2067, eval loss 1.7016080617904663\n",
                  "Epoch 91 train loss: 1.2631, eval loss 1.6991788148880005\n",
                  "Epoch 92 train loss: 1.2403, eval loss 1.6997325420379639\n",
                  "Epoch 93 train loss: 1.2203, eval loss 1.6980122327804565\n",
                  "Epoch 94 train loss: 1.2176, eval loss 1.7000105381011963\n",
                  "Epoch 95 train loss: 1.3053, eval loss 1.6989327669143677\n",
                  "Epoch 96 train loss: 1.2480, eval loss 1.6956533193588257\n",
                  "Epoch 97 train loss: 1.2072, eval loss 1.6964199542999268\n",
                  "Epoch 98 train loss: 1.1480, eval loss 1.6953544616699219\n",
                  "Epoch 99 train loss: 1.1836, eval loss 1.6960166692733765\n",
                  "Epoch 100 train loss: 1.3271, eval loss 1.6934962272644043\n",
                  "Epoch 101 train loss: 1.2714, eval loss 1.6963269710540771\n",
                  "Epoch 102 train loss: 1.2290, eval loss 1.695198893547058\n",
                  "Epoch 103 train loss: 1.2258, eval loss 1.693738341331482\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:46:42,914] Trial 12 finished with value: 1.6934962272644043 and parameters: {'hidden_layers_size': 71, 'dropout_p': 0.4939836605682455, 'learning_rate': 1.033644192116278e-06, 'batch_size': 194, 'l2_reg': 2.9209239251884415e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.5979, eval loss 1.7865238189697266\n",
                  "Epoch 1 train loss: 1.5293, eval loss 1.7772608995437622\n",
                  "Epoch 2 train loss: 1.4897, eval loss 1.7661323547363281\n",
                  "Epoch 3 train loss: 1.4827, eval loss 1.7574682235717773\n",
                  "Epoch 4 train loss: 1.4255, eval loss 1.7513375282287598\n",
                  "Epoch 5 train loss: 1.4739, eval loss 1.743520975112915\n",
                  "Epoch 6 train loss: 1.3635, eval loss 1.7355067729949951\n",
                  "Epoch 7 train loss: 1.3755, eval loss 1.7291876077651978\n",
                  "Epoch 8 train loss: 1.3983, eval loss 1.7203670740127563\n",
                  "Epoch 9 train loss: 1.3532, eval loss 1.7151226997375488\n",
                  "Epoch 10 train loss: 1.2783, eval loss 1.7068841457366943\n",
                  "Epoch 11 train loss: 1.1985, eval loss 1.7004923820495605\n",
                  "Epoch 12 train loss: 1.1922, eval loss 1.6932398080825806\n",
                  "Epoch 13 train loss: 1.2779, eval loss 1.6890958547592163\n",
                  "Epoch 14 train loss: 1.2338, eval loss 1.6832895278930664\n",
                  "Epoch 15 train loss: 1.2380, eval loss 1.6787375211715698\n",
                  "Epoch 16 train loss: 1.1496, eval loss 1.6729170083999634\n",
                  "Epoch 17 train loss: 1.1784, eval loss 1.6660369634628296\n",
                  "Epoch 18 train loss: 1.1861, eval loss 1.6624172925949097\n",
                  "Epoch 19 train loss: 1.1903, eval loss 1.657949686050415\n",
                  "Epoch 20 train loss: 1.1716, eval loss 1.6532809734344482\n",
                  "Epoch 21 train loss: 1.2003, eval loss 1.647663950920105\n",
                  "Epoch 22 train loss: 1.1315, eval loss 1.6444920301437378\n",
                  "Epoch 23 train loss: 1.1169, eval loss 1.641392707824707\n",
                  "Epoch 24 train loss: 1.1295, eval loss 1.6363179683685303\n",
                  "Epoch 25 train loss: 1.0966, eval loss 1.633334755897522\n",
                  "Epoch 26 train loss: 1.1513, eval loss 1.6276742219924927\n",
                  "Epoch 27 train loss: 1.1390, eval loss 1.625372290611267\n",
                  "Epoch 28 train loss: 1.2161, eval loss 1.6239700317382812\n",
                  "Epoch 29 train loss: 1.0697, eval loss 1.6215112209320068\n",
                  "Epoch 30 train loss: 1.0418, eval loss 1.617396354675293\n",
                  "Epoch 31 train loss: 1.0354, eval loss 1.6137460470199585\n",
                  "Epoch 32 train loss: 1.0471, eval loss 1.609309196472168\n",
                  "Epoch 33 train loss: 1.0816, eval loss 1.606711983680725\n",
                  "Epoch 34 train loss: 1.0804, eval loss 1.6040585041046143\n",
                  "Epoch 35 train loss: 1.0420, eval loss 1.6012146472930908\n",
                  "Epoch 36 train loss: 1.1040, eval loss 1.5975583791732788\n",
                  "Epoch 37 train loss: 1.0448, eval loss 1.5956238508224487\n",
                  "Epoch 38 train loss: 1.0171, eval loss 1.5940293073654175\n",
                  "Epoch 39 train loss: 1.1101, eval loss 1.591147780418396\n",
                  "Epoch 40 train loss: 1.0024, eval loss 1.5860683917999268\n",
                  "Epoch 41 train loss: 0.9821, eval loss 1.5860459804534912\n",
                  "Epoch 42 train loss: 0.9773, eval loss 1.582424283027649\n",
                  "Epoch 43 train loss: 1.0530, eval loss 1.580909252166748\n",
                  "Epoch 44 train loss: 0.9995, eval loss 1.5795150995254517\n",
                  "Epoch 45 train loss: 0.9694, eval loss 1.5742011070251465\n",
                  "Epoch 46 train loss: 1.0338, eval loss 1.5737743377685547\n",
                  "Epoch 47 train loss: 1.0271, eval loss 1.5722020864486694\n",
                  "Epoch 48 train loss: 0.9304, eval loss 1.5685055255889893\n",
                  "Epoch 49 train loss: 0.9320, eval loss 1.5662522315979004\n",
                  "Epoch 50 train loss: 1.0306, eval loss 1.5628975629806519\n",
                  "Epoch 51 train loss: 0.9847, eval loss 1.5623382329940796\n",
                  "Epoch 52 train loss: 0.9575, eval loss 1.558984637260437\n",
                  "Epoch 53 train loss: 0.9078, eval loss 1.558192491531372\n",
                  "Epoch 54 train loss: 0.9979, eval loss 1.555570363998413\n",
                  "Epoch 55 train loss: 0.8812, eval loss 1.5551342964172363\n",
                  "Epoch 56 train loss: 1.0197, eval loss 1.5520116090774536\n",
                  "Epoch 57 train loss: 0.9612, eval loss 1.5499016046524048\n",
                  "Epoch 58 train loss: 0.9199, eval loss 1.5482462644577026\n",
                  "Epoch 59 train loss: 0.9243, eval loss 1.5463765859603882\n",
                  "Epoch 60 train loss: 0.9523, eval loss 1.5446351766586304\n",
                  "Epoch 61 train loss: 0.9589, eval loss 1.5436676740646362\n",
                  "Epoch 62 train loss: 0.8840, eval loss 1.5412722826004028\n",
                  "Epoch 63 train loss: 0.9819, eval loss 1.539108157157898\n",
                  "Epoch 64 train loss: 0.9430, eval loss 1.5389245748519897\n",
                  "Epoch 65 train loss: 1.0038, eval loss 1.5375940799713135\n",
                  "Epoch 66 train loss: 0.9421, eval loss 1.5343068838119507\n",
                  "Epoch 67 train loss: 0.9135, eval loss 1.5334248542785645\n",
                  "Epoch 68 train loss: 0.8620, eval loss 1.5299453735351562\n",
                  "Epoch 69 train loss: 0.9479, eval loss 1.5289050340652466\n",
                  "Epoch 70 train loss: 0.9173, eval loss 1.5293138027191162\n",
                  "Epoch 71 train loss: 0.9008, eval loss 1.526592493057251\n",
                  "Epoch 72 train loss: 0.9147, eval loss 1.5254038572311401\n",
                  "Epoch 73 train loss: 0.8544, eval loss 1.5236001014709473\n",
                  "Epoch 74 train loss: 0.9020, eval loss 1.5220133066177368\n",
                  "Epoch 75 train loss: 0.8937, eval loss 1.5202906131744385\n",
                  "Epoch 76 train loss: 0.8318, eval loss 1.519127368927002\n",
                  "Epoch 77 train loss: 0.8777, eval loss 1.519373893737793\n",
                  "Epoch 78 train loss: 0.8601, eval loss 1.515852689743042\n",
                  "Epoch 79 train loss: 0.8919, eval loss 1.5144373178482056\n",
                  "Epoch 80 train loss: 0.8808, eval loss 1.5144680738449097\n",
                  "Epoch 81 train loss: 0.8398, eval loss 1.5131278038024902\n",
                  "Epoch 82 train loss: 0.8601, eval loss 1.5123475790023804\n",
                  "Epoch 83 train loss: 0.8143, eval loss 1.5108237266540527\n",
                  "Epoch 84 train loss: 0.9048, eval loss 1.5092633962631226\n",
                  "Epoch 85 train loss: 0.8932, eval loss 1.5066826343536377\n",
                  "Epoch 86 train loss: 0.8821, eval loss 1.5050040483474731\n",
                  "Epoch 87 train loss: 0.8432, eval loss 1.5048326253890991\n",
                  "Epoch 88 train loss: 0.8851, eval loss 1.5046372413635254\n",
                  "Epoch 89 train loss: 0.8567, eval loss 1.5018326044082642\n",
                  "Epoch 90 train loss: 0.8153, eval loss 1.5008121728897095\n",
                  "Epoch 91 train loss: 0.8539, eval loss 1.5000094175338745\n",
                  "Epoch 92 train loss: 0.7984, eval loss 1.4989523887634277\n",
                  "Epoch 93 train loss: 0.8652, eval loss 1.4985556602478027\n",
                  "Epoch 94 train loss: 0.8772, eval loss 1.4986541271209717\n",
                  "Epoch 95 train loss: 0.8976, eval loss 1.4974499940872192\n",
                  "Epoch 96 train loss: 0.7906, eval loss 1.4949486255645752\n",
                  "Epoch 97 train loss: 0.8525, eval loss 1.4930477142333984\n",
                  "Epoch 98 train loss: 0.8176, eval loss 1.4929308891296387\n",
                  "Epoch 99 train loss: 0.8532, eval loss 1.4918595552444458\n",
                  "Epoch 100 train loss: 0.8524, eval loss 1.4911363124847412\n",
                  "Epoch 101 train loss: 0.8484, eval loss 1.4901211261749268\n",
                  "Epoch 102 train loss: 0.8938, eval loss 1.490060567855835\n",
                  "Epoch 103 train loss: 0.8946, eval loss 1.488224983215332\n",
                  "Epoch 104 train loss: 0.7640, eval loss 1.486849069595337\n",
                  "Epoch 105 train loss: 0.7742, eval loss 1.4867533445358276\n",
                  "Epoch 106 train loss: 0.8322, eval loss 1.4851016998291016\n",
                  "Epoch 107 train loss: 0.7784, eval loss 1.4856820106506348\n",
                  "Epoch 108 train loss: 0.7765, eval loss 1.484365463256836\n",
                  "Epoch 109 train loss: 0.7758, eval loss 1.4838199615478516\n",
                  "Epoch 110 train loss: 0.8535, eval loss 1.4822626113891602\n",
                  "Epoch 111 train loss: 0.8390, eval loss 1.4815537929534912\n",
                  "Epoch 112 train loss: 0.8151, eval loss 1.481494665145874\n",
                  "Epoch 113 train loss: 0.8264, eval loss 1.4802186489105225\n",
                  "Epoch 114 train loss: 0.8083, eval loss 1.4800745248794556\n",
                  "Epoch 115 train loss: 0.8013, eval loss 1.4786161184310913\n",
                  "Epoch 116 train loss: 0.8135, eval loss 1.4775835275650024\n",
                  "Epoch 117 train loss: 0.8061, eval loss 1.4767924547195435\n",
                  "Epoch 118 train loss: 0.8322, eval loss 1.4770387411117554\n",
                  "Epoch 119 train loss: 0.8288, eval loss 1.4754292964935303\n",
                  "Epoch 120 train loss: 0.8560, eval loss 1.4755836725234985\n",
                  "Epoch 121 train loss: 0.8552, eval loss 1.4755884408950806\n",
                  "Epoch 122 train loss: 0.7452, eval loss 1.4743075370788574\n",
                  "Epoch 123 train loss: 0.7554, eval loss 1.4737391471862793\n",
                  "Epoch 124 train loss: 0.8413, eval loss 1.4719481468200684\n",
                  "Epoch 125 train loss: 0.8141, eval loss 1.472381830215454\n",
                  "Epoch 126 train loss: 0.8207, eval loss 1.470941185951233\n",
                  "Epoch 127 train loss: 0.7711, eval loss 1.4714840650558472\n",
                  "Epoch 128 train loss: 0.7860, eval loss 1.4697911739349365\n",
                  "Epoch 129 train loss: 0.8717, eval loss 1.4710230827331543\n",
                  "Epoch 130 train loss: 0.7858, eval loss 1.4693732261657715\n",
                  "Epoch 131 train loss: 0.7640, eval loss 1.4687607288360596\n",
                  "Epoch 132 train loss: 0.7678, eval loss 1.4679826498031616\n",
                  "Epoch 133 train loss: 0.8071, eval loss 1.4680536985397339\n",
                  "Epoch 134 train loss: 0.8178, eval loss 1.4688305854797363\n",
                  "Epoch 135 train loss: 0.8636, eval loss 1.4664438962936401\n",
                  "Epoch 136 train loss: 0.7595, eval loss 1.4667885303497314\n",
                  "Epoch 137 train loss: 0.7187, eval loss 1.4650890827178955\n",
                  "Epoch 138 train loss: 0.8307, eval loss 1.4663695096969604\n",
                  "Epoch 139 train loss: 0.8079, eval loss 1.4643263816833496\n",
                  "Epoch 140 train loss: 0.8590, eval loss 1.4644792079925537\n",
                  "Epoch 141 train loss: 0.7710, eval loss 1.4636268615722656\n",
                  "Epoch 142 train loss: 0.7749, eval loss 1.463363766670227\n",
                  "Epoch 143 train loss: 0.8352, eval loss 1.4630954265594482\n",
                  "Epoch 144 train loss: 0.8989, eval loss 1.4625409841537476\n",
                  "Epoch 145 train loss: 0.7482, eval loss 1.4625217914581299\n",
                  "Epoch 146 train loss: 0.7564, eval loss 1.4621177911758423\n",
                  "Epoch 147 train loss: 0.8020, eval loss 1.4621050357818604\n",
                  "Epoch 148 train loss: 0.8799, eval loss 1.4605149030685425\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:48:15,033] Trial 13 finished with value: 1.4605149030685425 and parameters: {'hidden_layers_size': 90, 'dropout_p': 0.4494963209683946, 'learning_rate': 9.15472107702422e-06, 'batch_size': 192, 'l2_reg': 3.1114172681127265e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.8082, eval loss 1.4607032537460327\n",
                  "Epoch 0 train loss: 1.4237, eval loss 1.777103066444397\n",
                  "Epoch 1 train loss: 1.4358, eval loss 1.7700467109680176\n",
                  "Epoch 2 train loss: 1.6049, eval loss 1.7624844312667847\n",
                  "Epoch 3 train loss: 1.5080, eval loss 1.7560837268829346\n",
                  "Epoch 4 train loss: 1.4703, eval loss 1.7485874891281128\n",
                  "Epoch 5 train loss: 1.4186, eval loss 1.7424554824829102\n",
                  "Epoch 6 train loss: 1.3537, eval loss 1.7363533973693848\n",
                  "Epoch 7 train loss: 1.1912, eval loss 1.731629729270935\n",
                  "Epoch 8 train loss: 1.3146, eval loss 1.7257176637649536\n",
                  "Epoch 9 train loss: 1.4118, eval loss 1.7181777954101562\n",
                  "Epoch 10 train loss: 1.2648, eval loss 1.714492678642273\n",
                  "Epoch 11 train loss: 1.4052, eval loss 1.7104334831237793\n",
                  "Epoch 12 train loss: 1.2246, eval loss 1.7051151990890503\n",
                  "Epoch 13 train loss: 1.2678, eval loss 1.6993274688720703\n",
                  "Epoch 14 train loss: 1.2306, eval loss 1.6943269968032837\n",
                  "Epoch 15 train loss: 1.1662, eval loss 1.690213918685913\n",
                  "Epoch 16 train loss: 1.2703, eval loss 1.6855360269546509\n",
                  "Epoch 17 train loss: 1.2143, eval loss 1.6807312965393066\n",
                  "Epoch 18 train loss: 1.2966, eval loss 1.676662564277649\n",
                  "Epoch 19 train loss: 1.1441, eval loss 1.6704189777374268\n",
                  "Epoch 20 train loss: 1.2290, eval loss 1.6678937673568726\n",
                  "Epoch 21 train loss: 1.2431, eval loss 1.6638339757919312\n",
                  "Epoch 22 train loss: 1.1357, eval loss 1.6620343923568726\n",
                  "Epoch 23 train loss: 1.1017, eval loss 1.6576647758483887\n",
                  "Epoch 24 train loss: 1.0888, eval loss 1.6535063982009888\n",
                  "Epoch 25 train loss: 1.1190, eval loss 1.6525182723999023\n",
                  "Epoch 26 train loss: 1.0484, eval loss 1.6470320224761963\n",
                  "Epoch 27 train loss: 1.0649, eval loss 1.643450140953064\n",
                  "Epoch 28 train loss: 1.1371, eval loss 1.6417078971862793\n",
                  "Epoch 29 train loss: 1.0988, eval loss 1.6361172199249268\n",
                  "Epoch 30 train loss: 1.0769, eval loss 1.634699821472168\n",
                  "Epoch 31 train loss: 1.1106, eval loss 1.631257176399231\n",
                  "Epoch 32 train loss: 1.0587, eval loss 1.628818392753601\n",
                  "Epoch 33 train loss: 1.0024, eval loss 1.6253771781921387\n",
                  "Epoch 34 train loss: 1.0911, eval loss 1.6245386600494385\n",
                  "Epoch 35 train loss: 1.1371, eval loss 1.6212596893310547\n",
                  "Epoch 36 train loss: 1.1361, eval loss 1.6174402236938477\n",
                  "Epoch 37 train loss: 1.0348, eval loss 1.6170555353164673\n",
                  "Epoch 38 train loss: 1.0073, eval loss 1.615592360496521\n",
                  "Epoch 39 train loss: 1.0951, eval loss 1.6095614433288574\n",
                  "Epoch 40 train loss: 1.0397, eval loss 1.6067982912063599\n",
                  "Epoch 41 train loss: 0.9433, eval loss 1.6056832075119019\n",
                  "Epoch 42 train loss: 1.0469, eval loss 1.6022584438323975\n",
                  "Epoch 43 train loss: 0.9492, eval loss 1.5998170375823975\n",
                  "Epoch 44 train loss: 1.0256, eval loss 1.5978983640670776\n",
                  "Epoch 45 train loss: 0.8691, eval loss 1.59601891040802\n",
                  "Epoch 46 train loss: 1.0154, eval loss 1.595819354057312\n",
                  "Epoch 47 train loss: 0.9590, eval loss 1.5923306941986084\n",
                  "Epoch 48 train loss: 0.8769, eval loss 1.586993932723999\n",
                  "Epoch 49 train loss: 0.9101, eval loss 1.5882596969604492\n",
                  "Epoch 50 train loss: 0.8967, eval loss 1.5857056379318237\n",
                  "Epoch 51 train loss: 0.9798, eval loss 1.5829983949661255\n",
                  "Epoch 52 train loss: 0.8781, eval loss 1.5807148218154907\n",
                  "Epoch 53 train loss: 0.9715, eval loss 1.5788425207138062\n",
                  "Epoch 54 train loss: 0.8283, eval loss 1.5789821147918701\n",
                  "Epoch 55 train loss: 1.0438, eval loss 1.5755155086517334\n",
                  "Epoch 56 train loss: 0.8452, eval loss 1.5748435258865356\n",
                  "Epoch 57 train loss: 0.9326, eval loss 1.5727996826171875\n",
                  "Epoch 58 train loss: 0.9198, eval loss 1.5702824592590332\n",
                  "Epoch 59 train loss: 0.9260, eval loss 1.568302869796753\n",
                  "Epoch 60 train loss: 0.8860, eval loss 1.5667767524719238\n",
                  "Epoch 61 train loss: 0.8801, eval loss 1.5642907619476318\n",
                  "Epoch 62 train loss: 0.8526, eval loss 1.5621488094329834\n",
                  "Epoch 63 train loss: 0.8840, eval loss 1.5639396905899048\n",
                  "Epoch 64 train loss: 0.8742, eval loss 1.5607701539993286\n",
                  "Epoch 65 train loss: 0.9383, eval loss 1.5579394102096558\n",
                  "Epoch 66 train loss: 0.8528, eval loss 1.5580507516860962\n",
                  "Epoch 67 train loss: 0.9320, eval loss 1.5562126636505127\n",
                  "Epoch 68 train loss: 0.9271, eval loss 1.5542418956756592\n",
                  "Epoch 69 train loss: 0.9171, eval loss 1.5514477491378784\n",
                  "Epoch 70 train loss: 0.8424, eval loss 1.5505280494689941\n",
                  "Epoch 71 train loss: 0.8727, eval loss 1.5509737730026245\n",
                  "Epoch 72 train loss: 0.8511, eval loss 1.5477274656295776\n",
                  "Epoch 73 train loss: 0.8192, eval loss 1.546722173690796\n",
                  "Epoch 74 train loss: 0.7661, eval loss 1.5445858240127563\n",
                  "Epoch 75 train loss: 0.8355, eval loss 1.5441217422485352\n",
                  "Epoch 76 train loss: 0.8017, eval loss 1.5418823957443237\n",
                  "Epoch 77 train loss: 0.9625, eval loss 1.5408167839050293\n",
                  "Epoch 78 train loss: 0.8532, eval loss 1.54034423828125\n",
                  "Epoch 79 train loss: 0.8291, eval loss 1.5392173528671265\n",
                  "Epoch 80 train loss: 0.8109, eval loss 1.5389947891235352\n",
                  "Epoch 81 train loss: 0.8542, eval loss 1.5368883609771729\n",
                  "Epoch 82 train loss: 0.8599, eval loss 1.5339899063110352\n",
                  "Epoch 83 train loss: 0.8260, eval loss 1.5326883792877197\n",
                  "Epoch 84 train loss: 0.8132, eval loss 1.5322308540344238\n",
                  "Epoch 85 train loss: 0.7468, eval loss 1.5290535688400269\n",
                  "Epoch 86 train loss: 0.8787, eval loss 1.5305120944976807\n",
                  "Epoch 87 train loss: 0.8571, eval loss 1.5271912813186646\n",
                  "Epoch 88 train loss: 0.8117, eval loss 1.5257799625396729\n",
                  "Epoch 89 train loss: 0.9218, eval loss 1.526273250579834\n",
                  "Epoch 90 train loss: 0.8597, eval loss 1.52469003200531\n",
                  "Epoch 91 train loss: 0.9020, eval loss 1.5230276584625244\n",
                  "Epoch 92 train loss: 0.8364, eval loss 1.5215665102005005\n",
                  "Epoch 93 train loss: 0.7579, eval loss 1.520590901374817\n",
                  "Epoch 94 train loss: 0.7479, eval loss 1.5199294090270996\n",
                  "Epoch 95 train loss: 0.8117, eval loss 1.5196789503097534\n",
                  "Epoch 96 train loss: 0.7521, eval loss 1.5172911882400513\n",
                  "Epoch 97 train loss: 0.8742, eval loss 1.5154327154159546\n",
                  "Epoch 98 train loss: 0.8338, eval loss 1.5153981447219849\n",
                  "Epoch 99 train loss: 0.8909, eval loss 1.5144089460372925\n",
                  "Epoch 100 train loss: 0.7712, eval loss 1.5142958164215088\n",
                  "Epoch 101 train loss: 0.7706, eval loss 1.5129897594451904\n",
                  "Epoch 102 train loss: 0.8447, eval loss 1.5120501518249512\n",
                  "Epoch 103 train loss: 0.7352, eval loss 1.5095841884613037\n",
                  "Epoch 104 train loss: 0.8015, eval loss 1.5078082084655762\n",
                  "Epoch 105 train loss: 0.7570, eval loss 1.5074808597564697\n",
                  "Epoch 106 train loss: 0.7969, eval loss 1.5084158182144165\n",
                  "Epoch 107 train loss: 0.7889, eval loss 1.5058895349502563\n",
                  "Epoch 108 train loss: 0.8279, eval loss 1.5046393871307373\n",
                  "Epoch 109 train loss: 0.7328, eval loss 1.505436658859253\n",
                  "Epoch 110 train loss: 0.7209, eval loss 1.5023902654647827\n",
                  "Epoch 111 train loss: 0.8179, eval loss 1.5018671751022339\n",
                  "Epoch 112 train loss: 0.7367, eval loss 1.5024179220199585\n",
                  "Epoch 113 train loss: 0.7753, eval loss 1.501105785369873\n",
                  "Epoch 114 train loss: 0.7332, eval loss 1.49958074092865\n",
                  "Epoch 115 train loss: 0.6730, eval loss 1.4989603757858276\n",
                  "Epoch 116 train loss: 0.7094, eval loss 1.4976718425750732\n",
                  "Epoch 117 train loss: 0.7604, eval loss 1.4971468448638916\n",
                  "Epoch 118 train loss: 0.7558, eval loss 1.4968373775482178\n",
                  "Epoch 119 train loss: 0.7059, eval loss 1.4956218004226685\n",
                  "Epoch 120 train loss: 0.7348, eval loss 1.4944133758544922\n",
                  "Epoch 121 train loss: 0.6820, eval loss 1.4935933351516724\n",
                  "Epoch 122 train loss: 0.7981, eval loss 1.4939537048339844\n",
                  "Epoch 123 train loss: 0.7563, eval loss 1.492270827293396\n",
                  "Epoch 124 train loss: 0.7645, eval loss 1.4909560680389404\n",
                  "Epoch 125 train loss: 0.8412, eval loss 1.4907852411270142\n",
                  "Epoch 126 train loss: 0.6329, eval loss 1.4911961555480957\n",
                  "Epoch 127 train loss: 0.7571, eval loss 1.489540934562683\n",
                  "Epoch 128 train loss: 0.7254, eval loss 1.4889811277389526\n",
                  "Epoch 129 train loss: 0.7554, eval loss 1.4881751537322998\n",
                  "Epoch 130 train loss: 0.7565, eval loss 1.486466646194458\n",
                  "Epoch 131 train loss: 0.7490, eval loss 1.4864531755447388\n",
                  "Epoch 132 train loss: 0.8002, eval loss 1.4844658374786377\n",
                  "Epoch 133 train loss: 0.7538, eval loss 1.4848514795303345\n",
                  "Epoch 134 train loss: 0.7342, eval loss 1.4839030504226685\n",
                  "Epoch 135 train loss: 0.7199, eval loss 1.4832110404968262\n",
                  "Epoch 136 train loss: 0.7972, eval loss 1.4834766387939453\n",
                  "Epoch 137 train loss: 0.7146, eval loss 1.4821995496749878\n",
                  "Epoch 138 train loss: 0.7421, eval loss 1.4817825555801392\n",
                  "Epoch 139 train loss: 0.7088, eval loss 1.481015920639038\n",
                  "Epoch 140 train loss: 0.7766, eval loss 1.4792245626449585\n",
                  "Epoch 141 train loss: 0.7330, eval loss 1.4784979820251465\n",
                  "Epoch 142 train loss: 0.7743, eval loss 1.47877037525177\n",
                  "Epoch 143 train loss: 0.8235, eval loss 1.4785504341125488\n",
                  "Epoch 144 train loss: 0.6999, eval loss 1.477510929107666\n",
                  "Epoch 145 train loss: 0.6374, eval loss 1.476528525352478\n",
                  "Epoch 146 train loss: 0.6677, eval loss 1.4745573997497559\n",
                  "Epoch 147 train loss: 0.6702, eval loss 1.475503921508789\n",
                  "Epoch 148 train loss: 0.7420, eval loss 1.4757143259048462\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:49:56,093] Trial 14 finished with value: 1.4745573997497559 and parameters: {'hidden_layers_size': 73, 'dropout_p': 0.4487116717919587, 'learning_rate': 7.333617542604693e-06, 'batch_size': 165, 'l2_reg': 8.575543293736724e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.7016, eval loss 1.4751015901565552\n",
                  "Epoch 0 train loss: 1.4657, eval loss 1.7590045928955078\n",
                  "Epoch 1 train loss: 1.3376, eval loss 1.7187418937683105\n",
                  "Epoch 2 train loss: 1.2415, eval loss 1.6887831687927246\n",
                  "Epoch 3 train loss: 1.1990, eval loss 1.664432168006897\n",
                  "Epoch 4 train loss: 1.1063, eval loss 1.6439323425292969\n",
                  "Epoch 5 train loss: 1.0439, eval loss 1.6281195878982544\n",
                  "Epoch 6 train loss: 0.9804, eval loss 1.6134161949157715\n",
                  "Epoch 7 train loss: 0.9675, eval loss 1.6012543439865112\n",
                  "Epoch 8 train loss: 0.9640, eval loss 1.5894718170166016\n",
                  "Epoch 9 train loss: 0.8970, eval loss 1.579715609550476\n",
                  "Epoch 10 train loss: 0.9028, eval loss 1.5691709518432617\n",
                  "Epoch 11 train loss: 0.8642, eval loss 1.5611250400543213\n",
                  "Epoch 12 train loss: 0.8643, eval loss 1.5530167818069458\n",
                  "Epoch 13 train loss: 0.8394, eval loss 1.5453025102615356\n",
                  "Epoch 14 train loss: 0.8127, eval loss 1.539689540863037\n",
                  "Epoch 15 train loss: 0.7857, eval loss 1.5333051681518555\n",
                  "Epoch 16 train loss: 0.8321, eval loss 1.5262490510940552\n",
                  "Epoch 17 train loss: 0.7779, eval loss 1.5219483375549316\n",
                  "Epoch 18 train loss: 0.7926, eval loss 1.5161103010177612\n",
                  "Epoch 19 train loss: 0.7488, eval loss 1.5113104581832886\n",
                  "Epoch 20 train loss: 0.7535, eval loss 1.5068707466125488\n",
                  "Epoch 21 train loss: 0.7732, eval loss 1.5028525590896606\n",
                  "Epoch 22 train loss: 0.7296, eval loss 1.4978269338607788\n",
                  "Epoch 23 train loss: 0.7528, eval loss 1.4940543174743652\n",
                  "Epoch 24 train loss: 0.7337, eval loss 1.4908126592636108\n",
                  "Epoch 25 train loss: 0.7198, eval loss 1.48737633228302\n",
                  "Epoch 26 train loss: 0.7591, eval loss 1.4846562147140503\n",
                  "Epoch 27 train loss: 0.7391, eval loss 1.4818003177642822\n",
                  "Epoch 28 train loss: 0.6807, eval loss 1.4784659147262573\n",
                  "Epoch 29 train loss: 0.7181, eval loss 1.47628915309906\n",
                  "Epoch 30 train loss: 0.7005, eval loss 1.473912000656128\n",
                  "Epoch 31 train loss: 0.7037, eval loss 1.4708892107009888\n",
                  "Epoch 32 train loss: 0.6993, eval loss 1.4684138298034668\n",
                  "Epoch 33 train loss: 0.6888, eval loss 1.4665838479995728\n",
                  "Epoch 34 train loss: 0.6843, eval loss 1.4643763303756714\n",
                  "Epoch 35 train loss: 0.6960, eval loss 1.4628807306289673\n",
                  "Epoch 36 train loss: 0.7107, eval loss 1.4608763456344604\n",
                  "Epoch 37 train loss: 0.6918, eval loss 1.4584236145019531\n",
                  "Epoch 38 train loss: 0.6574, eval loss 1.4577275514602661\n",
                  "Epoch 39 train loss: 0.7022, eval loss 1.456038475036621\n",
                  "Epoch 40 train loss: 0.6662, eval loss 1.4549607038497925\n",
                  "Epoch 41 train loss: 0.6600, eval loss 1.4531769752502441\n",
                  "Epoch 42 train loss: 0.6857, eval loss 1.4518554210662842\n",
                  "Epoch 43 train loss: 0.6755, eval loss 1.451002597808838\n",
                  "Epoch 44 train loss: 0.6696, eval loss 1.4497473239898682\n",
                  "Epoch 45 train loss: 0.6774, eval loss 1.448779582977295\n",
                  "Epoch 46 train loss: 0.6561, eval loss 1.447975754737854\n",
                  "Epoch 47 train loss: 0.6541, eval loss 1.4472441673278809\n",
                  "Epoch 48 train loss: 0.6824, eval loss 1.4460806846618652\n",
                  "Epoch 49 train loss: 0.6894, eval loss 1.444967269897461\n",
                  "Epoch 50 train loss: 0.6545, eval loss 1.4441289901733398\n",
                  "Epoch 51 train loss: 0.6510, eval loss 1.4437514543533325\n",
                  "Epoch 52 train loss: 0.6556, eval loss 1.442929983139038\n",
                  "Epoch 53 train loss: 0.6402, eval loss 1.4419227838516235\n",
                  "Epoch 54 train loss: 0.6750, eval loss 1.4413915872573853\n",
                  "Epoch 55 train loss: 0.6481, eval loss 1.4406453371047974\n",
                  "Epoch 56 train loss: 0.6617, eval loss 1.4406330585479736\n",
                  "Epoch 57 train loss: 0.6476, eval loss 1.4395787715911865\n",
                  "Epoch 58 train loss: 0.6547, eval loss 1.4391076564788818\n",
                  "Epoch 59 train loss: 0.6724, eval loss 1.4384373426437378\n",
                  "Epoch 60 train loss: 0.6662, eval loss 1.438289999961853\n",
                  "Epoch 61 train loss: 0.6461, eval loss 1.4374125003814697\n",
                  "Epoch 62 train loss: 0.6858, eval loss 1.4373095035552979\n",
                  "Epoch 63 train loss: 0.6708, eval loss 1.437207579612732\n",
                  "Epoch 64 train loss: 0.6992, eval loss 1.4360315799713135\n",
                  "Epoch 65 train loss: 0.6548, eval loss 1.4362729787826538\n",
                  "Epoch 66 train loss: 0.6755, eval loss 1.4359784126281738\n",
                  "Epoch 67 train loss: 0.6674, eval loss 1.4356365203857422\n",
                  "Epoch 68 train loss: 0.6511, eval loss 1.4349457025527954\n",
                  "Epoch 69 train loss: 0.6658, eval loss 1.434849739074707\n",
                  "Epoch 70 train loss: 0.6559, eval loss 1.4348212480545044\n",
                  "Epoch 71 train loss: 0.6428, eval loss 1.434562087059021\n",
                  "Epoch 72 train loss: 0.6297, eval loss 1.434370756149292\n",
                  "Epoch 73 train loss: 0.6471, eval loss 1.433780550956726\n",
                  "Epoch 74 train loss: 0.6445, eval loss 1.433701992034912\n",
                  "Epoch 75 train loss: 0.6758, eval loss 1.4336519241333008\n",
                  "Epoch 76 train loss: 0.6687, eval loss 1.4336626529693604\n",
                  "Epoch 77 train loss: 0.6537, eval loss 1.4332151412963867\n",
                  "Epoch 78 train loss: 0.6760, eval loss 1.432658076286316\n",
                  "Epoch 79 train loss: 0.6857, eval loss 1.4330580234527588\n",
                  "Epoch 80 train loss: 0.6582, eval loss 1.4326716661453247\n",
                  "Epoch 81 train loss: 0.6364, eval loss 1.43268620967865\n",
                  "Epoch 82 train loss: 0.6107, eval loss 1.4319311380386353\n",
                  "Epoch 83 train loss: 0.6365, eval loss 1.4323461055755615\n",
                  "Epoch 84 train loss: 0.6708, eval loss 1.431673288345337\n",
                  "Epoch 85 train loss: 0.6232, eval loss 1.4321491718292236\n",
                  "Epoch 86 train loss: 0.6658, eval loss 1.432038426399231\n",
                  "Epoch 87 train loss: 0.6998, eval loss 1.4320348501205444\n",
                  "Epoch 88 train loss: 0.6701, eval loss 1.4313690662384033\n",
                  "Epoch 89 train loss: 0.6295, eval loss 1.4315259456634521\n",
                  "Epoch 90 train loss: 0.6464, eval loss 1.4313517808914185\n",
                  "Epoch 91 train loss: 0.6169, eval loss 1.431636929512024\n",
                  "Epoch 92 train loss: 0.6622, eval loss 1.4316213130950928\n",
                  "Epoch 93 train loss: 0.6568, eval loss 1.4318000078201294\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:50:53,195] Trial 15 finished with value: 1.4313517808914185 and parameters: {'hidden_layers_size': 115, 'dropout_p': 0.2531603926074821, 'learning_rate': 3.7253343416482775e-05, 'batch_size': 215, 'l2_reg': 1.9092539539330737e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.7144, eval loss 1.786270022392273\n",
                  "Epoch 1 train loss: 1.5476, eval loss 1.7817648649215698\n",
                  "Epoch 2 train loss: 1.5744, eval loss 1.779555320739746\n",
                  "Epoch 3 train loss: 1.7134, eval loss 1.7772307395935059\n",
                  "Epoch 4 train loss: 1.5446, eval loss 1.7724343538284302\n",
                  "Epoch 5 train loss: 1.6539, eval loss 1.7685959339141846\n",
                  "Epoch 6 train loss: 1.5811, eval loss 1.76680326461792\n",
                  "Epoch 7 train loss: 1.6468, eval loss 1.765135645866394\n",
                  "Epoch 8 train loss: 1.6630, eval loss 1.7624340057373047\n",
                  "Epoch 9 train loss: 1.4899, eval loss 1.758819580078125\n",
                  "Epoch 10 train loss: 1.6336, eval loss 1.7547831535339355\n",
                  "Epoch 11 train loss: 1.3787, eval loss 1.751784086227417\n",
                  "Epoch 12 train loss: 1.4748, eval loss 1.7488884925842285\n",
                  "Epoch 13 train loss: 1.4539, eval loss 1.7456883192062378\n",
                  "Epoch 14 train loss: 1.3625, eval loss 1.7426722049713135\n",
                  "Epoch 15 train loss: 1.3371, eval loss 1.7398943901062012\n",
                  "Epoch 16 train loss: 1.4768, eval loss 1.7363457679748535\n",
                  "Epoch 17 train loss: 1.5363, eval loss 1.7335671186447144\n",
                  "Epoch 18 train loss: 1.4452, eval loss 1.7300385236740112\n",
                  "Epoch 19 train loss: 1.4041, eval loss 1.7296428680419922\n",
                  "Epoch 20 train loss: 1.3623, eval loss 1.7273147106170654\n",
                  "Epoch 21 train loss: 1.4467, eval loss 1.7227503061294556\n",
                  "Epoch 22 train loss: 1.4593, eval loss 1.721766710281372\n",
                  "Epoch 23 train loss: 1.4615, eval loss 1.7173182964324951\n",
                  "Epoch 24 train loss: 1.3936, eval loss 1.7155163288116455\n",
                  "Epoch 25 train loss: 1.3837, eval loss 1.7131669521331787\n",
                  "Epoch 26 train loss: 1.3880, eval loss 1.7109053134918213\n",
                  "Epoch 27 train loss: 1.4119, eval loss 1.7067722082138062\n",
                  "Epoch 28 train loss: 1.2346, eval loss 1.7052228450775146\n",
                  "Epoch 29 train loss: 1.4505, eval loss 1.7043046951293945\n",
                  "Epoch 30 train loss: 1.2615, eval loss 1.7019866704940796\n",
                  "Epoch 31 train loss: 1.3776, eval loss 1.6999523639678955\n",
                  "Epoch 32 train loss: 1.3277, eval loss 1.695858359336853\n",
                  "Epoch 33 train loss: 1.2500, eval loss 1.6939764022827148\n",
                  "Epoch 34 train loss: 1.2192, eval loss 1.6927931308746338\n",
                  "Epoch 35 train loss: 1.3061, eval loss 1.6909799575805664\n",
                  "Epoch 36 train loss: 1.2100, eval loss 1.687072992324829\n",
                  "Epoch 37 train loss: 1.2785, eval loss 1.6845502853393555\n",
                  "Epoch 38 train loss: 1.1605, eval loss 1.6842608451843262\n",
                  "Epoch 39 train loss: 1.1775, eval loss 1.681497573852539\n",
                  "Epoch 40 train loss: 1.3910, eval loss 1.6799834966659546\n",
                  "Epoch 41 train loss: 1.2728, eval loss 1.6785995960235596\n",
                  "Epoch 42 train loss: 1.1858, eval loss 1.6767313480377197\n",
                  "Epoch 43 train loss: 1.3170, eval loss 1.6739295721054077\n",
                  "Epoch 44 train loss: 1.3617, eval loss 1.6714433431625366\n",
                  "Epoch 45 train loss: 1.1393, eval loss 1.6682788133621216\n",
                  "Epoch 46 train loss: 1.1453, eval loss 1.66729736328125\n",
                  "Epoch 47 train loss: 1.1967, eval loss 1.6657755374908447\n",
                  "Epoch 48 train loss: 1.2347, eval loss 1.6642740964889526\n",
                  "Epoch 49 train loss: 1.3344, eval loss 1.6619142293930054\n",
                  "Epoch 50 train loss: 1.1267, eval loss 1.6595251560211182\n",
                  "Epoch 51 train loss: 1.2753, eval loss 1.6574070453643799\n",
                  "Epoch 52 train loss: 1.1213, eval loss 1.6552633047103882\n",
                  "Epoch 53 train loss: 1.1984, eval loss 1.6554166078567505\n",
                  "Epoch 54 train loss: 1.0883, eval loss 1.6517139673233032\n",
                  "Epoch 55 train loss: 1.0992, eval loss 1.6509495973587036\n",
                  "Epoch 56 train loss: 1.2390, eval loss 1.6490658521652222\n",
                  "Epoch 57 train loss: 1.1147, eval loss 1.6471714973449707\n",
                  "Epoch 58 train loss: 1.0838, eval loss 1.6449822187423706\n",
                  "Epoch 59 train loss: 1.2361, eval loss 1.6446759700775146\n",
                  "Epoch 60 train loss: 1.0987, eval loss 1.641809105873108\n",
                  "Epoch 61 train loss: 1.1230, eval loss 1.6427048444747925\n",
                  "Epoch 62 train loss: 1.1832, eval loss 1.6382644176483154\n",
                  "Epoch 63 train loss: 1.0311, eval loss 1.6383056640625\n",
                  "Epoch 64 train loss: 1.0089, eval loss 1.637952446937561\n",
                  "Epoch 65 train loss: 1.0059, eval loss 1.6353580951690674\n",
                  "Epoch 66 train loss: 1.0992, eval loss 1.6334539651870728\n",
                  "Epoch 67 train loss: 1.0544, eval loss 1.6326932907104492\n",
                  "Epoch 68 train loss: 1.1079, eval loss 1.6301333904266357\n",
                  "Epoch 69 train loss: 1.1294, eval loss 1.6287697553634644\n",
                  "Epoch 70 train loss: 1.1314, eval loss 1.62690007686615\n",
                  "Epoch 71 train loss: 1.1519, eval loss 1.6270779371261597\n",
                  "Epoch 72 train loss: 1.0541, eval loss 1.6241897344589233\n",
                  "Epoch 73 train loss: 1.1090, eval loss 1.6229814291000366\n",
                  "Epoch 74 train loss: 1.0021, eval loss 1.6213057041168213\n",
                  "Epoch 75 train loss: 1.0277, eval loss 1.619919776916504\n",
                  "Epoch 76 train loss: 1.0521, eval loss 1.6183346509933472\n",
                  "Epoch 77 train loss: 1.1538, eval loss 1.6173774003982544\n",
                  "Epoch 78 train loss: 1.0717, eval loss 1.617794394493103\n",
                  "Epoch 79 train loss: 0.9910, eval loss 1.6138468980789185\n",
                  "Epoch 80 train loss: 1.0140, eval loss 1.613153100013733\n",
                  "Epoch 81 train loss: 1.1358, eval loss 1.6133251190185547\n",
                  "Epoch 82 train loss: 1.0609, eval loss 1.6123285293579102\n",
                  "Epoch 83 train loss: 1.0500, eval loss 1.6100682020187378\n",
                  "Epoch 84 train loss: 0.9884, eval loss 1.6089463233947754\n",
                  "Epoch 85 train loss: 1.0250, eval loss 1.605735421180725\n",
                  "Epoch 86 train loss: 1.0996, eval loss 1.6068061590194702\n",
                  "Epoch 87 train loss: 0.9917, eval loss 1.6028063297271729\n",
                  "Epoch 88 train loss: 1.0380, eval loss 1.6029894351959229\n",
                  "Epoch 89 train loss: 0.9832, eval loss 1.6021146774291992\n",
                  "Epoch 90 train loss: 0.9579, eval loss 1.6017656326293945\n",
                  "Epoch 91 train loss: 1.0398, eval loss 1.5986050367355347\n",
                  "Epoch 92 train loss: 1.0246, eval loss 1.6002066135406494\n",
                  "Epoch 93 train loss: 0.9826, eval loss 1.5997722148895264\n",
                  "Epoch 94 train loss: 1.0681, eval loss 1.594683051109314\n",
                  "Epoch 95 train loss: 1.0464, eval loss 1.5954066514968872\n",
                  "Epoch 96 train loss: 1.1054, eval loss 1.5939422845840454\n",
                  "Epoch 97 train loss: 0.9868, eval loss 1.5955810546875\n",
                  "Epoch 98 train loss: 1.0274, eval loss 1.5906047821044922\n",
                  "Epoch 99 train loss: 1.1574, eval loss 1.5905789136886597\n",
                  "Epoch 100 train loss: 1.0543, eval loss 1.5897632837295532\n",
                  "Epoch 101 train loss: 0.9698, eval loss 1.5879446268081665\n",
                  "Epoch 102 train loss: 1.0321, eval loss 1.5884398221969604\n",
                  "Epoch 103 train loss: 1.0064, eval loss 1.587699294090271\n",
                  "Epoch 104 train loss: 0.9977, eval loss 1.5862679481506348\n",
                  "Epoch 105 train loss: 0.9079, eval loss 1.5855822563171387\n",
                  "Epoch 106 train loss: 0.9447, eval loss 1.5844671726226807\n",
                  "Epoch 107 train loss: 1.0219, eval loss 1.5830742120742798\n",
                  "Epoch 108 train loss: 0.9798, eval loss 1.5800790786743164\n",
                  "Epoch 109 train loss: 0.9020, eval loss 1.581099510192871\n",
                  "Epoch 110 train loss: 0.9670, eval loss 1.5795161724090576\n",
                  "Epoch 111 train loss: 1.0399, eval loss 1.5782287120819092\n",
                  "Epoch 112 train loss: 1.0221, eval loss 1.5781570672988892\n",
                  "Epoch 113 train loss: 0.9144, eval loss 1.5777504444122314\n",
                  "Epoch 114 train loss: 1.0074, eval loss 1.5740277767181396\n",
                  "Epoch 115 train loss: 0.9543, eval loss 1.5744706392288208\n",
                  "Epoch 116 train loss: 0.9390, eval loss 1.5732663869857788\n",
                  "Epoch 117 train loss: 0.8964, eval loss 1.572096824645996\n",
                  "Epoch 118 train loss: 0.8319, eval loss 1.572440266609192\n",
                  "Epoch 119 train loss: 0.9579, eval loss 1.5704855918884277\n",
                  "Epoch 120 train loss: 0.9707, eval loss 1.570806622505188\n",
                  "Epoch 121 train loss: 0.8806, eval loss 1.5713095664978027\n",
                  "Epoch 122 train loss: 0.9726, eval loss 1.5686522722244263\n",
                  "Epoch 123 train loss: 0.8538, eval loss 1.5680378675460815\n",
                  "Epoch 124 train loss: 0.8430, eval loss 1.5655293464660645\n",
                  "Epoch 125 train loss: 0.8731, eval loss 1.5664530992507935\n",
                  "Epoch 126 train loss: 0.8306, eval loss 1.5649393796920776\n",
                  "Epoch 127 train loss: 1.0637, eval loss 1.5654553174972534\n",
                  "Epoch 128 train loss: 0.9563, eval loss 1.5635265111923218\n",
                  "Epoch 129 train loss: 0.8984, eval loss 1.560272455215454\n",
                  "Epoch 130 train loss: 0.9540, eval loss 1.5614057779312134\n",
                  "Epoch 131 train loss: 0.8880, eval loss 1.5621708631515503\n",
                  "Epoch 132 train loss: 0.9190, eval loss 1.5578256845474243\n",
                  "Epoch 133 train loss: 0.9667, eval loss 1.559767484664917\n",
                  "Epoch 134 train loss: 0.9926, eval loss 1.5564111471176147\n",
                  "Epoch 135 train loss: 0.8785, eval loss 1.5574429035186768\n",
                  "Epoch 136 train loss: 0.9121, eval loss 1.5558098554611206\n",
                  "Epoch 137 train loss: 0.7992, eval loss 1.5565203428268433\n",
                  "Epoch 138 train loss: 0.8255, eval loss 1.5556904077529907\n",
                  "Epoch 139 train loss: 0.9682, eval loss 1.5538398027420044\n",
                  "Epoch 140 train loss: 0.9093, eval loss 1.553713321685791\n",
                  "Epoch 141 train loss: 0.9322, eval loss 1.5514686107635498\n",
                  "Epoch 142 train loss: 0.8723, eval loss 1.551190972328186\n",
                  "Epoch 143 train loss: 0.8718, eval loss 1.5511822700500488\n",
                  "Epoch 144 train loss: 0.7794, eval loss 1.5487169027328491\n",
                  "Epoch 145 train loss: 0.8647, eval loss 1.548546552658081\n",
                  "Epoch 146 train loss: 0.7913, eval loss 1.5495350360870361\n",
                  "Epoch 147 train loss: 0.8911, eval loss 1.5473084449768066\n",
                  "Epoch 148 train loss: 0.9664, eval loss 1.5474313497543335\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:52:31,844] Trial 16 finished with value: 1.5469430685043335 and parameters: {'hidden_layers_size': 88, 'dropout_p': 0.4522788736177, 'learning_rate': 3.1629953132741747e-06, 'batch_size': 160, 'l2_reg': 4.099293694265519e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.8466, eval loss 1.5469430685043335\n",
                  "Epoch 0 train loss: 1.4749, eval loss 1.7728410959243774\n",
                  "Epoch 1 train loss: 1.4255, eval loss 1.7479106187820435\n",
                  "Epoch 2 train loss: 1.3062, eval loss 1.7252166271209717\n",
                  "Epoch 3 train loss: 1.2784, eval loss 1.7031853199005127\n",
                  "Epoch 4 train loss: 1.1953, eval loss 1.6845165491104126\n",
                  "Epoch 5 train loss: 1.2204, eval loss 1.6656266450881958\n",
                  "Epoch 6 train loss: 1.1710, eval loss 1.6514856815338135\n",
                  "Epoch 7 train loss: 1.2647, eval loss 1.6378757953643799\n",
                  "Epoch 8 train loss: 1.0162, eval loss 1.6219855546951294\n",
                  "Epoch 9 train loss: 1.0394, eval loss 1.6117557287216187\n",
                  "Epoch 10 train loss: 1.0019, eval loss 1.6023176908493042\n",
                  "Epoch 11 train loss: 1.0163, eval loss 1.5910587310791016\n",
                  "Epoch 12 train loss: 0.9322, eval loss 1.580617070198059\n",
                  "Epoch 13 train loss: 0.9432, eval loss 1.5730935335159302\n",
                  "Epoch 14 train loss: 0.8868, eval loss 1.5639398097991943\n",
                  "Epoch 15 train loss: 0.9143, eval loss 1.556567668914795\n",
                  "Epoch 16 train loss: 0.9373, eval loss 1.5491204261779785\n",
                  "Epoch 17 train loss: 0.8590, eval loss 1.5404380559921265\n",
                  "Epoch 18 train loss: 0.8513, eval loss 1.5375325679779053\n",
                  "Epoch 19 train loss: 0.9069, eval loss 1.5306057929992676\n",
                  "Epoch 20 train loss: 0.8927, eval loss 1.5245208740234375\n",
                  "Epoch 21 train loss: 0.8787, eval loss 1.520855188369751\n",
                  "Epoch 22 train loss: 0.8331, eval loss 1.5145517587661743\n",
                  "Epoch 23 train loss: 0.8640, eval loss 1.5098727941513062\n",
                  "Epoch 24 train loss: 0.7818, eval loss 1.5064514875411987\n",
                  "Epoch 25 train loss: 0.8566, eval loss 1.501720905303955\n",
                  "Epoch 26 train loss: 0.8197, eval loss 1.4986730813980103\n",
                  "Epoch 27 train loss: 0.7895, eval loss 1.4957555532455444\n",
                  "Epoch 28 train loss: 0.7983, eval loss 1.4921400547027588\n",
                  "Epoch 29 train loss: 0.8724, eval loss 1.4876360893249512\n",
                  "Epoch 30 train loss: 0.7995, eval loss 1.486581563949585\n",
                  "Epoch 31 train loss: 0.7422, eval loss 1.4841006994247437\n",
                  "Epoch 32 train loss: 0.7285, eval loss 1.4790983200073242\n",
                  "Epoch 33 train loss: 0.8182, eval loss 1.477839469909668\n",
                  "Epoch 34 train loss: 0.7875, eval loss 1.4768444299697876\n",
                  "Epoch 35 train loss: 0.7380, eval loss 1.4737321138381958\n",
                  "Epoch 36 train loss: 0.7532, eval loss 1.4721566438674927\n",
                  "Epoch 37 train loss: 0.8346, eval loss 1.4699608087539673\n",
                  "Epoch 38 train loss: 0.7676, eval loss 1.4694973230361938\n",
                  "Epoch 39 train loss: 0.7320, eval loss 1.4672658443450928\n",
                  "Epoch 40 train loss: 0.7320, eval loss 1.4654755592346191\n",
                  "Epoch 41 train loss: 0.7696, eval loss 1.4647523164749146\n",
                  "Epoch 42 train loss: 0.7461, eval loss 1.4620611667633057\n",
                  "Epoch 43 train loss: 0.7400, eval loss 1.4625921249389648\n",
                  "Epoch 44 train loss: 0.7496, eval loss 1.4596774578094482\n",
                  "Epoch 45 train loss: 0.7169, eval loss 1.4590237140655518\n",
                  "Epoch 46 train loss: 0.7751, eval loss 1.4589076042175293\n",
                  "Epoch 47 train loss: 0.7050, eval loss 1.4573549032211304\n",
                  "Epoch 48 train loss: 0.8329, eval loss 1.4555118083953857\n",
                  "Epoch 49 train loss: 0.7635, eval loss 1.4556560516357422\n",
                  "Epoch 50 train loss: 0.7738, eval loss 1.4558743238449097\n",
                  "Epoch 51 train loss: 0.8771, eval loss 1.453601360321045\n",
                  "Epoch 52 train loss: 0.7596, eval loss 1.4531811475753784\n",
                  "Epoch 53 train loss: 0.7718, eval loss 1.452892541885376\n",
                  "Epoch 54 train loss: 0.7271, eval loss 1.4519882202148438\n",
                  "Epoch 55 train loss: 0.7875, eval loss 1.4518572092056274\n",
                  "Epoch 56 train loss: 0.7196, eval loss 1.4505271911621094\n",
                  "Epoch 57 train loss: 0.7429, eval loss 1.4488998651504517\n",
                  "Epoch 58 train loss: 0.6997, eval loss 1.4499846696853638\n",
                  "Epoch 59 train loss: 0.7854, eval loss 1.4501363039016724\n",
                  "Epoch 60 train loss: 0.7518, eval loss 1.4492136240005493\n",
                  "Epoch 61 train loss: 0.8221, eval loss 1.448241949081421\n",
                  "Epoch 62 train loss: 0.7465, eval loss 1.4490693807601929\n",
                  "Epoch 63 train loss: 0.7906, eval loss 1.4468629360198975\n",
                  "Epoch 64 train loss: 0.7626, eval loss 1.4471296072006226\n",
                  "Epoch 65 train loss: 0.6346, eval loss 1.446917176246643\n",
                  "Epoch 66 train loss: 0.7307, eval loss 1.4462769031524658\n",
                  "Epoch 67 train loss: 0.7938, eval loss 1.4465339183807373\n",
                  "Epoch 68 train loss: 0.6354, eval loss 1.4451038837432861\n",
                  "Epoch 69 train loss: 0.6404, eval loss 1.4461278915405273\n",
                  "Epoch 70 train loss: 0.6820, eval loss 1.444210410118103\n",
                  "Epoch 71 train loss: 0.8392, eval loss 1.4441728591918945\n",
                  "Epoch 72 train loss: 0.7961, eval loss 1.4453747272491455\n",
                  "Epoch 73 train loss: 0.8433, eval loss 1.4448573589324951\n",
                  "Epoch 74 train loss: 0.6416, eval loss 1.4434272050857544\n",
                  "Epoch 75 train loss: 0.6416, eval loss 1.4445698261260986\n",
                  "Epoch 76 train loss: 0.7062, eval loss 1.4440969228744507\n",
                  "Epoch 77 train loss: 0.7861, eval loss 1.444455862045288\n",
                  "Epoch 78 train loss: 0.7338, eval loss 1.4425759315490723\n",
                  "Epoch 79 train loss: 0.7221, eval loss 1.4436115026474\n",
                  "Epoch 80 train loss: 0.7495, eval loss 1.4430592060089111\n",
                  "Epoch 81 train loss: 0.7134, eval loss 1.4421428442001343\n",
                  "Epoch 82 train loss: 0.7650, eval loss 1.441368579864502\n",
                  "Epoch 83 train loss: 0.7704, eval loss 1.442738652229309\n",
                  "Epoch 84 train loss: 0.6788, eval loss 1.4416545629501343\n",
                  "Epoch 85 train loss: 0.6894, eval loss 1.4423720836639404\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 19:53:22,277] Trial 17 finished with value: 1.441368579864502 and parameters: {'hidden_layers_size': 98, 'dropout_p': 0.49977313487729114, 'learning_rate': 4.4882934157749176e-05, 'batch_size': 233, 'l2_reg': 1.797092635932884e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.5185, eval loss 1.7818670272827148\n",
                  "Epoch 1 train loss: 1.5892, eval loss 1.7793103456497192\n",
                  "Epoch 2 train loss: 1.4807, eval loss 1.7737443447113037\n",
                  "Epoch 3 train loss: 1.4993, eval loss 1.772374153137207\n",
                  "Epoch 4 train loss: 1.4975, eval loss 1.7692302465438843\n",
                  "Epoch 5 train loss: 1.4759, eval loss 1.7674707174301147\n",
                  "Epoch 6 train loss: 1.4724, eval loss 1.7641863822937012\n",
                  "Epoch 7 train loss: 1.5820, eval loss 1.7622827291488647\n",
                  "Epoch 8 train loss: 1.5265, eval loss 1.7618460655212402\n",
                  "Epoch 9 train loss: 1.4430, eval loss 1.7570180892944336\n",
                  "Epoch 10 train loss: 1.4452, eval loss 1.7538065910339355\n",
                  "Epoch 11 train loss: 1.4392, eval loss 1.7512365579605103\n",
                  "Epoch 12 train loss: 1.4346, eval loss 1.7500845193862915\n",
                  "Epoch 13 train loss: 1.4426, eval loss 1.7465213537216187\n",
                  "Epoch 14 train loss: 1.3959, eval loss 1.7452397346496582\n",
                  "Epoch 15 train loss: 1.4705, eval loss 1.7425554990768433\n",
                  "Epoch 16 train loss: 1.4072, eval loss 1.7409945726394653\n",
                  "Epoch 17 train loss: 1.4258, eval loss 1.7376458644866943\n",
                  "Epoch 18 train loss: 1.3388, eval loss 1.7352948188781738\n",
                  "Epoch 19 train loss: 1.3893, eval loss 1.7315616607666016\n",
                  "Epoch 20 train loss: 1.3316, eval loss 1.7306190729141235\n",
                  "Epoch 21 train loss: 1.3338, eval loss 1.728284478187561\n",
                  "Epoch 22 train loss: 1.3527, eval loss 1.7287259101867676\n",
                  "Epoch 23 train loss: 1.4534, eval loss 1.7230912446975708\n",
                  "Epoch 24 train loss: 1.3034, eval loss 1.722852110862732\n",
                  "Epoch 25 train loss: 1.2888, eval loss 1.7217166423797607\n",
                  "Epoch 26 train loss: 1.3589, eval loss 1.7180649042129517\n",
                  "Epoch 27 train loss: 1.3626, eval loss 1.7147893905639648\n",
                  "Epoch 28 train loss: 1.2765, eval loss 1.7103898525238037\n",
                  "Epoch 29 train loss: 1.2814, eval loss 1.7119883298873901\n",
                  "Epoch 30 train loss: 1.3729, eval loss 1.7084970474243164\n",
                  "Epoch 31 train loss: 1.2891, eval loss 1.7074936628341675\n",
                  "Epoch 32 train loss: 1.1940, eval loss 1.7043849229812622\n",
                  "Epoch 33 train loss: 1.2426, eval loss 1.704535722732544\n",
                  "Epoch 34 train loss: 1.3421, eval loss 1.7010440826416016\n",
                  "Epoch 35 train loss: 1.2694, eval loss 1.6999504566192627\n",
                  "Epoch 36 train loss: 1.2383, eval loss 1.698708176612854\n",
                  "Epoch 37 train loss: 1.3226, eval loss 1.69706130027771\n",
                  "Epoch 38 train loss: 1.2570, eval loss 1.6946219205856323\n",
                  "Epoch 39 train loss: 1.2921, eval loss 1.6910804510116577\n",
                  "Epoch 40 train loss: 1.3195, eval loss 1.6899151802062988\n",
                  "Epoch 41 train loss: 1.2677, eval loss 1.6878454685211182\n",
                  "Epoch 42 train loss: 1.1787, eval loss 1.6852531433105469\n",
                  "Epoch 43 train loss: 1.2984, eval loss 1.6846050024032593\n",
                  "Epoch 44 train loss: 1.2370, eval loss 1.6817556619644165\n",
                  "Epoch 45 train loss: 1.2570, eval loss 1.6826633214950562\n",
                  "Epoch 46 train loss: 1.2458, eval loss 1.677140235900879\n",
                  "Epoch 47 train loss: 1.2007, eval loss 1.6776288747787476\n",
                  "Epoch 48 train loss: 1.2422, eval loss 1.6749002933502197\n",
                  "Epoch 49 train loss: 1.2445, eval loss 1.6729750633239746\n",
                  "Epoch 50 train loss: 1.2175, eval loss 1.6715186834335327\n",
                  "Epoch 51 train loss: 1.2233, eval loss 1.6683844327926636\n",
                  "Epoch 52 train loss: 1.2026, eval loss 1.6696664094924927\n",
                  "Epoch 53 train loss: 1.1972, eval loss 1.6662392616271973\n",
                  "Epoch 54 train loss: 1.2032, eval loss 1.6652017831802368\n",
                  "Epoch 55 train loss: 1.1885, eval loss 1.6639914512634277\n",
                  "Epoch 56 train loss: 1.2142, eval loss 1.6637681722640991\n",
                  "Epoch 57 train loss: 1.2396, eval loss 1.6600629091262817\n",
                  "Epoch 58 train loss: 1.2205, eval loss 1.6578091382980347\n",
                  "Epoch 59 train loss: 1.2177, eval loss 1.6569504737854004\n",
                  "Epoch 60 train loss: 1.1601, eval loss 1.6554300785064697\n",
                  "Epoch 61 train loss: 1.1535, eval loss 1.6534178256988525\n",
                  "Epoch 62 train loss: 1.1356, eval loss 1.6536049842834473\n",
                  "Epoch 63 train loss: 1.0775, eval loss 1.6502549648284912\n",
                  "Epoch 64 train loss: 1.1390, eval loss 1.6495404243469238\n",
                  "Epoch 65 train loss: 1.1512, eval loss 1.6484639644622803\n",
                  "Epoch 66 train loss: 1.1685, eval loss 1.6458830833435059\n",
                  "Epoch 67 train loss: 1.1702, eval loss 1.646912693977356\n",
                  "Epoch 68 train loss: 1.1312, eval loss 1.643426537513733\n",
                  "Epoch 69 train loss: 1.1558, eval loss 1.6428133249282837\n",
                  "Epoch 70 train loss: 1.1503, eval loss 1.6425211429595947\n",
                  "Epoch 71 train loss: 1.1559, eval loss 1.6408731937408447\n",
                  "Epoch 72 train loss: 1.1625, eval loss 1.6394411325454712\n",
                  "Epoch 73 train loss: 1.1169, eval loss 1.6389833688735962\n",
                  "Epoch 74 train loss: 1.1230, eval loss 1.635699987411499\n",
                  "Epoch 75 train loss: 1.1446, eval loss 1.63494074344635\n",
                  "Epoch 76 train loss: 1.1437, eval loss 1.6327348947525024\n",
                  "Epoch 77 train loss: 1.0716, eval loss 1.6319165229797363\n",
                  "Epoch 78 train loss: 1.1468, eval loss 1.630664587020874\n",
                  "Epoch 79 train loss: 1.0772, eval loss 1.6282117366790771\n",
                  "Epoch 80 train loss: 1.0873, eval loss 1.6284288167953491\n",
                  "Epoch 81 train loss: 1.1088, eval loss 1.6270121335983276\n",
                  "Epoch 82 train loss: 1.0554, eval loss 1.6265716552734375\n",
                  "Epoch 83 train loss: 1.1349, eval loss 1.625102162361145\n",
                  "Epoch 84 train loss: 1.0690, eval loss 1.6248893737792969\n",
                  "Epoch 85 train loss: 1.0517, eval loss 1.621917963027954\n",
                  "Epoch 86 train loss: 1.0758, eval loss 1.6206127405166626\n",
                  "Epoch 87 train loss: 1.1000, eval loss 1.6189677715301514\n",
                  "Epoch 88 train loss: 1.0684, eval loss 1.616856336593628\n",
                  "Epoch 89 train loss: 1.0810, eval loss 1.6180363893508911\n",
                  "Epoch 90 train loss: 1.1309, eval loss 1.6167402267456055\n",
                  "Epoch 91 train loss: 1.0984, eval loss 1.61648690700531\n",
                  "Epoch 92 train loss: 1.0657, eval loss 1.6149431467056274\n",
                  "Epoch 93 train loss: 1.0460, eval loss 1.6123534440994263\n",
                  "Epoch 94 train loss: 1.0487, eval loss 1.6109007596969604\n",
                  "Epoch 95 train loss: 1.0773, eval loss 1.6105389595031738\n",
                  "Epoch 96 train loss: 1.1271, eval loss 1.608135461807251\n",
                  "Epoch 97 train loss: 1.1445, eval loss 1.608803153038025\n",
                  "Epoch 98 train loss: 1.0011, eval loss 1.6068978309631348\n",
                  "Epoch 99 train loss: 1.0564, eval loss 1.6050087213516235\n",
                  "Epoch 100 train loss: 1.0016, eval loss 1.6057837009429932\n",
                  "Epoch 101 train loss: 0.9932, eval loss 1.6015737056732178\n",
                  "Epoch 102 train loss: 0.9978, eval loss 1.6027557849884033\n",
                  "Epoch 103 train loss: 1.0628, eval loss 1.6016921997070312\n",
                  "Epoch 104 train loss: 0.9444, eval loss 1.5995419025421143\n",
                  "Epoch 105 train loss: 1.0349, eval loss 1.5990091562271118\n",
                  "Epoch 106 train loss: 1.0230, eval loss 1.5975141525268555\n",
                  "Epoch 107 train loss: 1.0195, eval loss 1.5972322225570679\n",
                  "Epoch 108 train loss: 1.0198, eval loss 1.5951827764511108\n",
                  "Epoch 109 train loss: 1.0320, eval loss 1.5960237979888916\n",
                  "Epoch 110 train loss: 1.0139, eval loss 1.5934644937515259\n",
                  "Epoch 111 train loss: 0.9368, eval loss 1.594652533531189\n",
                  "Epoch 112 train loss: 1.0521, eval loss 1.592246413230896\n",
                  "Epoch 113 train loss: 0.9900, eval loss 1.5919430255889893\n",
                  "Epoch 114 train loss: 1.0075, eval loss 1.5893999338150024\n",
                  "Epoch 115 train loss: 0.9671, eval loss 1.5900907516479492\n",
                  "Epoch 116 train loss: 1.0321, eval loss 1.5868059396743774\n",
                  "Epoch 117 train loss: 1.0624, eval loss 1.585813045501709\n",
                  "Epoch 118 train loss: 0.9950, eval loss 1.5850902795791626\n",
                  "Epoch 119 train loss: 1.0251, eval loss 1.5854823589324951\n",
                  "Epoch 120 train loss: 0.9821, eval loss 1.5841835737228394\n",
                  "Epoch 121 train loss: 1.0623, eval loss 1.5816562175750732\n",
                  "Epoch 122 train loss: 1.0222, eval loss 1.583062767982483\n",
                  "Epoch 123 train loss: 0.9584, eval loss 1.581038236618042\n",
                  "Epoch 124 train loss: 1.0126, eval loss 1.5787622928619385\n",
                  "Epoch 125 train loss: 0.9395, eval loss 1.5804039239883423\n",
                  "Epoch 126 train loss: 1.0355, eval loss 1.578343152999878\n",
                  "Epoch 127 train loss: 0.9817, eval loss 1.5765517950057983\n",
                  "Epoch 128 train loss: 1.0032, eval loss 1.5759235620498657\n",
                  "Epoch 129 train loss: 0.9675, eval loss 1.57696533203125\n",
                  "Epoch 130 train loss: 0.9164, eval loss 1.5760406255722046\n",
                  "Epoch 131 train loss: 0.9638, eval loss 1.5746660232543945\n",
                  "Epoch 132 train loss: 0.9549, eval loss 1.5738136768341064\n",
                  "Epoch 133 train loss: 0.9676, eval loss 1.5729644298553467\n",
                  "Epoch 134 train loss: 0.9414, eval loss 1.5710358619689941\n",
                  "Epoch 135 train loss: 0.9553, eval loss 1.5713584423065186\n",
                  "Epoch 136 train loss: 0.9938, eval loss 1.5687830448150635\n",
                  "Epoch 137 train loss: 0.9543, eval loss 1.5686918497085571\n",
                  "Epoch 138 train loss: 0.9507, eval loss 1.5662401914596558\n",
                  "Epoch 139 train loss: 0.9147, eval loss 1.5659959316253662\n",
                  "Epoch 140 train loss: 0.9104, eval loss 1.5660405158996582\n",
                  "Epoch 141 train loss: 0.9520, eval loss 1.5653952360153198\n",
                  "Epoch 142 train loss: 0.9263, eval loss 1.562921166419983\n",
                  "Epoch 143 train loss: 0.8829, eval loss 1.5633361339569092\n",
                  "Epoch 144 train loss: 0.9663, eval loss 1.5629453659057617\n",
                  "Epoch 145 train loss: 0.9622, eval loss 1.5618784427642822\n",
                  "Epoch 146 train loss: 0.9235, eval loss 1.5612924098968506\n",
                  "Epoch 147 train loss: 0.9984, eval loss 1.5599592924118042\n",
                  "Epoch 148 train loss: 0.9261, eval loss 1.5599220991134644\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-26 22:56:36,070] Trial 18 finished with value: 1.5564539432525635 and parameters: {'hidden_layers_size': 64, 'dropout_p': 0.402003374821153, 'learning_rate': 2.925001742148384e-06, 'batch_size': 128, 'l2_reg': 0.00014214787537244022}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.9608, eval loss 1.5564539432525635\n",
                  "Epoch 0 train loss: 1.4237, eval loss 1.7518784999847412\n",
                  "Epoch 1 train loss: 1.3585, eval loss 1.7327529191970825\n",
                  "Epoch 2 train loss: 1.2998, eval loss 1.7173981666564941\n",
                  "Epoch 3 train loss: 1.2192, eval loss 1.701725721359253\n",
                  "Epoch 4 train loss: 1.2220, eval loss 1.6877527236938477\n",
                  "Epoch 5 train loss: 1.1711, eval loss 1.6736961603164673\n",
                  "Epoch 6 train loss: 1.1422, eval loss 1.660018801689148\n",
                  "Epoch 7 train loss: 1.0869, eval loss 1.6502035856246948\n",
                  "Epoch 8 train loss: 1.0851, eval loss 1.6387524604797363\n",
                  "Epoch 9 train loss: 1.0721, eval loss 1.6298624277114868\n",
                  "Epoch 10 train loss: 1.0508, eval loss 1.620577335357666\n",
                  "Epoch 11 train loss: 1.0140, eval loss 1.6112985610961914\n",
                  "Epoch 12 train loss: 1.0173, eval loss 1.6012847423553467\n",
                  "Epoch 13 train loss: 0.9918, eval loss 1.5942553281784058\n",
                  "Epoch 14 train loss: 0.9412, eval loss 1.5863847732543945\n",
                  "Epoch 15 train loss: 0.9572, eval loss 1.5790684223175049\n",
                  "Epoch 16 train loss: 0.8800, eval loss 1.57260000705719\n",
                  "Epoch 17 train loss: 0.8827, eval loss 1.5653220415115356\n",
                  "Epoch 18 train loss: 0.8906, eval loss 1.5607333183288574\n",
                  "Epoch 19 train loss: 0.8806, eval loss 1.5556141138076782\n",
                  "Epoch 20 train loss: 0.8875, eval loss 1.5486234426498413\n",
                  "Epoch 21 train loss: 0.8600, eval loss 1.5442379713058472\n",
                  "Epoch 22 train loss: 0.8625, eval loss 1.5374544858932495\n",
                  "Epoch 23 train loss: 0.8215, eval loss 1.5315980911254883\n",
                  "Epoch 24 train loss: 0.8855, eval loss 1.52921724319458\n",
                  "Epoch 25 train loss: 0.9030, eval loss 1.525841236114502\n",
                  "Epoch 26 train loss: 0.8402, eval loss 1.5220195055007935\n",
                  "Epoch 27 train loss: 0.7994, eval loss 1.5164028406143188\n",
                  "Epoch 28 train loss: 0.7794, eval loss 1.5125811100006104\n",
                  "Epoch 29 train loss: 0.7731, eval loss 1.5112515687942505\n",
                  "Epoch 30 train loss: 0.7969, eval loss 1.507583498954773\n",
                  "Epoch 31 train loss: 0.7911, eval loss 1.5030803680419922\n",
                  "Epoch 32 train loss: 0.7748, eval loss 1.5024003982543945\n",
                  "Epoch 33 train loss: 0.7840, eval loss 1.4984217882156372\n",
                  "Epoch 34 train loss: 0.7990, eval loss 1.4960566759109497\n",
                  "Epoch 35 train loss: 0.8158, eval loss 1.4918735027313232\n",
                  "Epoch 36 train loss: 0.7655, eval loss 1.4911211729049683\n",
                  "Epoch 37 train loss: 0.8321, eval loss 1.4865466356277466\n",
                  "Epoch 38 train loss: 0.7520, eval loss 1.4857897758483887\n",
                  "Epoch 39 train loss: 0.7814, eval loss 1.4837238788604736\n",
                  "Epoch 40 train loss: 0.7813, eval loss 1.4809082746505737\n",
                  "Epoch 41 train loss: 0.7808, eval loss 1.4791823625564575\n",
                  "Epoch 42 train loss: 0.7741, eval loss 1.4785563945770264\n",
                  "Epoch 43 train loss: 0.8160, eval loss 1.4770073890686035\n",
                  "Epoch 44 train loss: 0.7539, eval loss 1.475518822669983\n",
                  "Epoch 45 train loss: 0.8418, eval loss 1.472933292388916\n",
                  "Epoch 46 train loss: 0.7363, eval loss 1.4715173244476318\n",
                  "Epoch 47 train loss: 0.7223, eval loss 1.4707845449447632\n",
                  "Epoch 48 train loss: 0.7975, eval loss 1.4689043760299683\n",
                  "Epoch 49 train loss: 0.7475, eval loss 1.4688429832458496\n",
                  "Epoch 50 train loss: 0.7425, eval loss 1.4660677909851074\n",
                  "Epoch 51 train loss: 0.7824, eval loss 1.4648652076721191\n",
                  "Epoch 52 train loss: 0.7732, eval loss 1.465348720550537\n",
                  "Epoch 53 train loss: 0.7750, eval loss 1.4627032279968262\n",
                  "Epoch 54 train loss: 0.7934, eval loss 1.4623249769210815\n",
                  "Epoch 55 train loss: 0.7408, eval loss 1.4619144201278687\n",
                  "Epoch 56 train loss: 0.7583, eval loss 1.4614382982254028\n",
                  "Epoch 57 train loss: 0.7327, eval loss 1.460315465927124\n",
                  "Epoch 58 train loss: 0.8096, eval loss 1.4585391283035278\n",
                  "Epoch 59 train loss: 0.8152, eval loss 1.4586269855499268\n",
                  "Epoch 60 train loss: 0.7364, eval loss 1.4566106796264648\n",
                  "Epoch 61 train loss: 0.7671, eval loss 1.457824468612671\n",
                  "Epoch 62 train loss: 0.7679, eval loss 1.4559587240219116\n",
                  "Epoch 63 train loss: 0.8489, eval loss 1.4560447931289673\n",
                  "Epoch 64 train loss: 0.7214, eval loss 1.4542845487594604\n",
                  "Epoch 65 train loss: 0.7805, eval loss 1.4550830125808716\n",
                  "Epoch 66 train loss: 0.7583, eval loss 1.4536653757095337\n",
                  "Epoch 67 train loss: 0.8174, eval loss 1.45457124710083\n",
                  "Epoch 68 train loss: 0.7050, eval loss 1.453822135925293\n",
                  "Epoch 69 train loss: 0.6427, eval loss 1.452039122581482\n",
                  "Epoch 70 train loss: 0.7948, eval loss 1.4527071714401245\n",
                  "Epoch 71 train loss: 0.7295, eval loss 1.4510992765426636\n",
                  "Epoch 72 train loss: 0.7939, eval loss 1.451401948928833\n",
                  "Epoch 73 train loss: 0.8261, eval loss 1.4514498710632324\n",
                  "Epoch 74 train loss: 0.7061, eval loss 1.4508124589920044\n",
                  "Epoch 75 train loss: 0.7226, eval loss 1.450002908706665\n",
                  "Epoch 76 train loss: 0.6422, eval loss 1.4498189687728882\n",
                  "Epoch 77 train loss: 0.7945, eval loss 1.4497112035751343\n",
                  "Epoch 78 train loss: 0.8328, eval loss 1.449005126953125\n",
                  "Epoch 79 train loss: 0.7843, eval loss 1.4489245414733887\n",
                  "Epoch 80 train loss: 0.7383, eval loss 1.4492058753967285\n",
                  "Epoch 81 train loss: 0.7819, eval loss 1.448482871055603\n",
                  "Epoch 82 train loss: 0.7445, eval loss 1.4488004446029663\n",
                  "Epoch 83 train loss: 0.7842, eval loss 1.448081135749817\n",
                  "Epoch 84 train loss: 0.7091, eval loss 1.447717547416687\n",
                  "Epoch 85 train loss: 0.7661, eval loss 1.4460344314575195\n",
                  "Epoch 86 train loss: 0.7429, eval loss 1.4473978281021118\n",
                  "Epoch 87 train loss: 0.7140, eval loss 1.4472627639770508\n",
                  "Epoch 88 train loss: 0.7559, eval loss 1.4472023248672485\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:42:41,064] Trial 19 finished with value: 1.4460344314575195 and parameters: {'hidden_layers_size': 75, 'dropout_p': 0.4588624374776901, 'learning_rate': 2.7174157288421963e-05, 'batch_size': 171, 'l2_reg': 4.677829356538687e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4269, eval loss 1.7673331499099731\n",
                  "Epoch 1 train loss: 1.4530, eval loss 1.7630044221878052\n",
                  "Epoch 2 train loss: 1.3523, eval loss 1.7583152055740356\n",
                  "Epoch 3 train loss: 1.3817, eval loss 1.7568224668502808\n",
                  "Epoch 4 train loss: 1.4731, eval loss 1.7527133226394653\n",
                  "Epoch 5 train loss: 1.3015, eval loss 1.7478392124176025\n",
                  "Epoch 6 train loss: 1.4865, eval loss 1.7453664541244507\n",
                  "Epoch 7 train loss: 1.4300, eval loss 1.7425817251205444\n",
                  "Epoch 8 train loss: 1.2708, eval loss 1.738852620124817\n",
                  "Epoch 9 train loss: 1.2890, eval loss 1.7351007461547852\n",
                  "Epoch 10 train loss: 1.2935, eval loss 1.7327368259429932\n",
                  "Epoch 11 train loss: 1.3577, eval loss 1.7305092811584473\n",
                  "Epoch 12 train loss: 1.3683, eval loss 1.7265955209732056\n",
                  "Epoch 13 train loss: 1.2783, eval loss 1.7229878902435303\n",
                  "Epoch 14 train loss: 1.3557, eval loss 1.720778465270996\n",
                  "Epoch 15 train loss: 1.2791, eval loss 1.7178107500076294\n",
                  "Epoch 16 train loss: 1.3057, eval loss 1.7160099744796753\n",
                  "Epoch 17 train loss: 1.2994, eval loss 1.7130117416381836\n",
                  "Epoch 18 train loss: 1.3013, eval loss 1.7086949348449707\n",
                  "Epoch 19 train loss: 1.2894, eval loss 1.706137776374817\n",
                  "Epoch 20 train loss: 1.2883, eval loss 1.7045378684997559\n",
                  "Epoch 21 train loss: 1.1237, eval loss 1.6998927593231201\n",
                  "Epoch 22 train loss: 1.2220, eval loss 1.699904203414917\n",
                  "Epoch 23 train loss: 1.2567, eval loss 1.6960686445236206\n",
                  "Epoch 24 train loss: 1.2209, eval loss 1.6939966678619385\n",
                  "Epoch 25 train loss: 1.1252, eval loss 1.6906827688217163\n",
                  "Epoch 26 train loss: 1.2184, eval loss 1.688563585281372\n",
                  "Epoch 27 train loss: 1.1804, eval loss 1.685500979423523\n",
                  "Epoch 28 train loss: 1.1871, eval loss 1.683287262916565\n",
                  "Epoch 29 train loss: 1.2428, eval loss 1.6816511154174805\n",
                  "Epoch 30 train loss: 1.2048, eval loss 1.6785337924957275\n",
                  "Epoch 31 train loss: 1.1954, eval loss 1.6788541078567505\n",
                  "Epoch 32 train loss: 1.1832, eval loss 1.6737573146820068\n",
                  "Epoch 33 train loss: 1.1478, eval loss 1.6724720001220703\n",
                  "Epoch 34 train loss: 1.1812, eval loss 1.669743299484253\n",
                  "Epoch 35 train loss: 1.0904, eval loss 1.6672452688217163\n",
                  "Epoch 36 train loss: 1.1298, eval loss 1.663902997970581\n",
                  "Epoch 37 train loss: 1.1082, eval loss 1.662822961807251\n",
                  "Epoch 38 train loss: 1.1234, eval loss 1.661940336227417\n",
                  "Epoch 39 train loss: 1.0809, eval loss 1.6575700044631958\n",
                  "Epoch 40 train loss: 1.1318, eval loss 1.6576111316680908\n",
                  "Epoch 41 train loss: 1.0894, eval loss 1.65571129322052\n",
                  "Epoch 42 train loss: 1.0538, eval loss 1.6524832248687744\n",
                  "Epoch 43 train loss: 1.1231, eval loss 1.6515247821807861\n",
                  "Epoch 44 train loss: 1.1146, eval loss 1.6496491432189941\n",
                  "Epoch 45 train loss: 1.0186, eval loss 1.646246314048767\n",
                  "Epoch 46 train loss: 1.0670, eval loss 1.6448777914047241\n",
                  "Epoch 47 train loss: 1.1173, eval loss 1.6442506313323975\n",
                  "Epoch 48 train loss: 1.0347, eval loss 1.6416758298873901\n",
                  "Epoch 49 train loss: 1.0999, eval loss 1.6392885446548462\n",
                  "Epoch 50 train loss: 1.0429, eval loss 1.6372931003570557\n",
                  "Epoch 51 train loss: 1.0428, eval loss 1.6357113122940063\n",
                  "Epoch 52 train loss: 0.9868, eval loss 1.6352057456970215\n",
                  "Epoch 53 train loss: 0.9727, eval loss 1.6328517198562622\n",
                  "Epoch 54 train loss: 1.1011, eval loss 1.6321595907211304\n",
                  "Epoch 55 train loss: 1.0286, eval loss 1.6296125650405884\n",
                  "Epoch 56 train loss: 0.9797, eval loss 1.6280646324157715\n",
                  "Epoch 57 train loss: 1.0543, eval loss 1.6261872053146362\n",
                  "Epoch 58 train loss: 1.0125, eval loss 1.6244335174560547\n",
                  "Epoch 59 train loss: 1.0558, eval loss 1.6233570575714111\n",
                  "Epoch 60 train loss: 1.0378, eval loss 1.6222556829452515\n",
                  "Epoch 61 train loss: 0.9566, eval loss 1.6213167905807495\n",
                  "Epoch 62 train loss: 1.0098, eval loss 1.6188651323318481\n",
                  "Epoch 63 train loss: 1.0004, eval loss 1.6164735555648804\n",
                  "Epoch 64 train loss: 0.9889, eval loss 1.6159043312072754\n",
                  "Epoch 65 train loss: 0.9346, eval loss 1.6156089305877686\n",
                  "Epoch 66 train loss: 0.9727, eval loss 1.6133579015731812\n",
                  "Epoch 67 train loss: 1.0540, eval loss 1.6125341653823853\n",
                  "Epoch 68 train loss: 1.0025, eval loss 1.6110576391220093\n",
                  "Epoch 69 train loss: 1.0544, eval loss 1.609477162361145\n",
                  "Epoch 70 train loss: 0.9384, eval loss 1.6054922342300415\n",
                  "Epoch 71 train loss: 0.9472, eval loss 1.6054461002349854\n",
                  "Epoch 72 train loss: 1.0473, eval loss 1.60489821434021\n",
                  "Epoch 73 train loss: 0.9845, eval loss 1.6044921875\n",
                  "Epoch 74 train loss: 0.9386, eval loss 1.6026074886322021\n",
                  "Epoch 75 train loss: 1.0192, eval loss 1.60328209400177\n",
                  "Epoch 76 train loss: 0.9403, eval loss 1.6001654863357544\n",
                  "Epoch 77 train loss: 0.9749, eval loss 1.5984513759613037\n",
                  "Epoch 78 train loss: 0.9764, eval loss 1.597381591796875\n",
                  "Epoch 79 train loss: 0.9538, eval loss 1.5966540575027466\n",
                  "Epoch 80 train loss: 0.9529, eval loss 1.5937621593475342\n",
                  "Epoch 81 train loss: 0.9384, eval loss 1.5946931838989258\n",
                  "Epoch 82 train loss: 0.9902, eval loss 1.5935704708099365\n",
                  "Epoch 83 train loss: 0.9249, eval loss 1.591626524925232\n",
                  "Epoch 84 train loss: 0.9809, eval loss 1.588984489440918\n",
                  "Epoch 85 train loss: 0.9287, eval loss 1.5897369384765625\n",
                  "Epoch 86 train loss: 0.8836, eval loss 1.5886932611465454\n",
                  "Epoch 87 train loss: 0.8911, eval loss 1.5868459939956665\n",
                  "Epoch 88 train loss: 0.9211, eval loss 1.5845189094543457\n",
                  "Epoch 89 train loss: 0.9213, eval loss 1.5853534936904907\n",
                  "Epoch 90 train loss: 1.0039, eval loss 1.5839834213256836\n",
                  "Epoch 91 train loss: 0.9146, eval loss 1.5823851823806763\n",
                  "Epoch 92 train loss: 0.9130, eval loss 1.5799546241760254\n",
                  "Epoch 93 train loss: 0.9635, eval loss 1.5809227228164673\n",
                  "Epoch 94 train loss: 0.9768, eval loss 1.581597924232483\n",
                  "Epoch 95 train loss: 0.8882, eval loss 1.5780303478240967\n",
                  "Epoch 96 train loss: 0.8612, eval loss 1.576328158378601\n",
                  "Epoch 97 train loss: 0.9160, eval loss 1.5769944190979004\n",
                  "Epoch 98 train loss: 0.8941, eval loss 1.5753928422927856\n",
                  "Epoch 99 train loss: 0.9174, eval loss 1.5740717649459839\n",
                  "Epoch 100 train loss: 0.8949, eval loss 1.573195457458496\n",
                  "Epoch 101 train loss: 0.9514, eval loss 1.5720970630645752\n",
                  "Epoch 102 train loss: 0.8503, eval loss 1.5713040828704834\n",
                  "Epoch 103 train loss: 0.8819, eval loss 1.5704208612442017\n",
                  "Epoch 104 train loss: 0.9319, eval loss 1.5694799423217773\n",
                  "Epoch 105 train loss: 0.8716, eval loss 1.5681172609329224\n",
                  "Epoch 106 train loss: 0.9180, eval loss 1.5668061971664429\n",
                  "Epoch 107 train loss: 0.8340, eval loss 1.5674591064453125\n",
                  "Epoch 108 train loss: 0.8636, eval loss 1.5664979219436646\n",
                  "Epoch 109 train loss: 0.9107, eval loss 1.565218210220337\n",
                  "Epoch 110 train loss: 0.8316, eval loss 1.5632264614105225\n",
                  "Epoch 111 train loss: 0.9185, eval loss 1.5633087158203125\n",
                  "Epoch 112 train loss: 0.8126, eval loss 1.561916708946228\n",
                  "Epoch 113 train loss: 0.9063, eval loss 1.5604935884475708\n",
                  "Epoch 114 train loss: 0.8631, eval loss 1.5601571798324585\n",
                  "Epoch 115 train loss: 0.8783, eval loss 1.5582797527313232\n",
                  "Epoch 116 train loss: 0.9273, eval loss 1.5595866441726685\n",
                  "Epoch 117 train loss: 0.8815, eval loss 1.556955099105835\n",
                  "Epoch 118 train loss: 0.8284, eval loss 1.5569323301315308\n",
                  "Epoch 119 train loss: 0.8727, eval loss 1.5554864406585693\n",
                  "Epoch 120 train loss: 0.8359, eval loss 1.5553467273712158\n",
                  "Epoch 121 train loss: 0.9118, eval loss 1.5546088218688965\n",
                  "Epoch 122 train loss: 0.9503, eval loss 1.5535914897918701\n",
                  "Epoch 123 train loss: 0.8926, eval loss 1.5515034198760986\n",
                  "Epoch 124 train loss: 0.8173, eval loss 1.5513049364089966\n",
                  "Epoch 125 train loss: 0.8077, eval loss 1.5516326427459717\n",
                  "Epoch 126 train loss: 0.8541, eval loss 1.5507276058197021\n",
                  "Epoch 127 train loss: 0.8332, eval loss 1.5505772829055786\n",
                  "Epoch 128 train loss: 0.8252, eval loss 1.5488331317901611\n",
                  "Epoch 129 train loss: 0.8678, eval loss 1.5484150648117065\n",
                  "Epoch 130 train loss: 0.9102, eval loss 1.546929121017456\n",
                  "Epoch 131 train loss: 0.8551, eval loss 1.5471117496490479\n",
                  "Epoch 132 train loss: 0.8272, eval loss 1.5448980331420898\n",
                  "Epoch 133 train loss: 0.8180, eval loss 1.5453758239746094\n",
                  "Epoch 134 train loss: 0.8502, eval loss 1.5431642532348633\n",
                  "Epoch 135 train loss: 0.8968, eval loss 1.5430562496185303\n",
                  "Epoch 136 train loss: 0.8317, eval loss 1.5432058572769165\n",
                  "Epoch 137 train loss: 0.8370, eval loss 1.541237235069275\n",
                  "Epoch 138 train loss: 0.8309, eval loss 1.5410281419754028\n",
                  "Epoch 139 train loss: 0.8866, eval loss 1.5408674478530884\n",
                  "Epoch 140 train loss: 0.9000, eval loss 1.5390127897262573\n",
                  "Epoch 141 train loss: 0.7722, eval loss 1.5389695167541504\n",
                  "Epoch 142 train loss: 0.8511, eval loss 1.5378317832946777\n",
                  "Epoch 143 train loss: 0.8052, eval loss 1.5371854305267334\n",
                  "Epoch 144 train loss: 0.8267, eval loss 1.5371801853179932\n",
                  "Epoch 145 train loss: 0.8429, eval loss 1.5356162786483765\n",
                  "Epoch 146 train loss: 0.8578, eval loss 1.5345615148544312\n",
                  "Epoch 147 train loss: 0.8684, eval loss 1.5337110757827759\n",
                  "Epoch 148 train loss: 0.8118, eval loss 1.5322023630142212\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:44:11,008] Trial 20 finished with value: 1.5322023630142212 and parameters: {'hidden_layers_size': 122, 'dropout_p': 0.41861694589624343, 'learning_rate': 3.0356108100441192e-06, 'batch_size': 212, 'l2_reg': 3.439449349888914e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.8130, eval loss 1.5334504842758179\n",
                  "Epoch 0 train loss: 1.3047, eval loss 1.725730299949646\n",
                  "Epoch 1 train loss: 1.2731, eval loss 1.7242501974105835\n",
                  "Epoch 2 train loss: 1.1229, eval loss 1.7224653959274292\n",
                  "Epoch 3 train loss: 1.2695, eval loss 1.7231342792510986\n",
                  "Epoch 4 train loss: 1.2164, eval loss 1.7227023839950562\n",
                  "Epoch 5 train loss: 1.3150, eval loss 1.7227174043655396\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:44:15,504] Trial 21 finished with value: 1.7224653959274292 and parameters: {'hidden_layers_size': 71, 'dropout_p': 0.49736577627839546, 'learning_rate': 1.0551039883362064e-06, 'batch_size': 198, 'l2_reg': 2.6094790630653748e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.3705, eval loss 1.733750581741333\n",
                  "Epoch 1 train loss: 1.3743, eval loss 1.731155514717102\n",
                  "Epoch 2 train loss: 1.3849, eval loss 1.7319997549057007\n",
                  "Epoch 3 train loss: 1.2764, eval loss 1.7319679260253906\n",
                  "Epoch 4 train loss: 1.3692, eval loss 1.7291090488433838\n",
                  "Epoch 5 train loss: 1.3313, eval loss 1.728620171546936\n",
                  "Epoch 6 train loss: 1.3177, eval loss 1.7274119853973389\n",
                  "Epoch 7 train loss: 1.3013, eval loss 1.7271579504013062\n",
                  "Epoch 8 train loss: 1.2869, eval loss 1.7253220081329346\n",
                  "Epoch 9 train loss: 1.3078, eval loss 1.7242521047592163\n",
                  "Epoch 10 train loss: 1.3349, eval loss 1.7224780321121216\n",
                  "Epoch 11 train loss: 1.2946, eval loss 1.7216899394989014\n",
                  "Epoch 12 train loss: 1.2717, eval loss 1.7211766242980957\n",
                  "Epoch 13 train loss: 1.3488, eval loss 1.7201303243637085\n",
                  "Epoch 14 train loss: 1.3259, eval loss 1.7197123765945435\n",
                  "Epoch 15 train loss: 1.3178, eval loss 1.716759443283081\n",
                  "Epoch 16 train loss: 1.3303, eval loss 1.7160741090774536\n",
                  "Epoch 17 train loss: 1.3054, eval loss 1.7153841257095337\n",
                  "Epoch 18 train loss: 1.3528, eval loss 1.714979648590088\n",
                  "Epoch 19 train loss: 1.2877, eval loss 1.713847279548645\n",
                  "Epoch 20 train loss: 1.3019, eval loss 1.7107549905776978\n",
                  "Epoch 21 train loss: 1.2999, eval loss 1.710797667503357\n",
                  "Epoch 22 train loss: 1.2636, eval loss 1.7098394632339478\n",
                  "Epoch 23 train loss: 1.2630, eval loss 1.709180474281311\n",
                  "Epoch 24 train loss: 1.2589, eval loss 1.7079102993011475\n",
                  "Epoch 25 train loss: 1.3377, eval loss 1.7067316770553589\n",
                  "Epoch 26 train loss: 1.2558, eval loss 1.7055634260177612\n",
                  "Epoch 27 train loss: 1.2217, eval loss 1.7050402164459229\n",
                  "Epoch 28 train loss: 1.2864, eval loss 1.7039774656295776\n",
                  "Epoch 29 train loss: 1.2974, eval loss 1.7036683559417725\n",
                  "Epoch 30 train loss: 1.2831, eval loss 1.702599048614502\n",
                  "Epoch 31 train loss: 1.2596, eval loss 1.7000839710235596\n",
                  "Epoch 32 train loss: 1.1772, eval loss 1.699746012687683\n",
                  "Epoch 33 train loss: 1.2184, eval loss 1.6981828212738037\n",
                  "Epoch 34 train loss: 1.2913, eval loss 1.6986676454544067\n",
                  "Epoch 35 train loss: 1.2875, eval loss 1.6969705820083618\n",
                  "Epoch 36 train loss: 1.2444, eval loss 1.6974561214447021\n",
                  "Epoch 37 train loss: 1.3087, eval loss 1.6948143243789673\n",
                  "Epoch 38 train loss: 1.3094, eval loss 1.6943782567977905\n",
                  "Epoch 39 train loss: 1.2628, eval loss 1.6923824548721313\n",
                  "Epoch 40 train loss: 1.2365, eval loss 1.6932772397994995\n",
                  "Epoch 41 train loss: 1.2799, eval loss 1.6908628940582275\n",
                  "Epoch 42 train loss: 1.2883, eval loss 1.6889207363128662\n",
                  "Epoch 43 train loss: 1.2860, eval loss 1.6887258291244507\n",
                  "Epoch 44 train loss: 1.1552, eval loss 1.688542127609253\n",
                  "Epoch 45 train loss: 1.2627, eval loss 1.6868417263031006\n",
                  "Epoch 46 train loss: 1.2356, eval loss 1.6869571208953857\n",
                  "Epoch 47 train loss: 1.2442, eval loss 1.685944676399231\n",
                  "Epoch 48 train loss: 1.2139, eval loss 1.684516191482544\n",
                  "Epoch 49 train loss: 1.1889, eval loss 1.6828631162643433\n",
                  "Epoch 50 train loss: 1.2898, eval loss 1.6828241348266602\n",
                  "Epoch 51 train loss: 1.1940, eval loss 1.6806361675262451\n",
                  "Epoch 52 train loss: 1.1484, eval loss 1.6805193424224854\n",
                  "Epoch 53 train loss: 1.1584, eval loss 1.6787465810775757\n",
                  "Epoch 54 train loss: 1.2127, eval loss 1.6783390045166016\n",
                  "Epoch 55 train loss: 1.1654, eval loss 1.678597331047058\n",
                  "Epoch 56 train loss: 1.1647, eval loss 1.6768004894256592\n",
                  "Epoch 57 train loss: 1.2513, eval loss 1.6734812259674072\n",
                  "Epoch 58 train loss: 1.1906, eval loss 1.6740549802780151\n",
                  "Epoch 59 train loss: 1.1351, eval loss 1.673482894897461\n",
                  "Epoch 60 train loss: 1.1911, eval loss 1.6734989881515503\n",
                  "Epoch 61 train loss: 1.2208, eval loss 1.6722768545150757\n",
                  "Epoch 62 train loss: 1.2728, eval loss 1.67180597782135\n",
                  "Epoch 63 train loss: 1.2050, eval loss 1.6707137823104858\n",
                  "Epoch 64 train loss: 1.1818, eval loss 1.6696107387542725\n",
                  "Epoch 65 train loss: 1.1131, eval loss 1.6680840253829956\n",
                  "Epoch 66 train loss: 1.1932, eval loss 1.6681917905807495\n",
                  "Epoch 67 train loss: 1.1656, eval loss 1.6658263206481934\n",
                  "Epoch 68 train loss: 1.2114, eval loss 1.6650910377502441\n",
                  "Epoch 69 train loss: 1.1692, eval loss 1.6651313304901123\n",
                  "Epoch 70 train loss: 1.1479, eval loss 1.6641918420791626\n",
                  "Epoch 71 train loss: 1.2021, eval loss 1.6627424955368042\n",
                  "Epoch 72 train loss: 1.1372, eval loss 1.6627672910690308\n",
                  "Epoch 73 train loss: 1.1449, eval loss 1.6614347696304321\n",
                  "Epoch 74 train loss: 1.1844, eval loss 1.6608390808105469\n",
                  "Epoch 75 train loss: 1.1645, eval loss 1.6599299907684326\n",
                  "Epoch 76 train loss: 1.1493, eval loss 1.658995270729065\n",
                  "Epoch 77 train loss: 1.1848, eval loss 1.6577764749526978\n",
                  "Epoch 78 train loss: 1.1267, eval loss 1.6569712162017822\n",
                  "Epoch 79 train loss: 1.1027, eval loss 1.656029224395752\n",
                  "Epoch 80 train loss: 1.1888, eval loss 1.6558458805084229\n",
                  "Epoch 81 train loss: 1.1475, eval loss 1.6549596786499023\n",
                  "Epoch 82 train loss: 1.1338, eval loss 1.653656005859375\n",
                  "Epoch 83 train loss: 1.1668, eval loss 1.6539651155471802\n",
                  "Epoch 84 train loss: 1.1613, eval loss 1.6520715951919556\n",
                  "Epoch 85 train loss: 1.1640, eval loss 1.6517417430877686\n",
                  "Epoch 86 train loss: 1.1216, eval loss 1.6508949995040894\n",
                  "Epoch 87 train loss: 1.1173, eval loss 1.6500529050827026\n",
                  "Epoch 88 train loss: 1.1152, eval loss 1.6490997076034546\n",
                  "Epoch 89 train loss: 1.1668, eval loss 1.646999478340149\n",
                  "Epoch 90 train loss: 1.1435, eval loss 1.6472240686416626\n",
                  "Epoch 91 train loss: 1.1106, eval loss 1.6454671621322632\n",
                  "Epoch 92 train loss: 1.1422, eval loss 1.6451047658920288\n",
                  "Epoch 93 train loss: 1.1422, eval loss 1.6445043087005615\n",
                  "Epoch 94 train loss: 1.1407, eval loss 1.6457023620605469\n",
                  "Epoch 95 train loss: 1.0431, eval loss 1.6438438892364502\n",
                  "Epoch 96 train loss: 1.1217, eval loss 1.6423280239105225\n",
                  "Epoch 97 train loss: 1.1230, eval loss 1.6421599388122559\n",
                  "Epoch 98 train loss: 1.0131, eval loss 1.641148328781128\n",
                  "Epoch 99 train loss: 1.0565, eval loss 1.6418731212615967\n",
                  "Epoch 100 train loss: 1.0461, eval loss 1.6414194107055664\n",
                  "Epoch 101 train loss: 1.0924, eval loss 1.6396631002426147\n",
                  "Epoch 102 train loss: 1.1315, eval loss 1.6384748220443726\n",
                  "Epoch 103 train loss: 1.1041, eval loss 1.6377041339874268\n",
                  "Epoch 104 train loss: 1.0586, eval loss 1.6372647285461426\n",
                  "Epoch 105 train loss: 1.0267, eval loss 1.6360137462615967\n",
                  "Epoch 106 train loss: 1.1138, eval loss 1.6362828016281128\n",
                  "Epoch 107 train loss: 1.0691, eval loss 1.6342219114303589\n",
                  "Epoch 108 train loss: 1.0741, eval loss 1.6343525648117065\n",
                  "Epoch 109 train loss: 1.1463, eval loss 1.6325238943099976\n",
                  "Epoch 110 train loss: 1.0909, eval loss 1.632598876953125\n",
                  "Epoch 111 train loss: 1.1276, eval loss 1.6305137872695923\n",
                  "Epoch 112 train loss: 1.0242, eval loss 1.630598783493042\n",
                  "Epoch 113 train loss: 1.0996, eval loss 1.6300376653671265\n",
                  "Epoch 114 train loss: 1.0995, eval loss 1.6278963088989258\n",
                  "Epoch 115 train loss: 1.0172, eval loss 1.6277921199798584\n",
                  "Epoch 116 train loss: 1.0796, eval loss 1.6273844242095947\n",
                  "Epoch 117 train loss: 1.0318, eval loss 1.6271922588348389\n",
                  "Epoch 118 train loss: 1.0542, eval loss 1.6270301342010498\n",
                  "Epoch 119 train loss: 1.0534, eval loss 1.6263940334320068\n",
                  "Epoch 120 train loss: 1.0192, eval loss 1.6252282857894897\n",
                  "Epoch 121 train loss: 1.1287, eval loss 1.624140977859497\n",
                  "Epoch 122 train loss: 1.0502, eval loss 1.624626636505127\n",
                  "Epoch 123 train loss: 1.0417, eval loss 1.6228504180908203\n",
                  "Epoch 124 train loss: 1.0302, eval loss 1.6211061477661133\n",
                  "Epoch 125 train loss: 0.9984, eval loss 1.620654821395874\n",
                  "Epoch 126 train loss: 1.0991, eval loss 1.6217716932296753\n",
                  "Epoch 127 train loss: 1.1200, eval loss 1.619053602218628\n",
                  "Epoch 128 train loss: 1.1091, eval loss 1.619455337524414\n",
                  "Epoch 129 train loss: 1.0784, eval loss 1.6197524070739746\n",
                  "Epoch 130 train loss: 1.0075, eval loss 1.6179970502853394\n",
                  "Epoch 131 train loss: 1.0474, eval loss 1.6177520751953125\n",
                  "Epoch 132 train loss: 1.0710, eval loss 1.6163396835327148\n",
                  "Epoch 133 train loss: 1.0423, eval loss 1.6145164966583252\n",
                  "Epoch 134 train loss: 1.0222, eval loss 1.615534782409668\n",
                  "Epoch 135 train loss: 1.0395, eval loss 1.614393949508667\n",
                  "Epoch 136 train loss: 0.9983, eval loss 1.6150981187820435\n",
                  "Epoch 137 train loss: 1.0423, eval loss 1.6142224073410034\n",
                  "Epoch 138 train loss: 1.0493, eval loss 1.6143430471420288\n",
                  "Epoch 139 train loss: 1.0362, eval loss 1.613107442855835\n",
                  "Epoch 140 train loss: 1.0285, eval loss 1.61228609085083\n",
                  "Epoch 141 train loss: 1.0200, eval loss 1.6115580797195435\n",
                  "Epoch 142 train loss: 1.0751, eval loss 1.610437273979187\n",
                  "Epoch 143 train loss: 1.0698, eval loss 1.6107455492019653\n",
                  "Epoch 144 train loss: 1.0432, eval loss 1.6084901094436646\n",
                  "Epoch 145 train loss: 1.0258, eval loss 1.6079570055007935\n",
                  "Epoch 146 train loss: 1.0511, eval loss 1.60891854763031\n",
                  "Epoch 147 train loss: 1.1164, eval loss 1.6079124212265015\n",
                  "Epoch 148 train loss: 1.0058, eval loss 1.6074211597442627\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:45:39,605] Trial 22 finished with value: 1.6054496765136719 and parameters: {'hidden_layers_size': 84, 'dropout_p': 0.47029535203187567, 'learning_rate': 1.4477541362750555e-06, 'batch_size': 185, 'l2_reg': 2.360547036473938e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 1.0208, eval loss 1.6054496765136719\n",
                  "Epoch 0 train loss: 1.5935, eval loss 1.7880141735076904\n",
                  "Epoch 1 train loss: 1.5351, eval loss 1.7857710123062134\n",
                  "Epoch 2 train loss: 1.5461, eval loss 1.7841145992279053\n",
                  "Epoch 3 train loss: 1.4897, eval loss 1.7827039957046509\n",
                  "Epoch 4 train loss: 1.4503, eval loss 1.7825782299041748\n",
                  "Epoch 5 train loss: 1.5530, eval loss 1.781283974647522\n",
                  "Epoch 6 train loss: 1.5136, eval loss 1.779823899269104\n",
                  "Epoch 7 train loss: 1.5754, eval loss 1.7769471406936646\n",
                  "Epoch 8 train loss: 1.5425, eval loss 1.7757607698440552\n",
                  "Epoch 9 train loss: 1.5614, eval loss 1.7736562490463257\n",
                  "Epoch 10 train loss: 1.4793, eval loss 1.7723870277404785\n",
                  "Epoch 11 train loss: 1.4729, eval loss 1.7714372873306274\n",
                  "Epoch 12 train loss: 1.4448, eval loss 1.771135687828064\n",
                  "Epoch 13 train loss: 1.4714, eval loss 1.768185019493103\n",
                  "Epoch 14 train loss: 1.4410, eval loss 1.7664580345153809\n",
                  "Epoch 15 train loss: 1.4678, eval loss 1.7658615112304688\n",
                  "Epoch 16 train loss: 1.4633, eval loss 1.7633495330810547\n",
                  "Epoch 17 train loss: 1.4400, eval loss 1.7631474733352661\n",
                  "Epoch 18 train loss: 1.4228, eval loss 1.7615065574645996\n",
                  "Epoch 19 train loss: 1.3871, eval loss 1.7610012292861938\n",
                  "Epoch 20 train loss: 1.4665, eval loss 1.7583627700805664\n",
                  "Epoch 21 train loss: 1.4885, eval loss 1.7572134733200073\n",
                  "Epoch 22 train loss: 1.4407, eval loss 1.7563495635986328\n",
                  "Epoch 23 train loss: 1.4162, eval loss 1.7546942234039307\n",
                  "Epoch 24 train loss: 1.4607, eval loss 1.7530972957611084\n",
                  "Epoch 25 train loss: 1.3852, eval loss 1.752567172050476\n",
                  "Epoch 26 train loss: 1.3528, eval loss 1.750510573387146\n",
                  "Epoch 27 train loss: 1.4447, eval loss 1.7494328022003174\n",
                  "Epoch 28 train loss: 1.4090, eval loss 1.7472563982009888\n",
                  "Epoch 29 train loss: 1.4602, eval loss 1.7464513778686523\n",
                  "Epoch 30 train loss: 1.3401, eval loss 1.7444686889648438\n",
                  "Epoch 31 train loss: 1.3789, eval loss 1.7438815832138062\n",
                  "Epoch 32 train loss: 1.4337, eval loss 1.7424105405807495\n",
                  "Epoch 33 train loss: 1.4119, eval loss 1.740817904472351\n",
                  "Epoch 34 train loss: 1.3901, eval loss 1.7404839992523193\n",
                  "Epoch 35 train loss: 1.4129, eval loss 1.7389905452728271\n",
                  "Epoch 36 train loss: 1.4048, eval loss 1.7375463247299194\n",
                  "Epoch 37 train loss: 1.3694, eval loss 1.7357193231582642\n",
                  "Epoch 38 train loss: 1.3544, eval loss 1.7345114946365356\n",
                  "Epoch 39 train loss: 1.3505, eval loss 1.7323508262634277\n",
                  "Epoch 40 train loss: 1.3589, eval loss 1.7305388450622559\n",
                  "Epoch 41 train loss: 1.3170, eval loss 1.7314262390136719\n",
                  "Epoch 42 train loss: 1.3939, eval loss 1.7286044359207153\n",
                  "Epoch 43 train loss: 1.4110, eval loss 1.7283326387405396\n",
                  "Epoch 44 train loss: 1.2941, eval loss 1.727417230606079\n",
                  "Epoch 45 train loss: 1.3187, eval loss 1.725979208946228\n",
                  "Epoch 46 train loss: 1.2600, eval loss 1.723677396774292\n",
                  "Epoch 47 train loss: 1.3153, eval loss 1.7235126495361328\n",
                  "Epoch 48 train loss: 1.2610, eval loss 1.7206988334655762\n",
                  "Epoch 49 train loss: 1.3305, eval loss 1.7214537858963013\n",
                  "Epoch 50 train loss: 1.2817, eval loss 1.719382405281067\n",
                  "Epoch 51 train loss: 1.3510, eval loss 1.717875361442566\n",
                  "Epoch 52 train loss: 1.2328, eval loss 1.7165358066558838\n",
                  "Epoch 53 train loss: 1.3329, eval loss 1.715855598449707\n",
                  "Epoch 54 train loss: 1.3135, eval loss 1.7143968343734741\n",
                  "Epoch 55 train loss: 1.1967, eval loss 1.7124384641647339\n",
                  "Epoch 56 train loss: 1.3139, eval loss 1.7108107805252075\n",
                  "Epoch 57 train loss: 1.2604, eval loss 1.7107669115066528\n",
                  "Epoch 58 train loss: 1.2867, eval loss 1.7086666822433472\n",
                  "Epoch 59 train loss: 1.2541, eval loss 1.708951711654663\n",
                  "Epoch 60 train loss: 1.2520, eval loss 1.707767128944397\n",
                  "Epoch 61 train loss: 1.3067, eval loss 1.706236720085144\n",
                  "Epoch 62 train loss: 1.2809, eval loss 1.7050538063049316\n",
                  "Epoch 63 train loss: 1.2274, eval loss 1.7028656005859375\n",
                  "Epoch 64 train loss: 1.3166, eval loss 1.7037975788116455\n",
                  "Epoch 65 train loss: 1.3104, eval loss 1.6998305320739746\n",
                  "Epoch 66 train loss: 1.2926, eval loss 1.7010048627853394\n",
                  "Epoch 67 train loss: 1.3039, eval loss 1.6992483139038086\n",
                  "Epoch 68 train loss: 1.2155, eval loss 1.6995999813079834\n",
                  "Epoch 69 train loss: 1.2019, eval loss 1.6974924802780151\n",
                  "Epoch 70 train loss: 1.2102, eval loss 1.6958646774291992\n",
                  "Epoch 71 train loss: 1.2045, eval loss 1.694856882095337\n",
                  "Epoch 72 train loss: 1.2488, eval loss 1.6936492919921875\n",
                  "Epoch 73 train loss: 1.2208, eval loss 1.6925960779190063\n",
                  "Epoch 74 train loss: 1.1970, eval loss 1.6903376579284668\n",
                  "Epoch 75 train loss: 1.2320, eval loss 1.690843939781189\n",
                  "Epoch 76 train loss: 1.1987, eval loss 1.6896551847457886\n",
                  "Epoch 77 train loss: 1.2319, eval loss 1.689412236213684\n",
                  "Epoch 78 train loss: 1.2234, eval loss 1.688734531402588\n",
                  "Epoch 79 train loss: 1.2498, eval loss 1.6859339475631714\n",
                  "Epoch 80 train loss: 1.2077, eval loss 1.684126615524292\n",
                  "Epoch 81 train loss: 1.1737, eval loss 1.6850192546844482\n",
                  "Epoch 82 train loss: 1.2154, eval loss 1.6830295324325562\n",
                  "Epoch 83 train loss: 1.2352, eval loss 1.6831227540969849\n",
                  "Epoch 84 train loss: 1.1488, eval loss 1.6795365810394287\n",
                  "Epoch 85 train loss: 1.1342, eval loss 1.6799654960632324\n",
                  "Epoch 86 train loss: 1.2107, eval loss 1.6795397996902466\n",
                  "Epoch 87 train loss: 1.1494, eval loss 1.6776201725006104\n",
                  "Epoch 88 train loss: 1.2369, eval loss 1.676268219947815\n",
                  "Epoch 89 train loss: 1.1441, eval loss 1.6748626232147217\n",
                  "Epoch 90 train loss: 1.2213, eval loss 1.6754790544509888\n",
                  "Epoch 91 train loss: 1.1870, eval loss 1.673515796661377\n",
                  "Epoch 92 train loss: 1.1453, eval loss 1.6729786396026611\n",
                  "Epoch 93 train loss: 1.1632, eval loss 1.6719518899917603\n",
                  "Epoch 94 train loss: 1.2167, eval loss 1.670576572418213\n",
                  "Epoch 95 train loss: 1.1869, eval loss 1.6709797382354736\n",
                  "Epoch 96 train loss: 1.1822, eval loss 1.6698026657104492\n",
                  "Epoch 97 train loss: 1.1911, eval loss 1.6690245866775513\n",
                  "Epoch 98 train loss: 1.2527, eval loss 1.6658203601837158\n",
                  "Epoch 99 train loss: 1.1220, eval loss 1.6653409004211426\n",
                  "Epoch 100 train loss: 1.1436, eval loss 1.6656262874603271\n",
                  "Epoch 101 train loss: 1.1550, eval loss 1.6644266843795776\n",
                  "Epoch 102 train loss: 1.1144, eval loss 1.6630940437316895\n",
                  "Epoch 103 train loss: 1.1216, eval loss 1.6615591049194336\n",
                  "Epoch 104 train loss: 1.1542, eval loss 1.660296082496643\n",
                  "Epoch 105 train loss: 1.0979, eval loss 1.6597023010253906\n",
                  "Epoch 106 train loss: 1.1257, eval loss 1.6593830585479736\n",
                  "Epoch 107 train loss: 1.0805, eval loss 1.6591391563415527\n",
                  "Epoch 108 train loss: 1.1684, eval loss 1.6582908630371094\n",
                  "Epoch 109 train loss: 1.1554, eval loss 1.6570606231689453\n",
                  "Epoch 110 train loss: 1.0918, eval loss 1.6561259031295776\n",
                  "Epoch 111 train loss: 1.1547, eval loss 1.6552890539169312\n",
                  "Epoch 112 train loss: 1.1618, eval loss 1.6517044305801392\n",
                  "Epoch 113 train loss: 1.0699, eval loss 1.6523261070251465\n",
                  "Epoch 114 train loss: 1.0840, eval loss 1.6504502296447754\n",
                  "Epoch 115 train loss: 1.1129, eval loss 1.6514098644256592\n",
                  "Epoch 116 train loss: 1.0331, eval loss 1.6508616209030151\n",
                  "Epoch 117 train loss: 1.0353, eval loss 1.649013876914978\n",
                  "Epoch 118 train loss: 1.0308, eval loss 1.648156762123108\n",
                  "Epoch 119 train loss: 1.0804, eval loss 1.6478043794631958\n",
                  "Epoch 120 train loss: 1.1206, eval loss 1.645969271659851\n",
                  "Epoch 121 train loss: 1.1169, eval loss 1.6463595628738403\n",
                  "Epoch 122 train loss: 1.1667, eval loss 1.645134449005127\n",
                  "Epoch 123 train loss: 1.1528, eval loss 1.644189715385437\n",
                  "Epoch 124 train loss: 1.0961, eval loss 1.6430745124816895\n",
                  "Epoch 125 train loss: 1.0439, eval loss 1.641608715057373\n",
                  "Epoch 126 train loss: 1.0221, eval loss 1.6417489051818848\n",
                  "Epoch 127 train loss: 1.0161, eval loss 1.641656517982483\n",
                  "Epoch 128 train loss: 1.1446, eval loss 1.6412816047668457\n",
                  "Epoch 129 train loss: 1.0373, eval loss 1.6390169858932495\n",
                  "Epoch 130 train loss: 1.1071, eval loss 1.6384353637695312\n",
                  "Epoch 131 train loss: 1.0638, eval loss 1.6381710767745972\n",
                  "Epoch 132 train loss: 1.0723, eval loss 1.6394087076187134\n",
                  "Epoch 133 train loss: 1.1222, eval loss 1.6367027759552002\n",
                  "Epoch 134 train loss: 1.0875, eval loss 1.6354811191558838\n",
                  "Epoch 135 train loss: 1.0545, eval loss 1.6345593929290771\n",
                  "Epoch 136 train loss: 1.0479, eval loss 1.63283371925354\n",
                  "Epoch 137 train loss: 1.0655, eval loss 1.6309117078781128\n",
                  "Epoch 138 train loss: 1.0605, eval loss 1.6311973333358765\n",
                  "Epoch 139 train loss: 1.1031, eval loss 1.6306841373443604\n",
                  "Epoch 140 train loss: 1.0302, eval loss 1.6308344602584839\n",
                  "Epoch 141 train loss: 1.0621, eval loss 1.6296682357788086\n",
                  "Epoch 142 train loss: 1.0524, eval loss 1.6272556781768799\n",
                  "Epoch 143 train loss: 1.0577, eval loss 1.6263642311096191\n",
                  "Epoch 144 train loss: 1.0353, eval loss 1.6273634433746338\n",
                  "Epoch 145 train loss: 1.0454, eval loss 1.6251420974731445\n",
                  "Epoch 146 train loss: 1.0160, eval loss 1.6246689558029175\n",
                  "Epoch 147 train loss: 0.9668, eval loss 1.6229764223098755\n",
                  "Epoch 148 train loss: 1.0688, eval loss 1.6219995021820068\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:46:49,695] Trial 23 finished with value: 1.6219995021820068 and parameters: {'hidden_layers_size': 65, 'dropout_p': 0.4748843483501442, 'learning_rate': 2.9979960530676105e-06, 'batch_size': 218, 'l2_reg': 1.6027807222253194e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 1.0009, eval loss 1.6235897541046143\n",
                  "Epoch 0 train loss: 1.6496, eval loss 1.8034542798995972\n",
                  "Epoch 1 train loss: 1.6724, eval loss 1.7963709831237793\n",
                  "Epoch 2 train loss: 1.5607, eval loss 1.7871358394622803\n",
                  "Epoch 3 train loss: 1.5602, eval loss 1.7805551290512085\n",
                  "Epoch 4 train loss: 1.6096, eval loss 1.7757809162139893\n",
                  "Epoch 5 train loss: 1.5294, eval loss 1.7659406661987305\n",
                  "Epoch 6 train loss: 1.4712, eval loss 1.759475827217102\n",
                  "Epoch 7 train loss: 1.5088, eval loss 1.7517280578613281\n",
                  "Epoch 8 train loss: 1.4498, eval loss 1.7465214729309082\n",
                  "Epoch 9 train loss: 1.4121, eval loss 1.7404727935791016\n",
                  "Epoch 10 train loss: 1.3451, eval loss 1.734407663345337\n",
                  "Epoch 11 train loss: 1.3423, eval loss 1.7313398122787476\n",
                  "Epoch 12 train loss: 1.3664, eval loss 1.7253390550613403\n",
                  "Epoch 13 train loss: 1.3494, eval loss 1.717074990272522\n",
                  "Epoch 14 train loss: 1.3497, eval loss 1.7133262157440186\n",
                  "Epoch 15 train loss: 1.3323, eval loss 1.7084074020385742\n",
                  "Epoch 16 train loss: 1.3513, eval loss 1.7037036418914795\n",
                  "Epoch 17 train loss: 1.3154, eval loss 1.6987557411193848\n",
                  "Epoch 18 train loss: 1.3093, eval loss 1.6953374147415161\n",
                  "Epoch 19 train loss: 1.2908, eval loss 1.6897698640823364\n",
                  "Epoch 20 train loss: 1.2719, eval loss 1.6855865716934204\n",
                  "Epoch 21 train loss: 1.2514, eval loss 1.6815623044967651\n",
                  "Epoch 22 train loss: 1.2022, eval loss 1.676567554473877\n",
                  "Epoch 23 train loss: 1.2714, eval loss 1.674761414527893\n",
                  "Epoch 24 train loss: 1.1836, eval loss 1.6686277389526367\n",
                  "Epoch 25 train loss: 1.1448, eval loss 1.6655293703079224\n",
                  "Epoch 26 train loss: 1.2113, eval loss 1.6615965366363525\n",
                  "Epoch 27 train loss: 1.1585, eval loss 1.6584794521331787\n",
                  "Epoch 28 train loss: 1.1496, eval loss 1.6541439294815063\n",
                  "Epoch 29 train loss: 1.1756, eval loss 1.6512633562088013\n",
                  "Epoch 30 train loss: 1.1071, eval loss 1.6481274366378784\n",
                  "Epoch 31 train loss: 1.0782, eval loss 1.6454874277114868\n",
                  "Epoch 32 train loss: 1.1517, eval loss 1.6442080736160278\n",
                  "Epoch 33 train loss: 1.0808, eval loss 1.6392790079116821\n",
                  "Epoch 34 train loss: 1.1980, eval loss 1.6359230279922485\n",
                  "Epoch 35 train loss: 1.1123, eval loss 1.6345115900039673\n",
                  "Epoch 36 train loss: 1.1180, eval loss 1.6302621364593506\n",
                  "Epoch 37 train loss: 1.0289, eval loss 1.6263699531555176\n",
                  "Epoch 38 train loss: 1.0187, eval loss 1.6245392560958862\n",
                  "Epoch 39 train loss: 1.0435, eval loss 1.6230089664459229\n",
                  "Epoch 40 train loss: 1.0489, eval loss 1.6187388896942139\n",
                  "Epoch 41 train loss: 0.9943, eval loss 1.6155991554260254\n",
                  "Epoch 42 train loss: 1.0363, eval loss 1.6135976314544678\n",
                  "Epoch 43 train loss: 1.0084, eval loss 1.6117208003997803\n",
                  "Epoch 44 train loss: 1.0021, eval loss 1.6082932949066162\n",
                  "Epoch 45 train loss: 1.0038, eval loss 1.6063791513442993\n",
                  "Epoch 46 train loss: 0.9544, eval loss 1.60380220413208\n",
                  "Epoch 47 train loss: 0.9923, eval loss 1.60105562210083\n",
                  "Epoch 48 train loss: 1.0232, eval loss 1.6003341674804688\n",
                  "Epoch 49 train loss: 0.9921, eval loss 1.595794439315796\n",
                  "Epoch 50 train loss: 0.9938, eval loss 1.5958399772644043\n",
                  "Epoch 51 train loss: 0.9800, eval loss 1.5914700031280518\n",
                  "Epoch 52 train loss: 0.9893, eval loss 1.5899049043655396\n",
                  "Epoch 53 train loss: 1.0089, eval loss 1.588496208190918\n",
                  "Epoch 54 train loss: 0.9727, eval loss 1.585402011871338\n",
                  "Epoch 55 train loss: 0.9953, eval loss 1.5835225582122803\n",
                  "Epoch 56 train loss: 0.9724, eval loss 1.5804996490478516\n",
                  "Epoch 57 train loss: 0.9418, eval loss 1.5799806118011475\n",
                  "Epoch 58 train loss: 0.9553, eval loss 1.5786678791046143\n",
                  "Epoch 59 train loss: 0.9413, eval loss 1.5749313831329346\n",
                  "Epoch 60 train loss: 0.9190, eval loss 1.5735867023468018\n",
                  "Epoch 61 train loss: 0.9604, eval loss 1.5700324773788452\n",
                  "Epoch 62 train loss: 0.9510, eval loss 1.5705138444900513\n",
                  "Epoch 63 train loss: 0.9352, eval loss 1.5685774087905884\n",
                  "Epoch 64 train loss: 0.9651, eval loss 1.565674066543579\n",
                  "Epoch 65 train loss: 0.9386, eval loss 1.5637879371643066\n",
                  "Epoch 66 train loss: 0.9006, eval loss 1.5623047351837158\n",
                  "Epoch 67 train loss: 0.9153, eval loss 1.5607095956802368\n",
                  "Epoch 68 train loss: 0.8739, eval loss 1.558519721031189\n",
                  "Epoch 69 train loss: 0.8824, eval loss 1.5570112466812134\n",
                  "Epoch 70 train loss: 0.9177, eval loss 1.5546101331710815\n",
                  "Epoch 71 train loss: 0.8269, eval loss 1.5529975891113281\n",
                  "Epoch 72 train loss: 0.8835, eval loss 1.5523186922073364\n",
                  "Epoch 73 train loss: 0.9095, eval loss 1.550399661064148\n",
                  "Epoch 74 train loss: 0.8410, eval loss 1.549817681312561\n",
                  "Epoch 75 train loss: 0.8117, eval loss 1.5481901168823242\n",
                  "Epoch 76 train loss: 0.8877, eval loss 1.5443452596664429\n",
                  "Epoch 77 train loss: 0.8138, eval loss 1.5432486534118652\n",
                  "Epoch 78 train loss: 0.7864, eval loss 1.541451334953308\n",
                  "Epoch 79 train loss: 0.8748, eval loss 1.539109706878662\n",
                  "Epoch 80 train loss: 0.8310, eval loss 1.5394736528396606\n",
                  "Epoch 81 train loss: 0.8942, eval loss 1.536368727684021\n",
                  "Epoch 82 train loss: 0.8509, eval loss 1.5361523628234863\n",
                  "Epoch 83 train loss: 0.8474, eval loss 1.5346862077713013\n",
                  "Epoch 84 train loss: 0.8334, eval loss 1.5332255363464355\n",
                  "Epoch 85 train loss: 0.8478, eval loss 1.5314387083053589\n",
                  "Epoch 86 train loss: 0.8336, eval loss 1.5309251546859741\n",
                  "Epoch 87 train loss: 0.8543, eval loss 1.529951810836792\n",
                  "Epoch 88 train loss: 0.8369, eval loss 1.5279960632324219\n",
                  "Epoch 89 train loss: 0.8407, eval loss 1.527328372001648\n",
                  "Epoch 90 train loss: 0.7975, eval loss 1.5244296789169312\n",
                  "Epoch 91 train loss: 0.7930, eval loss 1.5241457223892212\n",
                  "Epoch 92 train loss: 0.8569, eval loss 1.5228502750396729\n",
                  "Epoch 93 train loss: 0.8611, eval loss 1.5203063488006592\n",
                  "Epoch 94 train loss: 0.8270, eval loss 1.5190072059631348\n",
                  "Epoch 95 train loss: 0.8588, eval loss 1.5187792778015137\n",
                  "Epoch 96 train loss: 0.8251, eval loss 1.5175349712371826\n",
                  "Epoch 97 train loss: 0.8046, eval loss 1.515991449356079\n",
                  "Epoch 98 train loss: 0.8007, eval loss 1.5141985416412354\n",
                  "Epoch 99 train loss: 0.7711, eval loss 1.5130815505981445\n",
                  "Epoch 100 train loss: 0.8046, eval loss 1.5121841430664062\n",
                  "Epoch 101 train loss: 0.8043, eval loss 1.512342095375061\n",
                  "Epoch 102 train loss: 0.8008, eval loss 1.5096650123596191\n",
                  "Epoch 103 train loss: 0.8185, eval loss 1.5092988014221191\n",
                  "Epoch 104 train loss: 0.7879, eval loss 1.5075502395629883\n",
                  "Epoch 105 train loss: 0.8025, eval loss 1.5070061683654785\n",
                  "Epoch 106 train loss: 0.7548, eval loss 1.505195140838623\n",
                  "Epoch 107 train loss: 0.7762, eval loss 1.5050640106201172\n",
                  "Epoch 108 train loss: 0.7786, eval loss 1.503246545791626\n",
                  "Epoch 109 train loss: 0.7832, eval loss 1.5017298460006714\n",
                  "Epoch 110 train loss: 0.7981, eval loss 1.50144362449646\n",
                  "Epoch 111 train loss: 0.7717, eval loss 1.5007941722869873\n",
                  "Epoch 112 train loss: 0.8061, eval loss 1.4994536638259888\n",
                  "Epoch 113 train loss: 0.7951, eval loss 1.498060703277588\n",
                  "Epoch 114 train loss: 0.7183, eval loss 1.4966471195220947\n",
                  "Epoch 115 train loss: 0.7770, eval loss 1.4961724281311035\n",
                  "Epoch 116 train loss: 0.8039, eval loss 1.4954696893692017\n",
                  "Epoch 117 train loss: 0.7756, eval loss 1.4945805072784424\n",
                  "Epoch 118 train loss: 0.8058, eval loss 1.4934417009353638\n",
                  "Epoch 119 train loss: 0.7827, eval loss 1.4931905269622803\n",
                  "Epoch 120 train loss: 0.7621, eval loss 1.492629051208496\n",
                  "Epoch 121 train loss: 0.7552, eval loss 1.4907400608062744\n",
                  "Epoch 122 train loss: 0.7848, eval loss 1.489794135093689\n",
                  "Epoch 123 train loss: 0.7199, eval loss 1.48883056640625\n",
                  "Epoch 124 train loss: 0.7649, eval loss 1.4881139993667603\n",
                  "Epoch 125 train loss: 0.7286, eval loss 1.487944483757019\n",
                  "Epoch 126 train loss: 0.7660, eval loss 1.4861446619033813\n",
                  "Epoch 127 train loss: 0.7416, eval loss 1.4869012832641602\n",
                  "Epoch 128 train loss: 0.6946, eval loss 1.4846513271331787\n",
                  "Epoch 129 train loss: 0.7334, eval loss 1.4847633838653564\n",
                  "Epoch 130 train loss: 0.7670, eval loss 1.484062671661377\n",
                  "Epoch 131 train loss: 0.7638, eval loss 1.4830361604690552\n",
                  "Epoch 132 train loss: 0.7822, eval loss 1.4829961061477661\n",
                  "Epoch 133 train loss: 0.7702, eval loss 1.4822185039520264\n",
                  "Epoch 134 train loss: 0.7958, eval loss 1.4807275533676147\n",
                  "Epoch 135 train loss: 0.7084, eval loss 1.4794721603393555\n",
                  "Epoch 136 train loss: 0.7542, eval loss 1.47948157787323\n",
                  "Epoch 137 train loss: 0.7608, eval loss 1.4795629978179932\n",
                  "Epoch 138 train loss: 0.7232, eval loss 1.4786512851715088\n",
                  "Epoch 139 train loss: 0.7572, eval loss 1.4783658981323242\n",
                  "Epoch 140 train loss: 0.6955, eval loss 1.477036952972412\n",
                  "Epoch 141 train loss: 0.7326, eval loss 1.476219892501831\n",
                  "Epoch 142 train loss: 0.6840, eval loss 1.4766144752502441\n",
                  "Epoch 143 train loss: 0.7260, eval loss 1.475031852722168\n",
                  "Epoch 144 train loss: 0.7443, eval loss 1.4738037586212158\n",
                  "Epoch 145 train loss: 0.7355, eval loss 1.4735723733901978\n",
                  "Epoch 146 train loss: 0.7462, eval loss 1.4721843004226685\n",
                  "Epoch 147 train loss: 0.7106, eval loss 1.472124695777893\n",
                  "Epoch 148 train loss: 0.7503, eval loss 1.4710332155227661\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:48:17,888] Trial 24 finished with value: 1.4707778692245483 and parameters: {'hidden_layers_size': 96, 'dropout_p': 0.4317841855591858, 'learning_rate': 6.154684650866806e-06, 'batch_size': 151, 'l2_reg': 3.686034660496669e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.7528, eval loss 1.4707778692245483\n",
                  "Epoch 0 train loss: 1.2679, eval loss 1.770382285118103\n",
                  "Epoch 1 train loss: 1.2088, eval loss 1.7655798196792603\n",
                  "Epoch 2 train loss: 1.3492, eval loss 1.7646434307098389\n",
                  "Epoch 3 train loss: 1.3397, eval loss 1.7632484436035156\n",
                  "Epoch 4 train loss: 1.3373, eval loss 1.7616194486618042\n",
                  "Epoch 5 train loss: 1.1794, eval loss 1.7616240978240967\n",
                  "Epoch 6 train loss: 1.4206, eval loss 1.7612793445587158\n",
                  "Epoch 7 train loss: 1.4506, eval loss 1.7612364292144775\n",
                  "Epoch 8 train loss: 1.4705, eval loss 1.754624366760254\n",
                  "Epoch 9 train loss: 1.1209, eval loss 1.756594181060791\n",
                  "Epoch 10 train loss: 1.5553, eval loss 1.7512987852096558\n",
                  "Epoch 11 train loss: 1.3383, eval loss 1.7509173154830933\n",
                  "Epoch 12 train loss: 1.1843, eval loss 1.7521178722381592\n",
                  "Epoch 13 train loss: 1.1302, eval loss 1.7495754957199097\n",
                  "Epoch 14 train loss: 1.2286, eval loss 1.7470674514770508\n",
                  "Epoch 15 train loss: 1.0775, eval loss 1.7458668947219849\n",
                  "Epoch 16 train loss: 1.3368, eval loss 1.7423315048217773\n",
                  "Epoch 17 train loss: 1.2931, eval loss 1.7440698146820068\n",
                  "Epoch 18 train loss: 1.2134, eval loss 1.7426668405532837\n",
                  "Epoch 19 train loss: 0.9862, eval loss 1.7421095371246338\n",
                  "Epoch 20 train loss: 1.2770, eval loss 1.7387545108795166\n",
                  "Epoch 21 train loss: 1.1398, eval loss 1.7389672994613647\n",
                  "Epoch 22 train loss: 1.0871, eval loss 1.739350438117981\n",
                  "Epoch 23 train loss: 1.4149, eval loss 1.734492540359497\n",
                  "Epoch 24 train loss: 1.5911, eval loss 1.7359423637390137\n",
                  "Epoch 25 train loss: 1.1840, eval loss 1.7326701879501343\n",
                  "Epoch 26 train loss: 1.1379, eval loss 1.7299749851226807\n",
                  "Epoch 27 train loss: 1.0629, eval loss 1.7285325527191162\n",
                  "Epoch 28 train loss: 1.2418, eval loss 1.7271754741668701\n",
                  "Epoch 29 train loss: 1.1328, eval loss 1.7291523218154907\n",
                  "Epoch 30 train loss: 1.1152, eval loss 1.724206805229187\n",
                  "Epoch 31 train loss: 1.0207, eval loss 1.7231451272964478\n",
                  "Epoch 32 train loss: 1.1585, eval loss 1.7254709005355835\n",
                  "Epoch 33 train loss: 1.1226, eval loss 1.7234179973602295\n",
                  "Epoch 34 train loss: 1.1307, eval loss 1.7223273515701294\n",
                  "Epoch 35 train loss: 1.2857, eval loss 1.7196789979934692\n",
                  "Epoch 36 train loss: 1.0500, eval loss 1.71662437915802\n",
                  "Epoch 37 train loss: 1.0403, eval loss 1.7186734676361084\n",
                  "Epoch 38 train loss: 1.1527, eval loss 1.7144511938095093\n",
                  "Epoch 39 train loss: 1.2129, eval loss 1.7154626846313477\n",
                  "Epoch 40 train loss: 1.1280, eval loss 1.7125434875488281\n",
                  "Epoch 41 train loss: 1.0968, eval loss 1.7099602222442627\n",
                  "Epoch 42 train loss: 1.0520, eval loss 1.7124450206756592\n",
                  "Epoch 43 train loss: 1.0269, eval loss 1.7073609828948975\n",
                  "Epoch 44 train loss: 1.2377, eval loss 1.7061924934387207\n",
                  "Epoch 45 train loss: 1.1683, eval loss 1.7093734741210938\n",
                  "Epoch 46 train loss: 1.0499, eval loss 1.7055984735488892\n",
                  "Epoch 47 train loss: 1.0453, eval loss 1.7035999298095703\n",
                  "Epoch 48 train loss: 1.3200, eval loss 1.706383466720581\n",
                  "Epoch 49 train loss: 1.1536, eval loss 1.6982091665267944\n",
                  "Epoch 50 train loss: 1.2094, eval loss 1.703837513923645\n",
                  "Epoch 51 train loss: 0.9567, eval loss 1.6981000900268555\n",
                  "Epoch 52 train loss: 1.0988, eval loss 1.7017028331756592\n",
                  "Epoch 53 train loss: 1.7033, eval loss 1.6993179321289062\n",
                  "Epoch 54 train loss: 1.0674, eval loss 1.6976091861724854\n",
                  "Epoch 55 train loss: 1.0435, eval loss 1.6979310512542725\n",
                  "Epoch 56 train loss: 1.1161, eval loss 1.6964517831802368\n",
                  "Epoch 57 train loss: 1.0529, eval loss 1.6934183835983276\n",
                  "Epoch 58 train loss: 1.0443, eval loss 1.693878173828125\n",
                  "Epoch 59 train loss: 1.1267, eval loss 1.6909087896347046\n",
                  "Epoch 60 train loss: 0.9813, eval loss 1.691400408744812\n",
                  "Epoch 61 train loss: 1.1217, eval loss 1.6875416040420532\n",
                  "Epoch 62 train loss: 0.8008, eval loss 1.6873582601547241\n",
                  "Epoch 63 train loss: 1.0999, eval loss 1.6900724172592163\n",
                  "Epoch 64 train loss: 0.9853, eval loss 1.688281774520874\n",
                  "Epoch 65 train loss: 0.9149, eval loss 1.683087706565857\n",
                  "Epoch 66 train loss: 1.0498, eval loss 1.685681700706482\n",
                  "Epoch 67 train loss: 1.3238, eval loss 1.6844269037246704\n",
                  "Epoch 68 train loss: 0.9091, eval loss 1.6801937818527222\n",
                  "Epoch 69 train loss: 1.1808, eval loss 1.679755687713623\n",
                  "Epoch 70 train loss: 0.9711, eval loss 1.6805928945541382\n",
                  "Epoch 71 train loss: 1.0412, eval loss 1.678232192993164\n",
                  "Epoch 72 train loss: 1.0664, eval loss 1.6782078742980957\n",
                  "Epoch 73 train loss: 1.0166, eval loss 1.6770384311676025\n",
                  "Epoch 74 train loss: 0.8901, eval loss 1.6761045455932617\n",
                  "Epoch 75 train loss: 0.9562, eval loss 1.6769981384277344\n",
                  "Epoch 76 train loss: 1.1029, eval loss 1.6739836931228638\n",
                  "Epoch 77 train loss: 0.9130, eval loss 1.6719071865081787\n",
                  "Epoch 78 train loss: 0.9908, eval loss 1.6721700429916382\n",
                  "Epoch 79 train loss: 0.7605, eval loss 1.6708885431289673\n",
                  "Epoch 80 train loss: 1.1390, eval loss 1.6715192794799805\n",
                  "Epoch 81 train loss: 0.8574, eval loss 1.6678253412246704\n",
                  "Epoch 82 train loss: 0.9154, eval loss 1.6673029661178589\n",
                  "Epoch 83 train loss: 0.8798, eval loss 1.669440746307373\n",
                  "Epoch 84 train loss: 0.9329, eval loss 1.6678125858306885\n",
                  "Epoch 85 train loss: 1.0992, eval loss 1.6705539226531982\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:49:02,380] Trial 25 finished with value: 1.6673029661178589 and parameters: {'hidden_layers_size': 79, 'dropout_p': 0.47541602771403274, 'learning_rate': 1.638060456814591e-06, 'batch_size': 178, 'l2_reg': 1.1879550697796984e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4276, eval loss 1.760650873184204\n",
                  "Epoch 1 train loss: 1.4243, eval loss 1.7473496198654175\n",
                  "Epoch 2 train loss: 1.4075, eval loss 1.7382439374923706\n",
                  "Epoch 3 train loss: 1.3131, eval loss 1.7317560911178589\n",
                  "Epoch 4 train loss: 1.3350, eval loss 1.7215149402618408\n",
                  "Epoch 5 train loss: 1.2760, eval loss 1.711403489112854\n",
                  "Epoch 6 train loss: 1.3024, eval loss 1.703236699104309\n",
                  "Epoch 7 train loss: 1.2752, eval loss 1.69441819190979\n",
                  "Epoch 8 train loss: 1.2684, eval loss 1.6880757808685303\n",
                  "Epoch 9 train loss: 1.2750, eval loss 1.6802438497543335\n",
                  "Epoch 10 train loss: 1.1779, eval loss 1.6739473342895508\n",
                  "Epoch 11 train loss: 1.1015, eval loss 1.6680017709732056\n",
                  "Epoch 12 train loss: 1.1309, eval loss 1.6602672338485718\n",
                  "Epoch 13 train loss: 1.1185, eval loss 1.653985857963562\n",
                  "Epoch 14 train loss: 1.0809, eval loss 1.6494412422180176\n",
                  "Epoch 15 train loss: 1.0625, eval loss 1.6443842649459839\n",
                  "Epoch 16 train loss: 1.0799, eval loss 1.6403449773788452\n",
                  "Epoch 17 train loss: 1.0327, eval loss 1.6337041854858398\n",
                  "Epoch 18 train loss: 1.0769, eval loss 1.6295416355133057\n",
                  "Epoch 19 train loss: 1.0617, eval loss 1.624611258506775\n",
                  "Epoch 20 train loss: 1.0872, eval loss 1.6210397481918335\n",
                  "Epoch 21 train loss: 1.0784, eval loss 1.616349458694458\n",
                  "Epoch 22 train loss: 1.0127, eval loss 1.6119332313537598\n",
                  "Epoch 23 train loss: 1.0216, eval loss 1.609316110610962\n",
                  "Epoch 24 train loss: 1.0774, eval loss 1.6050305366516113\n",
                  "Epoch 25 train loss: 0.9963, eval loss 1.599855661392212\n",
                  "Epoch 26 train loss: 1.0786, eval loss 1.5972609519958496\n",
                  "Epoch 27 train loss: 0.9632, eval loss 1.5945966243743896\n",
                  "Epoch 28 train loss: 0.9841, eval loss 1.5895496606826782\n",
                  "Epoch 29 train loss: 0.9645, eval loss 1.5873748064041138\n",
                  "Epoch 30 train loss: 1.0080, eval loss 1.5845110416412354\n",
                  "Epoch 31 train loss: 0.9625, eval loss 1.5800702571868896\n",
                  "Epoch 32 train loss: 0.9140, eval loss 1.5776164531707764\n",
                  "Epoch 33 train loss: 0.9204, eval loss 1.573986530303955\n",
                  "Epoch 34 train loss: 0.9128, eval loss 1.5721116065979004\n",
                  "Epoch 35 train loss: 0.9456, eval loss 1.5689743757247925\n",
                  "Epoch 36 train loss: 0.9044, eval loss 1.5658036470413208\n",
                  "Epoch 37 train loss: 0.9307, eval loss 1.564959168434143\n",
                  "Epoch 38 train loss: 0.9439, eval loss 1.5606940984725952\n",
                  "Epoch 39 train loss: 0.9128, eval loss 1.5578325986862183\n",
                  "Epoch 40 train loss: 0.9151, eval loss 1.556566596031189\n",
                  "Epoch 41 train loss: 0.9255, eval loss 1.5544508695602417\n",
                  "Epoch 42 train loss: 0.9105, eval loss 1.5523574352264404\n",
                  "Epoch 43 train loss: 0.9015, eval loss 1.5496559143066406\n",
                  "Epoch 44 train loss: 0.8918, eval loss 1.5481672286987305\n",
                  "Epoch 45 train loss: 0.8751, eval loss 1.5453779697418213\n",
                  "Epoch 46 train loss: 0.8800, eval loss 1.5423974990844727\n",
                  "Epoch 47 train loss: 0.8795, eval loss 1.5398794412612915\n",
                  "Epoch 48 train loss: 0.9073, eval loss 1.538259744644165\n",
                  "Epoch 49 train loss: 0.8994, eval loss 1.5365524291992188\n",
                  "Epoch 50 train loss: 0.8630, eval loss 1.533186674118042\n",
                  "Epoch 51 train loss: 0.8580, eval loss 1.5319180488586426\n",
                  "Epoch 52 train loss: 0.9626, eval loss 1.5308623313903809\n",
                  "Epoch 53 train loss: 0.8573, eval loss 1.5291664600372314\n",
                  "Epoch 54 train loss: 0.8906, eval loss 1.5255155563354492\n",
                  "Epoch 55 train loss: 0.8112, eval loss 1.525126576423645\n",
                  "Epoch 56 train loss: 0.8613, eval loss 1.5218654870986938\n",
                  "Epoch 57 train loss: 0.7885, eval loss 1.5210117101669312\n",
                  "Epoch 58 train loss: 0.8098, eval loss 1.5194566249847412\n",
                  "Epoch 59 train loss: 0.8743, eval loss 1.5174835920333862\n",
                  "Epoch 60 train loss: 0.8572, eval loss 1.5164763927459717\n",
                  "Epoch 61 train loss: 0.8133, eval loss 1.513860821723938\n",
                  "Epoch 62 train loss: 0.8349, eval loss 1.512378215789795\n",
                  "Epoch 63 train loss: 0.8212, eval loss 1.5115125179290771\n",
                  "Epoch 64 train loss: 0.8924, eval loss 1.5092010498046875\n",
                  "Epoch 65 train loss: 0.7594, eval loss 1.507811427116394\n",
                  "Epoch 66 train loss: 0.8583, eval loss 1.5067458152770996\n",
                  "Epoch 67 train loss: 0.8451, eval loss 1.504640817642212\n",
                  "Epoch 68 train loss: 0.7743, eval loss 1.5024104118347168\n",
                  "Epoch 69 train loss: 0.8280, eval loss 1.501865267753601\n",
                  "Epoch 70 train loss: 0.8009, eval loss 1.5019536018371582\n",
                  "Epoch 71 train loss: 0.7806, eval loss 1.499906301498413\n",
                  "Epoch 72 train loss: 0.8117, eval loss 1.4979299306869507\n",
                  "Epoch 73 train loss: 0.8337, eval loss 1.497624158859253\n",
                  "Epoch 74 train loss: 0.7572, eval loss 1.4954668283462524\n",
                  "Epoch 75 train loss: 0.8310, eval loss 1.4950309991836548\n",
                  "Epoch 76 train loss: 0.7768, eval loss 1.4935851097106934\n",
                  "Epoch 77 train loss: 0.8196, eval loss 1.4915530681610107\n",
                  "Epoch 78 train loss: 0.8131, eval loss 1.4899450540542603\n",
                  "Epoch 79 train loss: 0.8223, eval loss 1.4894345998764038\n",
                  "Epoch 80 train loss: 0.8116, eval loss 1.4883416891098022\n",
                  "Epoch 81 train loss: 0.8424, eval loss 1.4874294996261597\n",
                  "Epoch 82 train loss: 0.8480, eval loss 1.4874602556228638\n",
                  "Epoch 83 train loss: 0.8075, eval loss 1.4867440462112427\n",
                  "Epoch 84 train loss: 0.7502, eval loss 1.4845556020736694\n",
                  "Epoch 85 train loss: 0.7940, eval loss 1.483200192451477\n",
                  "Epoch 86 train loss: 0.7639, eval loss 1.48388671875\n",
                  "Epoch 87 train loss: 0.8217, eval loss 1.4814565181732178\n",
                  "Epoch 88 train loss: 0.7009, eval loss 1.480507493019104\n",
                  "Epoch 89 train loss: 0.8209, eval loss 1.479318380355835\n",
                  "Epoch 90 train loss: 0.6732, eval loss 1.4777313470840454\n",
                  "Epoch 91 train loss: 0.7583, eval loss 1.4778268337249756\n",
                  "Epoch 92 train loss: 0.8192, eval loss 1.4775596857070923\n",
                  "Epoch 93 train loss: 0.8224, eval loss 1.4763755798339844\n",
                  "Epoch 94 train loss: 0.8078, eval loss 1.4750958681106567\n",
                  "Epoch 95 train loss: 0.7671, eval loss 1.474909782409668\n",
                  "Epoch 96 train loss: 0.7166, eval loss 1.4740639925003052\n",
                  "Epoch 97 train loss: 0.7820, eval loss 1.4729282855987549\n",
                  "Epoch 98 train loss: 0.7783, eval loss 1.4722797870635986\n",
                  "Epoch 99 train loss: 0.7420, eval loss 1.4715913534164429\n",
                  "Epoch 100 train loss: 0.7429, eval loss 1.4709047079086304\n",
                  "Epoch 101 train loss: 0.7298, eval loss 1.4707202911376953\n",
                  "Epoch 102 train loss: 0.7774, eval loss 1.4690487384796143\n",
                  "Epoch 103 train loss: 0.7205, eval loss 1.4688200950622559\n",
                  "Epoch 104 train loss: 0.7535, eval loss 1.4675041437149048\n",
                  "Epoch 105 train loss: 0.7385, eval loss 1.4682693481445312\n",
                  "Epoch 106 train loss: 0.7233, eval loss 1.4663479328155518\n",
                  "Epoch 107 train loss: 0.7047, eval loss 1.4658589363098145\n",
                  "Epoch 108 train loss: 0.6964, eval loss 1.4655998945236206\n",
                  "Epoch 109 train loss: 0.7396, eval loss 1.4649102687835693\n",
                  "Epoch 110 train loss: 0.7700, eval loss 1.464913010597229\n",
                  "Epoch 111 train loss: 0.7587, eval loss 1.4644416570663452\n",
                  "Epoch 112 train loss: 0.8030, eval loss 1.4640077352523804\n",
                  "Epoch 113 train loss: 0.7320, eval loss 1.4623990058898926\n",
                  "Epoch 114 train loss: 0.7776, eval loss 1.4618926048278809\n",
                  "Epoch 115 train loss: 0.7581, eval loss 1.4620592594146729\n",
                  "Epoch 116 train loss: 0.7337, eval loss 1.4618126153945923\n",
                  "Epoch 117 train loss: 0.7974, eval loss 1.4609804153442383\n",
                  "Epoch 118 train loss: 0.7718, eval loss 1.4605525732040405\n",
                  "Epoch 119 train loss: 0.7774, eval loss 1.4597796201705933\n",
                  "Epoch 120 train loss: 0.7634, eval loss 1.4599337577819824\n",
                  "Epoch 121 train loss: 0.8023, eval loss 1.458981990814209\n",
                  "Epoch 122 train loss: 0.7691, eval loss 1.458969235420227\n",
                  "Epoch 123 train loss: 0.7929, eval loss 1.458040475845337\n",
                  "Epoch 124 train loss: 0.7418, eval loss 1.4580806493759155\n",
                  "Epoch 125 train loss: 0.7363, eval loss 1.4584780931472778\n",
                  "Epoch 126 train loss: 0.8135, eval loss 1.457403302192688\n",
                  "Epoch 127 train loss: 0.7631, eval loss 1.4575867652893066\n",
                  "Epoch 128 train loss: 0.7879, eval loss 1.4568244218826294\n",
                  "Epoch 129 train loss: 0.8106, eval loss 1.4560903310775757\n",
                  "Epoch 130 train loss: 0.7520, eval loss 1.4564034938812256\n",
                  "Epoch 131 train loss: 0.7963, eval loss 1.4553470611572266\n",
                  "Epoch 132 train loss: 0.7270, eval loss 1.455986738204956\n",
                  "Epoch 133 train loss: 0.7719, eval loss 1.4545923471450806\n",
                  "Epoch 134 train loss: 0.7833, eval loss 1.454858660697937\n",
                  "Epoch 135 train loss: 0.7916, eval loss 1.4540908336639404\n",
                  "Epoch 136 train loss: 0.7396, eval loss 1.4543683528900146\n",
                  "Epoch 137 train loss: 0.8346, eval loss 1.4542196989059448\n",
                  "Epoch 138 train loss: 0.7001, eval loss 1.4527662992477417\n",
                  "Epoch 139 train loss: 0.7641, eval loss 1.4535391330718994\n",
                  "Epoch 140 train loss: 0.6907, eval loss 1.4530161619186401\n",
                  "Epoch 141 train loss: 0.7354, eval loss 1.4521688222885132\n",
                  "Epoch 142 train loss: 0.7159, eval loss 1.4522861242294312\n",
                  "Epoch 143 train loss: 0.7557, eval loss 1.452348232269287\n",
                  "Epoch 144 train loss: 0.7192, eval loss 1.4519410133361816\n",
                  "Epoch 145 train loss: 0.7089, eval loss 1.4521061182022095\n",
                  "Epoch 146 train loss: 0.7198, eval loss 1.4519469738006592\n",
                  "Epoch 147 train loss: 0.7803, eval loss 1.4517778158187866\n",
                  "Epoch 148 train loss: 0.7003, eval loss 1.4510561227798462\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:50:13,017] Trial 26 finished with value: 1.4506356716156006 and parameters: {'hidden_layers_size': 70, 'dropout_p': 0.42804938275340926, 'learning_rate': 1.3969789722062875e-05, 'batch_size': 205, 'l2_reg': 2.5361931652833666e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.8825, eval loss 1.4506356716156006\n",
                  "Epoch 0 train loss: 1.4238, eval loss 1.7925430536270142\n",
                  "Epoch 1 train loss: 1.6231, eval loss 1.7941477298736572\n",
                  "Epoch 2 train loss: 1.5431, eval loss 1.7907640933990479\n",
                  "Epoch 3 train loss: 1.5265, eval loss 1.7889207601547241\n",
                  "Epoch 4 train loss: 1.5184, eval loss 1.7869890928268433\n",
                  "Epoch 5 train loss: 1.3531, eval loss 1.787232756614685\n",
                  "Epoch 6 train loss: 1.2907, eval loss 1.7830820083618164\n",
                  "Epoch 7 train loss: 1.5182, eval loss 1.7821002006530762\n",
                  "Epoch 8 train loss: 1.7434, eval loss 1.7828713655471802\n",
                  "Epoch 9 train loss: 1.3894, eval loss 1.7801830768585205\n",
                  "Epoch 10 train loss: 1.4357, eval loss 1.7767705917358398\n",
                  "Epoch 11 train loss: 1.3385, eval loss 1.7765315771102905\n",
                  "Epoch 12 train loss: 1.4539, eval loss 1.7717212438583374\n",
                  "Epoch 13 train loss: 1.4663, eval loss 1.7755857706069946\n",
                  "Epoch 14 train loss: 1.5224, eval loss 1.7728086709976196\n",
                  "Epoch 15 train loss: 1.5189, eval loss 1.7702667713165283\n",
                  "Epoch 16 train loss: 1.3087, eval loss 1.7658336162567139\n",
                  "Epoch 17 train loss: 1.7367, eval loss 1.767400860786438\n",
                  "Epoch 18 train loss: 1.3278, eval loss 1.7651370763778687\n",
                  "Epoch 19 train loss: 1.2595, eval loss 1.7632144689559937\n",
                  "Epoch 20 train loss: 1.2052, eval loss 1.7627888917922974\n",
                  "Epoch 21 train loss: 1.3822, eval loss 1.760340929031372\n",
                  "Epoch 22 train loss: 1.3325, eval loss 1.7556962966918945\n",
                  "Epoch 23 train loss: 1.3659, eval loss 1.7567975521087646\n",
                  "Epoch 24 train loss: 1.5168, eval loss 1.7556074857711792\n",
                  "Epoch 25 train loss: 1.4017, eval loss 1.7552751302719116\n",
                  "Epoch 26 train loss: 1.3453, eval loss 1.7476656436920166\n",
                  "Epoch 27 train loss: 1.3693, eval loss 1.7510406970977783\n",
                  "Epoch 28 train loss: 1.3084, eval loss 1.7506440877914429\n",
                  "Epoch 29 train loss: 1.3479, eval loss 1.747901201248169\n",
                  "Epoch 30 train loss: 1.2098, eval loss 1.746457576751709\n",
                  "Epoch 31 train loss: 1.2526, eval loss 1.7445436716079712\n",
                  "Epoch 32 train loss: 1.2949, eval loss 1.7418891191482544\n",
                  "Epoch 33 train loss: 1.4211, eval loss 1.742099642753601\n",
                  "Epoch 34 train loss: 1.3385, eval loss 1.7368946075439453\n",
                  "Epoch 35 train loss: 1.3955, eval loss 1.739033579826355\n",
                  "Epoch 36 train loss: 1.5235, eval loss 1.7394484281539917\n",
                  "Epoch 37 train loss: 1.6306, eval loss 1.7371959686279297\n",
                  "Epoch 38 train loss: 1.0037, eval loss 1.7335045337677002\n",
                  "Epoch 39 train loss: 1.2155, eval loss 1.7324401140213013\n",
                  "Epoch 40 train loss: 1.2647, eval loss 1.7308237552642822\n",
                  "Epoch 41 train loss: 1.2536, eval loss 1.7293285131454468\n",
                  "Epoch 42 train loss: 1.1234, eval loss 1.725339412689209\n",
                  "Epoch 43 train loss: 1.0754, eval loss 1.7296597957611084\n",
                  "Epoch 44 train loss: 1.0532, eval loss 1.7256782054901123\n",
                  "Epoch 45 train loss: 1.1352, eval loss 1.7237306833267212\n",
                  "Epoch 46 train loss: 1.0726, eval loss 1.723077416419983\n",
                  "Epoch 47 train loss: 1.0636, eval loss 1.7216877937316895\n",
                  "Epoch 48 train loss: 1.1499, eval loss 1.7190266847610474\n",
                  "Epoch 49 train loss: 1.0550, eval loss 1.7214854955673218\n",
                  "Epoch 50 train loss: 1.0914, eval loss 1.7180616855621338\n",
                  "Epoch 51 train loss: 1.2194, eval loss 1.715587854385376\n",
                  "Epoch 52 train loss: 1.0116, eval loss 1.713850498199463\n",
                  "Epoch 53 train loss: 1.3455, eval loss 1.7144118547439575\n",
                  "Epoch 54 train loss: 1.0371, eval loss 1.712223768234253\n",
                  "Epoch 55 train loss: 1.0797, eval loss 1.7098263502120972\n",
                  "Epoch 56 train loss: 1.3096, eval loss 1.7128369808197021\n",
                  "Epoch 57 train loss: 1.1691, eval loss 1.7116947174072266\n",
                  "Epoch 58 train loss: 1.2424, eval loss 1.7094162702560425\n",
                  "Epoch 59 train loss: 0.9709, eval loss 1.7075166702270508\n",
                  "Epoch 60 train loss: 1.2234, eval loss 1.7094801664352417\n",
                  "Epoch 61 train loss: 0.8534, eval loss 1.705705165863037\n",
                  "Epoch 62 train loss: 0.9874, eval loss 1.7008819580078125\n",
                  "Epoch 63 train loss: 1.0143, eval loss 1.702789068222046\n",
                  "Epoch 64 train loss: 0.9594, eval loss 1.7010200023651123\n",
                  "Epoch 65 train loss: 1.2975, eval loss 1.6988290548324585\n",
                  "Epoch 66 train loss: 1.1602, eval loss 1.6995986700057983\n",
                  "Epoch 67 train loss: 1.0784, eval loss 1.6947722434997559\n",
                  "Epoch 68 train loss: 1.1572, eval loss 1.696531057357788\n",
                  "Epoch 69 train loss: 1.0795, eval loss 1.6976978778839111\n",
                  "Epoch 70 train loss: 1.0304, eval loss 1.6936553716659546\n",
                  "Epoch 71 train loss: 0.9637, eval loss 1.691908597946167\n",
                  "Epoch 72 train loss: 1.3529, eval loss 1.6940281391143799\n",
                  "Epoch 73 train loss: 1.0379, eval loss 1.6879061460494995\n",
                  "Epoch 74 train loss: 1.0423, eval loss 1.6877057552337646\n",
                  "Epoch 75 train loss: 1.0047, eval loss 1.686481237411499\n",
                  "Epoch 76 train loss: 1.2064, eval loss 1.684445858001709\n",
                  "Epoch 77 train loss: 1.0681, eval loss 1.684767484664917\n",
                  "Epoch 78 train loss: 1.0374, eval loss 1.6853569746017456\n",
                  "Epoch 79 train loss: 1.0572, eval loss 1.6820309162139893\n",
                  "Epoch 80 train loss: 1.0072, eval loss 1.6822019815444946\n",
                  "Epoch 81 train loss: 0.9545, eval loss 1.681026816368103\n",
                  "Epoch 82 train loss: 0.8899, eval loss 1.6807941198349\n",
                  "Epoch 83 train loss: 1.0378, eval loss 1.680550456047058\n",
                  "Epoch 84 train loss: 0.9987, eval loss 1.6769198179244995\n",
                  "Epoch 85 train loss: 0.9613, eval loss 1.675295352935791\n",
                  "Epoch 86 train loss: 0.9952, eval loss 1.675821304321289\n",
                  "Epoch 87 train loss: 0.9709, eval loss 1.6764569282531738\n",
                  "Epoch 88 train loss: 1.1050, eval loss 1.6726864576339722\n",
                  "Epoch 89 train loss: 0.8423, eval loss 1.6694382429122925\n",
                  "Epoch 90 train loss: 0.9571, eval loss 1.6704336404800415\n",
                  "Epoch 91 train loss: 0.9852, eval loss 1.6704747676849365\n",
                  "Epoch 92 train loss: 1.1214, eval loss 1.6687320470809937\n",
                  "Epoch 93 train loss: 0.8842, eval loss 1.6656135320663452\n",
                  "Epoch 94 train loss: 1.0055, eval loss 1.6700767278671265\n",
                  "Epoch 95 train loss: 0.8262, eval loss 1.6628854274749756\n",
                  "Epoch 96 train loss: 0.8806, eval loss 1.6665871143341064\n",
                  "Epoch 97 train loss: 1.1685, eval loss 1.6636501550674438\n",
                  "Epoch 98 train loss: 0.8681, eval loss 1.661995768547058\n",
                  "Epoch 99 train loss: 0.9376, eval loss 1.66042959690094\n",
                  "Epoch 100 train loss: 1.0714, eval loss 1.6595720052719116\n",
                  "Epoch 101 train loss: 0.9268, eval loss 1.664055347442627\n",
                  "Epoch 102 train loss: 1.2701, eval loss 1.6570336818695068\n",
                  "Epoch 103 train loss: 0.8087, eval loss 1.6559841632843018\n",
                  "Epoch 104 train loss: 1.0890, eval loss 1.653850793838501\n",
                  "Epoch 105 train loss: 0.9001, eval loss 1.6585030555725098\n",
                  "Epoch 106 train loss: 0.9351, eval loss 1.6568119525909424\n",
                  "Epoch 107 train loss: 0.9202, eval loss 1.6509578227996826\n",
                  "Epoch 108 train loss: 0.8804, eval loss 1.6497172117233276\n",
                  "Epoch 109 train loss: 0.9663, eval loss 1.6500909328460693\n",
                  "Epoch 110 train loss: 1.0451, eval loss 1.6490586996078491\n",
                  "Epoch 111 train loss: 1.1574, eval loss 1.6485852003097534\n",
                  "Epoch 112 train loss: 0.9106, eval loss 1.6507855653762817\n",
                  "Epoch 113 train loss: 0.8148, eval loss 1.647566318511963\n",
                  "Epoch 114 train loss: 1.0575, eval loss 1.650091290473938\n",
                  "Epoch 115 train loss: 0.8042, eval loss 1.647019863128662\n",
                  "Epoch 116 train loss: 0.8228, eval loss 1.647443175315857\n",
                  "Epoch 117 train loss: 0.8086, eval loss 1.642776608467102\n",
                  "Epoch 118 train loss: 0.9587, eval loss 1.6401976346969604\n",
                  "Epoch 119 train loss: 0.8008, eval loss 1.644758939743042\n",
                  "Epoch 120 train loss: 0.5873, eval loss 1.6412291526794434\n",
                  "Epoch 121 train loss: 0.9439, eval loss 1.6396543979644775\n",
                  "Epoch 122 train loss: 0.8439, eval loss 1.6393667459487915\n",
                  "Epoch 123 train loss: 0.8527, eval loss 1.6350082159042358\n",
                  "Epoch 124 train loss: 0.9510, eval loss 1.6387916803359985\n",
                  "Epoch 125 train loss: 0.9285, eval loss 1.6362468004226685\n",
                  "Epoch 126 train loss: 0.9336, eval loss 1.6358397006988525\n",
                  "Epoch 127 train loss: 0.9024, eval loss 1.6344331502914429\n",
                  "Epoch 128 train loss: 0.7868, eval loss 1.632407784461975\n",
                  "Epoch 129 train loss: 0.6697, eval loss 1.6305371522903442\n",
                  "Epoch 130 train loss: 0.9122, eval loss 1.6325184106826782\n",
                  "Epoch 131 train loss: 1.0275, eval loss 1.63124418258667\n",
                  "Epoch 132 train loss: 0.9416, eval loss 1.63263738155365\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:51:14,398] Trial 27 finished with value: 1.6305371522903442 and parameters: {'hidden_layers_size': 60, 'dropout_p': 0.47982771184794165, 'learning_rate': 3.577955374815888e-06, 'batch_size': 234, 'l2_reg': 5.919527564908214e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.2771, eval loss 1.7177616357803345\n",
                  "Epoch 1 train loss: 1.2727, eval loss 1.7166469097137451\n",
                  "Epoch 2 train loss: 1.3116, eval loss 1.7170028686523438\n",
                  "Epoch 3 train loss: 1.2433, eval loss 1.7163504362106323\n",
                  "Epoch 4 train loss: 1.3255, eval loss 1.7139432430267334\n",
                  "Epoch 5 train loss: 1.2617, eval loss 1.7126247882843018\n",
                  "Epoch 6 train loss: 1.2948, eval loss 1.712152123451233\n",
                  "Epoch 7 train loss: 1.2256, eval loss 1.7103025913238525\n",
                  "Epoch 8 train loss: 1.3471, eval loss 1.710056185722351\n",
                  "Epoch 9 train loss: 1.2161, eval loss 1.7073607444763184\n",
                  "Epoch 10 train loss: 1.2790, eval loss 1.706572413444519\n",
                  "Epoch 11 train loss: 1.2451, eval loss 1.7064988613128662\n",
                  "Epoch 12 train loss: 1.2516, eval loss 1.7061971426010132\n",
                  "Epoch 13 train loss: 1.2541, eval loss 1.70404851436615\n",
                  "Epoch 14 train loss: 1.2387, eval loss 1.7046082019805908\n",
                  "Epoch 15 train loss: 1.3183, eval loss 1.702425479888916\n",
                  "Epoch 16 train loss: 1.1571, eval loss 1.7015222311019897\n",
                  "Epoch 17 train loss: 1.2398, eval loss 1.700684666633606\n",
                  "Epoch 18 train loss: 1.2280, eval loss 1.6998952627182007\n",
                  "Epoch 19 train loss: 1.2629, eval loss 1.6990891695022583\n",
                  "Epoch 20 train loss: 1.2060, eval loss 1.6983211040496826\n",
                  "Epoch 21 train loss: 1.2388, eval loss 1.6983401775360107\n",
                  "Epoch 22 train loss: 1.2733, eval loss 1.6957361698150635\n",
                  "Epoch 23 train loss: 1.1591, eval loss 1.6958260536193848\n",
                  "Epoch 24 train loss: 1.1742, eval loss 1.6941192150115967\n",
                  "Epoch 25 train loss: 1.2390, eval loss 1.6929612159729004\n",
                  "Epoch 26 train loss: 1.2159, eval loss 1.6924934387207031\n",
                  "Epoch 27 train loss: 1.2082, eval loss 1.6918725967407227\n",
                  "Epoch 28 train loss: 1.2104, eval loss 1.689263105392456\n",
                  "Epoch 29 train loss: 1.1811, eval loss 1.6894217729568481\n",
                  "Epoch 30 train loss: 1.1906, eval loss 1.6880894899368286\n",
                  "Epoch 31 train loss: 1.1780, eval loss 1.6877148151397705\n",
                  "Epoch 32 train loss: 1.1377, eval loss 1.687106966972351\n",
                  "Epoch 33 train loss: 1.2287, eval loss 1.6866148710250854\n",
                  "Epoch 34 train loss: 1.1846, eval loss 1.6853336095809937\n",
                  "Epoch 35 train loss: 1.2078, eval loss 1.6832643747329712\n",
                  "Epoch 36 train loss: 1.1729, eval loss 1.6835172176361084\n",
                  "Epoch 37 train loss: 1.1940, eval loss 1.6811635494232178\n",
                  "Epoch 38 train loss: 1.1998, eval loss 1.681318759918213\n",
                  "Epoch 39 train loss: 1.1896, eval loss 1.6808075904846191\n",
                  "Epoch 40 train loss: 1.2467, eval loss 1.6807448863983154\n",
                  "Epoch 41 train loss: 1.1099, eval loss 1.6778382062911987\n",
                  "Epoch 42 train loss: 1.2199, eval loss 1.6794670820236206\n",
                  "Epoch 43 train loss: 1.1164, eval loss 1.6775798797607422\n",
                  "Epoch 44 train loss: 1.1155, eval loss 1.6743510961532593\n",
                  "Epoch 45 train loss: 1.1643, eval loss 1.6758546829223633\n",
                  "Epoch 46 train loss: 1.1153, eval loss 1.6739859580993652\n",
                  "Epoch 47 train loss: 1.1499, eval loss 1.6734652519226074\n",
                  "Epoch 48 train loss: 1.1596, eval loss 1.67233145236969\n",
                  "Epoch 49 train loss: 1.1586, eval loss 1.6692924499511719\n",
                  "Epoch 50 train loss: 1.1515, eval loss 1.6710624694824219\n",
                  "Epoch 51 train loss: 1.0702, eval loss 1.6706926822662354\n",
                  "Epoch 52 train loss: 1.1945, eval loss 1.6685824394226074\n",
                  "Epoch 53 train loss: 1.0958, eval loss 1.6692296266555786\n",
                  "Epoch 54 train loss: 1.1125, eval loss 1.6666330099105835\n",
                  "Epoch 55 train loss: 1.1139, eval loss 1.667123794555664\n",
                  "Epoch 56 train loss: 1.1637, eval loss 1.666227102279663\n",
                  "Epoch 57 train loss: 1.1423, eval loss 1.6661243438720703\n",
                  "Epoch 58 train loss: 1.1571, eval loss 1.6645396947860718\n",
                  "Epoch 59 train loss: 1.1024, eval loss 1.6638243198394775\n",
                  "Epoch 60 train loss: 1.0895, eval loss 1.6610870361328125\n",
                  "Epoch 61 train loss: 1.1144, eval loss 1.6612457036972046\n",
                  "Epoch 62 train loss: 1.1357, eval loss 1.6598668098449707\n",
                  "Epoch 63 train loss: 1.1237, eval loss 1.6580595970153809\n",
                  "Epoch 64 train loss: 1.0939, eval loss 1.6568692922592163\n",
                  "Epoch 65 train loss: 1.1253, eval loss 1.6585850715637207\n",
                  "Epoch 66 train loss: 1.1360, eval loss 1.6561980247497559\n",
                  "Epoch 67 train loss: 1.1841, eval loss 1.6556106805801392\n",
                  "Epoch 68 train loss: 1.1643, eval loss 1.6553759574890137\n",
                  "Epoch 69 train loss: 1.0860, eval loss 1.654436707496643\n",
                  "Epoch 70 train loss: 1.1021, eval loss 1.6535718441009521\n",
                  "Epoch 71 train loss: 1.1479, eval loss 1.6535155773162842\n",
                  "Epoch 72 train loss: 1.1760, eval loss 1.6522347927093506\n",
                  "Epoch 73 train loss: 1.0649, eval loss 1.6505930423736572\n",
                  "Epoch 74 train loss: 1.1152, eval loss 1.651138186454773\n",
                  "Epoch 75 train loss: 1.0959, eval loss 1.6494007110595703\n",
                  "Epoch 76 train loss: 1.0789, eval loss 1.6503111124038696\n",
                  "Epoch 77 train loss: 1.0929, eval loss 1.6485505104064941\n",
                  "Epoch 78 train loss: 1.1176, eval loss 1.6467878818511963\n",
                  "Epoch 79 train loss: 1.1106, eval loss 1.6457704305648804\n",
                  "Epoch 80 train loss: 1.1978, eval loss 1.644677996635437\n",
                  "Epoch 81 train loss: 1.0446, eval loss 1.6457128524780273\n",
                  "Epoch 82 train loss: 1.1154, eval loss 1.6454312801361084\n",
                  "Epoch 83 train loss: 1.0766, eval loss 1.6441494226455688\n",
                  "Epoch 84 train loss: 1.0957, eval loss 1.6423832178115845\n",
                  "Epoch 85 train loss: 1.0130, eval loss 1.6425695419311523\n",
                  "Epoch 86 train loss: 1.1023, eval loss 1.6410926580429077\n",
                  "Epoch 87 train loss: 1.0817, eval loss 1.6404743194580078\n",
                  "Epoch 88 train loss: 1.0734, eval loss 1.6396223306655884\n",
                  "Epoch 89 train loss: 1.0540, eval loss 1.637776494026184\n",
                  "Epoch 90 train loss: 1.0898, eval loss 1.638036847114563\n",
                  "Epoch 91 train loss: 1.0651, eval loss 1.6386263370513916\n",
                  "Epoch 92 train loss: 1.0394, eval loss 1.6384366750717163\n",
                  "Epoch 93 train loss: 1.0342, eval loss 1.63717782497406\n",
                  "Epoch 94 train loss: 1.0002, eval loss 1.6360244750976562\n",
                  "Epoch 95 train loss: 1.0154, eval loss 1.6348812580108643\n",
                  "Epoch 96 train loss: 1.0887, eval loss 1.6344292163848877\n",
                  "Epoch 97 train loss: 1.0722, eval loss 1.6338304281234741\n",
                  "Epoch 98 train loss: 1.0575, eval loss 1.632643461227417\n",
                  "Epoch 99 train loss: 1.0414, eval loss 1.6325324773788452\n",
                  "Epoch 100 train loss: 1.0686, eval loss 1.6321684122085571\n",
                  "Epoch 101 train loss: 1.0722, eval loss 1.6289376020431519\n",
                  "Epoch 102 train loss: 1.0574, eval loss 1.6302021741867065\n",
                  "Epoch 103 train loss: 1.0901, eval loss 1.6288012266159058\n",
                  "Epoch 104 train loss: 1.0217, eval loss 1.628291130065918\n",
                  "Epoch 105 train loss: 1.0469, eval loss 1.6270766258239746\n",
                  "Epoch 106 train loss: 0.9753, eval loss 1.6274539232254028\n",
                  "Epoch 107 train loss: 1.0824, eval loss 1.6255700588226318\n",
                  "Epoch 108 train loss: 1.0050, eval loss 1.6256121397018433\n",
                  "Epoch 109 train loss: 1.0235, eval loss 1.624563217163086\n",
                  "Epoch 110 train loss: 1.0282, eval loss 1.624458909034729\n",
                  "Epoch 111 train loss: 1.0171, eval loss 1.6232131719589233\n",
                  "Epoch 112 train loss: 1.0827, eval loss 1.6236169338226318\n",
                  "Epoch 113 train loss: 1.0147, eval loss 1.6225343942642212\n",
                  "Epoch 114 train loss: 1.0164, eval loss 1.6223978996276855\n",
                  "Epoch 115 train loss: 1.0684, eval loss 1.6211540699005127\n",
                  "Epoch 116 train loss: 1.0513, eval loss 1.621181607246399\n",
                  "Epoch 117 train loss: 0.9563, eval loss 1.6189309358596802\n",
                  "Epoch 118 train loss: 1.0013, eval loss 1.6193069219589233\n",
                  "Epoch 119 train loss: 1.0530, eval loss 1.61900794506073\n",
                  "Epoch 120 train loss: 1.0331, eval loss 1.617369532585144\n",
                  "Epoch 121 train loss: 1.0075, eval loss 1.616456151008606\n",
                  "Epoch 122 train loss: 0.9864, eval loss 1.6159371137619019\n",
                  "Epoch 123 train loss: 0.9527, eval loss 1.6169320344924927\n",
                  "Epoch 124 train loss: 1.1092, eval loss 1.6142083406448364\n",
                  "Epoch 125 train loss: 1.0263, eval loss 1.6160914897918701\n",
                  "Epoch 126 train loss: 0.9598, eval loss 1.6136500835418701\n",
                  "Epoch 127 train loss: 0.9646, eval loss 1.6139463186264038\n",
                  "Epoch 128 train loss: 0.9702, eval loss 1.6133766174316406\n",
                  "Epoch 129 train loss: 0.9578, eval loss 1.6140315532684326\n",
                  "Epoch 130 train loss: 0.9898, eval loss 1.6126761436462402\n",
                  "Epoch 131 train loss: 0.9284, eval loss 1.6108558177947998\n",
                  "Epoch 132 train loss: 0.9843, eval loss 1.6111011505126953\n",
                  "Epoch 133 train loss: 1.0138, eval loss 1.6105473041534424\n",
                  "Epoch 134 train loss: 1.0091, eval loss 1.6108837127685547\n",
                  "Epoch 135 train loss: 0.9681, eval loss 1.6099026203155518\n",
                  "Epoch 136 train loss: 0.9550, eval loss 1.608352541923523\n",
                  "Epoch 137 train loss: 0.9962, eval loss 1.606992244720459\n",
                  "Epoch 138 train loss: 0.9900, eval loss 1.6064426898956299\n",
                  "Epoch 139 train loss: 0.9941, eval loss 1.6057251691818237\n",
                  "Epoch 140 train loss: 1.0032, eval loss 1.6059894561767578\n",
                  "Epoch 141 train loss: 1.0591, eval loss 1.6057674884796143\n",
                  "Epoch 142 train loss: 0.9538, eval loss 1.6053776741027832\n",
                  "Epoch 143 train loss: 0.9288, eval loss 1.6029366254806519\n",
                  "Epoch 144 train loss: 1.0253, eval loss 1.6035431623458862\n",
                  "Epoch 145 train loss: 0.9734, eval loss 1.6028828620910645\n",
                  "Epoch 146 train loss: 0.9913, eval loss 1.603346586227417\n",
                  "Epoch 147 train loss: 0.9742, eval loss 1.6015342473983765\n",
                  "Epoch 148 train loss: 0.9591, eval loss 1.600983738899231\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:52:36,359] Trial 28 finished with value: 1.6001883745193481 and parameters: {'hidden_layers_size': 79, 'dropout_p': 0.4463536527276778, 'learning_rate': 1.4766328684709177e-06, 'batch_size': 170, 'l2_reg': 1.842529501485208e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.9733, eval loss 1.6001883745193481\n",
                  "Epoch 0 train loss: 1.5046, eval loss 1.757185459136963\n",
                  "Epoch 1 train loss: 1.4353, eval loss 1.7334235906600952\n",
                  "Epoch 2 train loss: 1.2461, eval loss 1.713902473449707\n",
                  "Epoch 3 train loss: 1.3236, eval loss 1.6958099603652954\n",
                  "Epoch 4 train loss: 1.3059, eval loss 1.6798927783966064\n",
                  "Epoch 5 train loss: 1.2648, eval loss 1.6658111810684204\n",
                  "Epoch 6 train loss: 1.1690, eval loss 1.654659628868103\n",
                  "Epoch 7 train loss: 1.1958, eval loss 1.643080472946167\n",
                  "Epoch 8 train loss: 1.1282, eval loss 1.6330649852752686\n",
                  "Epoch 9 train loss: 1.1166, eval loss 1.6222590208053589\n",
                  "Epoch 10 train loss: 1.0446, eval loss 1.6160861253738403\n",
                  "Epoch 11 train loss: 1.0212, eval loss 1.6068029403686523\n",
                  "Epoch 12 train loss: 1.0978, eval loss 1.6002264022827148\n",
                  "Epoch 13 train loss: 1.1367, eval loss 1.5936744213104248\n",
                  "Epoch 14 train loss: 0.9698, eval loss 1.5877196788787842\n",
                  "Epoch 15 train loss: 0.9761, eval loss 1.5817508697509766\n",
                  "Epoch 16 train loss: 1.0350, eval loss 1.5782742500305176\n",
                  "Epoch 17 train loss: 0.9178, eval loss 1.569973111152649\n",
                  "Epoch 18 train loss: 0.8807, eval loss 1.565456509590149\n",
                  "Epoch 19 train loss: 0.9856, eval loss 1.5605010986328125\n",
                  "Epoch 20 train loss: 0.9145, eval loss 1.5571011304855347\n",
                  "Epoch 21 train loss: 0.8481, eval loss 1.5519641637802124\n",
                  "Epoch 22 train loss: 0.9150, eval loss 1.5463058948516846\n",
                  "Epoch 23 train loss: 0.7977, eval loss 1.5431954860687256\n",
                  "Epoch 24 train loss: 0.8868, eval loss 1.541045069694519\n",
                  "Epoch 25 train loss: 0.8199, eval loss 1.5372790098190308\n",
                  "Epoch 26 train loss: 0.8664, eval loss 1.5336061716079712\n",
                  "Epoch 27 train loss: 0.8720, eval loss 1.530259609222412\n",
                  "Epoch 28 train loss: 0.7905, eval loss 1.5285718441009521\n",
                  "Epoch 29 train loss: 0.7954, eval loss 1.5253220796585083\n",
                  "Epoch 30 train loss: 0.7846, eval loss 1.5229911804199219\n",
                  "Epoch 31 train loss: 0.7456, eval loss 1.5190081596374512\n",
                  "Epoch 32 train loss: 0.8070, eval loss 1.5173453092575073\n",
                  "Epoch 33 train loss: 0.8734, eval loss 1.5127009153366089\n",
                  "Epoch 34 train loss: 0.7980, eval loss 1.5116593837738037\n",
                  "Epoch 35 train loss: 0.7366, eval loss 1.5084631443023682\n",
                  "Epoch 36 train loss: 0.8237, eval loss 1.5063809156417847\n",
                  "Epoch 37 train loss: 0.7160, eval loss 1.5045385360717773\n",
                  "Epoch 38 train loss: 0.7513, eval loss 1.5012791156768799\n",
                  "Epoch 39 train loss: 0.7343, eval loss 1.4994232654571533\n",
                  "Epoch 40 train loss: 0.7310, eval loss 1.4973585605621338\n",
                  "Epoch 41 train loss: 0.6313, eval loss 1.4954421520233154\n",
                  "Epoch 42 train loss: 0.7952, eval loss 1.4939321279525757\n",
                  "Epoch 43 train loss: 0.8009, eval loss 1.4916634559631348\n",
                  "Epoch 44 train loss: 0.7178, eval loss 1.4893840551376343\n",
                  "Epoch 45 train loss: 0.7128, eval loss 1.4893406629562378\n",
                  "Epoch 46 train loss: 0.7524, eval loss 1.487283706665039\n",
                  "Epoch 47 train loss: 0.6829, eval loss 1.4854373931884766\n",
                  "Epoch 48 train loss: 0.7493, eval loss 1.4827449321746826\n",
                  "Epoch 49 train loss: 0.6781, eval loss 1.4826207160949707\n",
                  "Epoch 50 train loss: 0.7652, eval loss 1.4805214405059814\n",
                  "Epoch 51 train loss: 0.6069, eval loss 1.4788453578948975\n",
                  "Epoch 52 train loss: 0.7393, eval loss 1.4781004190444946\n",
                  "Epoch 53 train loss: 0.6154, eval loss 1.476700782775879\n",
                  "Epoch 54 train loss: 0.6576, eval loss 1.4750688076019287\n",
                  "Epoch 55 train loss: 0.7518, eval loss 1.4739340543746948\n",
                  "Epoch 56 train loss: 0.7647, eval loss 1.4726130962371826\n",
                  "Epoch 57 train loss: 0.7259, eval loss 1.4723575115203857\n",
                  "Epoch 58 train loss: 0.5769, eval loss 1.4709956645965576\n",
                  "Epoch 59 train loss: 0.6968, eval loss 1.4692610502243042\n",
                  "Epoch 60 train loss: 0.6397, eval loss 1.4677175283432007\n",
                  "Epoch 61 train loss: 0.6731, eval loss 1.4684311151504517\n",
                  "Epoch 62 train loss: 0.6247, eval loss 1.466898798942566\n",
                  "Epoch 63 train loss: 0.6862, eval loss 1.4649732112884521\n",
                  "Epoch 64 train loss: 0.6638, eval loss 1.465075135231018\n",
                  "Epoch 65 train loss: 0.6404, eval loss 1.4633049964904785\n",
                  "Epoch 66 train loss: 0.6727, eval loss 1.4633158445358276\n",
                  "Epoch 67 train loss: 0.6189, eval loss 1.46257483959198\n",
                  "Epoch 68 train loss: 0.7569, eval loss 1.4618885517120361\n",
                  "Epoch 69 train loss: 0.6690, eval loss 1.4604119062423706\n",
                  "Epoch 70 train loss: 0.6602, eval loss 1.4597938060760498\n",
                  "Epoch 71 train loss: 0.6291, eval loss 1.459557056427002\n",
                  "Epoch 72 train loss: 0.5797, eval loss 1.4584910869598389\n",
                  "Epoch 73 train loss: 0.5734, eval loss 1.4583011865615845\n",
                  "Epoch 74 train loss: 0.6882, eval loss 1.4577932357788086\n",
                  "Epoch 75 train loss: 0.7203, eval loss 1.4564979076385498\n",
                  "Epoch 76 train loss: 0.5782, eval loss 1.455932378768921\n",
                  "Epoch 77 train loss: 0.5625, eval loss 1.4556807279586792\n",
                  "Epoch 78 train loss: 0.7000, eval loss 1.454479455947876\n",
                  "Epoch 79 train loss: 0.6484, eval loss 1.454861044883728\n",
                  "Epoch 80 train loss: 0.5771, eval loss 1.4536213874816895\n",
                  "Epoch 81 train loss: 0.7419, eval loss 1.4534566402435303\n",
                  "Epoch 82 train loss: 0.6608, eval loss 1.4532020092010498\n",
                  "Epoch 83 train loss: 0.7770, eval loss 1.4523124694824219\n",
                  "Epoch 84 train loss: 0.7116, eval loss 1.4512866735458374\n",
                  "Epoch 85 train loss: 0.6341, eval loss 1.4514683485031128\n",
                  "Epoch 86 train loss: 0.5333, eval loss 1.451780915260315\n",
                  "Epoch 87 train loss: 0.6819, eval loss 1.4498957395553589\n",
                  "Epoch 88 train loss: 0.6400, eval loss 1.4503581523895264\n",
                  "Epoch 89 train loss: 0.6702, eval loss 1.4494134187698364\n",
                  "Epoch 90 train loss: 0.5546, eval loss 1.4493716955184937\n",
                  "Epoch 91 train loss: 0.5973, eval loss 1.4492030143737793\n",
                  "Epoch 92 train loss: 0.6372, eval loss 1.4482121467590332\n",
                  "Epoch 93 train loss: 0.6856, eval loss 1.4479413032531738\n",
                  "Epoch 94 train loss: 0.5734, eval loss 1.4476685523986816\n",
                  "Epoch 95 train loss: 0.6388, eval loss 1.4472436904907227\n",
                  "Epoch 96 train loss: 0.5603, eval loss 1.4478610754013062\n",
                  "Epoch 97 train loss: 0.7282, eval loss 1.4460885524749756\n",
                  "Epoch 98 train loss: 0.6021, eval loss 1.4458329677581787\n",
                  "Epoch 99 train loss: 0.7133, eval loss 1.4455492496490479\n",
                  "Epoch 100 train loss: 0.5221, eval loss 1.4453943967819214\n",
                  "Epoch 101 train loss: 0.6059, eval loss 1.444701910018921\n",
                  "Epoch 102 train loss: 0.5647, eval loss 1.4446789026260376\n",
                  "Epoch 103 train loss: 0.5873, eval loss 1.4450608491897583\n",
                  "Epoch 104 train loss: 0.6323, eval loss 1.444161057472229\n",
                  "Epoch 105 train loss: 0.6590, eval loss 1.4445639848709106\n",
                  "Epoch 106 train loss: 0.5841, eval loss 1.4447877407073975\n",
                  "Epoch 107 train loss: 0.5677, eval loss 1.4438536167144775\n",
                  "Epoch 108 train loss: 0.6581, eval loss 1.4445891380310059\n",
                  "Epoch 109 train loss: 0.6711, eval loss 1.4437711238861084\n",
                  "Epoch 110 train loss: 0.6398, eval loss 1.443689227104187\n",
                  "Epoch 111 train loss: 0.7011, eval loss 1.4432127475738525\n",
                  "Epoch 112 train loss: 0.5778, eval loss 1.4422134160995483\n",
                  "Epoch 113 train loss: 0.5436, eval loss 1.4419505596160889\n",
                  "Epoch 114 train loss: 0.5903, eval loss 1.442033052444458\n",
                  "Epoch 115 train loss: 0.5682, eval loss 1.442536473274231\n",
                  "Epoch 116 train loss: 0.5565, eval loss 1.4421452283859253\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:53:37,256] Trial 29 finished with value: 1.4419505596160889 and parameters: {'hidden_layers_size': 162, 'dropout_p': 0.38057582115731, 'learning_rate': 1.806728087577791e-05, 'batch_size': 281, 'l2_reg': 1.0012225147470624e-05}. Best is trial 1 with value: 1.803401231765747.\n"
               ]
            }
         ],
         "source": [
            "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
            "study.optimize(objective, n_trials=30)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 165,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'hidden_layers_size': 87,\n",
                     " 'dropout_p': 0.489907983890005,\n",
                     " 'learning_rate': 1.5461728272636294e-06,\n",
                     " 'batch_size': 217,\n",
                     " 'l2_reg': 3.202440992612983e-05}"
                  ]
               },
               "execution_count": 165,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "trial_normal = study.best_trial\n",
            "\n",
            "trial_normal.params"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 166,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_model_stats(trial):\n",
            "\n",
            "    model = OptimizedMLP(\n",
            "        input_size = X_train.shape[1], \n",
            "        dropout_p = trial.params['dropout_p'],\n",
            "        hidden_layer_size = trial.params['hidden_layers_size']\n",
            "    )\n",
            "\n",
            "    learning_rate = trial.params['learning_rate']\n",
            "\n",
            "    batch_size = trial.params['batch_size']\n",
            "\n",
            "    l2_reg = trial.params['l2_reg']\n",
            "\n",
            "    optimizer = torch.optim.AdamW(\n",
            "        model.parameters(), \n",
            "        lr=learning_rate, \n",
            "        weight_decay=l2_reg\n",
            "    )\n",
            "\n",
            "    weights = compute_class_weight(\n",
            "        \"balanced\",\n",
            "        classes = np.unique(y),\n",
            "        y = y\n",
            "    )\n",
            "\n",
            "    loss_fn = torch.nn.BCEWithLogitsLoss(\n",
            "        weight = torch.from_numpy(weights)[1]\n",
            "    )\n",
            "\n",
            "    best_model, _, best_threshold = train(model, optimizer, loss_fn, batch_size)\n",
            "\n",
            "    return best_model, evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 167,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4518, eval loss 1.7525720596313477\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 1 train loss: 1.7764, eval loss 1.7541908025741577\n",
                  "Epoch 2 train loss: 1.7850, eval loss 1.749670147895813\n",
                  "Epoch 3 train loss: 1.4623, eval loss 1.7484052181243896\n",
                  "Epoch 4 train loss: 1.8020, eval loss 1.748795509338379\n",
                  "Epoch 5 train loss: 1.6802, eval loss 1.7490952014923096\n",
                  "Epoch 6 train loss: 1.6459, eval loss 1.7474589347839355\n",
                  "Epoch 7 train loss: 1.2107, eval loss 1.7454420328140259\n",
                  "Epoch 8 train loss: 1.3921, eval loss 1.746488332748413\n",
                  "Epoch 9 train loss: 1.3181, eval loss 1.742182970046997\n",
                  "Epoch 10 train loss: 1.9769, eval loss 1.743636965751648\n",
                  "Epoch 11 train loss: 1.6915, eval loss 1.7435826063156128\n",
                  "Epoch 12 train loss: 1.1238, eval loss 1.7427477836608887\n",
                  "Epoch 13 train loss: 1.2819, eval loss 1.7392184734344482\n",
                  "Epoch 14 train loss: 1.5363, eval loss 1.7394280433654785\n",
                  "Epoch 15 train loss: 1.3040, eval loss 1.7377707958221436\n",
                  "Epoch 16 train loss: 1.2825, eval loss 1.737252950668335\n",
                  "Epoch 17 train loss: 1.4492, eval loss 1.7370936870574951\n",
                  "Epoch 18 train loss: 1.3899, eval loss 1.733497977256775\n",
                  "Epoch 19 train loss: 1.9849, eval loss 1.7353686094284058\n",
                  "Epoch 20 train loss: 1.2020, eval loss 1.731025218963623\n",
                  "Epoch 21 train loss: 1.7230, eval loss 1.7328964471817017\n",
                  "Epoch 22 train loss: 1.1396, eval loss 1.72922682762146\n",
                  "Epoch 23 train loss: 1.4251, eval loss 1.7294230461120605\n",
                  "Epoch 24 train loss: 1.5417, eval loss 1.729026198387146\n",
                  "Epoch 25 train loss: 1.6641, eval loss 1.7306809425354004\n",
                  "Epoch 26 train loss: 1.4716, eval loss 1.7270632982254028\n",
                  "Epoch 27 train loss: 1.4822, eval loss 1.7261439561843872\n",
                  "Epoch 28 train loss: 1.2754, eval loss 1.7261217832565308\n",
                  "Epoch 29 train loss: 1.0223, eval loss 1.7239199876785278\n",
                  "Epoch 30 train loss: 1.3383, eval loss 1.7220888137817383\n",
                  "Epoch 31 train loss: 1.3297, eval loss 1.7238153219223022\n",
                  "Epoch 32 train loss: 1.3872, eval loss 1.7221040725708008\n",
                  "Epoch 33 train loss: 1.3038, eval loss 1.7226862907409668\n",
                  "Epoch 34 train loss: 1.1561, eval loss 1.7199751138687134\n",
                  "Epoch 35 train loss: 1.3917, eval loss 1.7205065488815308\n",
                  "Epoch 36 train loss: 1.3307, eval loss 1.7149733304977417\n",
                  "Epoch 37 train loss: 1.3505, eval loss 1.7150206565856934\n",
                  "Epoch 38 train loss: 1.0456, eval loss 1.7164850234985352\n",
                  "Epoch 39 train loss: 1.4653, eval loss 1.7165918350219727\n",
                  "Epoch 40 train loss: 1.4769, eval loss 1.7136257886886597\n",
                  "Epoch 41 train loss: 1.1117, eval loss 1.7120558023452759\n",
                  "Epoch 42 train loss: 1.1483, eval loss 1.7106578350067139\n",
                  "Epoch 43 train loss: 1.5297, eval loss 1.7128909826278687\n",
                  "Epoch 44 train loss: 1.5493, eval loss 1.7093160152435303\n",
                  "Epoch 45 train loss: 1.4395, eval loss 1.7085580825805664\n",
                  "Epoch 46 train loss: 1.4953, eval loss 1.706663727760315\n",
                  "Epoch 47 train loss: 1.3664, eval loss 1.7076698541641235\n",
                  "Epoch 48 train loss: 1.2506, eval loss 1.7024974822998047\n",
                  "Epoch 49 train loss: 1.3923, eval loss 1.7047394514083862\n",
                  "Epoch 50 train loss: 1.1503, eval loss 1.7011361122131348\n",
                  "Epoch 51 train loss: 1.3671, eval loss 1.703773021697998\n",
                  "Epoch 52 train loss: 1.3680, eval loss 1.70265793800354\n",
                  "Epoch 53 train loss: 1.3439, eval loss 1.7006916999816895\n",
                  "Epoch 54 train loss: 1.4803, eval loss 1.7009795904159546\n",
                  "Epoch 55 train loss: 1.5888, eval loss 1.7008227109909058\n",
                  "Epoch 56 train loss: 1.1162, eval loss 1.69718337059021\n",
                  "Epoch 57 train loss: 1.3897, eval loss 1.7006639242172241\n",
                  "Epoch 58 train loss: 1.1762, eval loss 1.6981000900268555\n",
                  "Epoch 59 train loss: 1.0789, eval loss 1.6963807344436646\n",
                  "Epoch 60 train loss: 1.2754, eval loss 1.6940255165100098\n",
                  "Epoch 61 train loss: 1.5972, eval loss 1.6935408115386963\n",
                  "Epoch 62 train loss: 1.5478, eval loss 1.690911054611206\n",
                  "Epoch 63 train loss: 0.9506, eval loss 1.6883314847946167\n",
                  "Epoch 64 train loss: 1.4651, eval loss 1.6949313879013062\n",
                  "Epoch 65 train loss: 1.1997, eval loss 1.6894668340682983\n",
                  "Epoch 66 train loss: 1.0516, eval loss 1.690386176109314\n",
                  "====================================\n",
                  "AUROC: 80.45%\n",
                  "F1: 56.22%\n",
                  "Precision: 44.62%\n",
                  "Recall: 75.96%\n"
               ]
            }
         ],
         "source": [
            "model_normal, test_metrics = get_model_stats(trial_normal)\n",
            "\n",
            "print(\"====================================\")\n",
            "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
            "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
            "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
            "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 168,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:54:08,564] A new study created in memory with name: no-name-8169d178-11fd-44d6-860e-2f7e6143765b\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.8483, eval loss 1.4294427633285522\n",
                  "Epoch 1 train loss: 0.7080, eval loss 1.4293476343154907\n",
                  "Epoch 2 train loss: 0.7429, eval loss 1.428944706916809\n",
                  "Epoch 3 train loss: 0.6840, eval loss 1.4253966808319092\n",
                  "Epoch 4 train loss: 0.8044, eval loss 1.4266140460968018\n",
                  "Epoch 5 train loss: 0.7936, eval loss 1.426932454109192\n",
                  "Epoch 6 train loss: 0.7518, eval loss 1.4290273189544678\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:54:14,342] Trial 0 finished with value: 1.4253966808319092 and parameters: {'hidden_layers_size': 150, 'dropout_p': 0.3628296173006377, 'learning_rate': 0.006425345715231554, 'batch_size': 128, 'l2_reg': 0.00010462337567426411}. Best is trial 0 with value: 1.4253966808319092.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.0986, eval loss 1.6443737745285034\n",
                  "Epoch 1 train loss: 0.9132, eval loss 1.5810939073562622\n",
                  "Epoch 2 train loss: 0.8471, eval loss 1.5462111234664917\n",
                  "Epoch 3 train loss: 0.7430, eval loss 1.5210002660751343\n",
                  "Epoch 4 train loss: 0.7164, eval loss 1.5024244785308838\n",
                  "Epoch 5 train loss: 0.6614, eval loss 1.4885109663009644\n",
                  "Epoch 6 train loss: 0.7938, eval loss 1.478353500366211\n",
                  "Epoch 7 train loss: 0.6661, eval loss 1.4710912704467773\n",
                  "Epoch 8 train loss: 0.5395, eval loss 1.4644955396652222\n",
                  "Epoch 9 train loss: 0.5444, eval loss 1.4594765901565552\n",
                  "Epoch 10 train loss: 0.5207, eval loss 1.455345630645752\n",
                  "Epoch 11 train loss: 0.6399, eval loss 1.451279878616333\n",
                  "Epoch 12 train loss: 0.5078, eval loss 1.4484246969223022\n",
                  "Epoch 13 train loss: 0.3864, eval loss 1.447231650352478\n",
                  "Epoch 14 train loss: 0.4575, eval loss 1.444014072418213\n",
                  "Epoch 15 train loss: 0.4260, eval loss 1.4437662363052368\n",
                  "Epoch 16 train loss: 0.4313, eval loss 1.4409680366516113\n",
                  "Epoch 17 train loss: 0.4543, eval loss 1.4395335912704468\n",
                  "Epoch 18 train loss: 0.3838, eval loss 1.43954336643219\n",
                  "Epoch 19 train loss: 0.3797, eval loss 1.4376556873321533\n",
                  "Epoch 20 train loss: 0.4016, eval loss 1.4363725185394287\n",
                  "Epoch 21 train loss: 0.3438, eval loss 1.4360504150390625\n",
                  "Epoch 22 train loss: 0.2681, eval loss 1.435094952583313\n",
                  "Epoch 23 train loss: 0.3685, eval loss 1.4342303276062012\n",
                  "Epoch 24 train loss: 0.2895, eval loss 1.4341962337493896\n",
                  "Epoch 25 train loss: 0.2762, eval loss 1.4327619075775146\n",
                  "Epoch 26 train loss: 0.2687, eval loss 1.4330371618270874\n",
                  "Epoch 27 train loss: 0.3272, eval loss 1.4322603940963745\n",
                  "Epoch 28 train loss: 0.3341, eval loss 1.4318886995315552\n",
                  "Epoch 29 train loss: 0.2475, eval loss 1.4322844743728638\n",
                  "Epoch 30 train loss: 0.2861, eval loss 1.4314477443695068\n",
                  "Epoch 31 train loss: 0.2646, eval loss 1.4318957328796387\n",
                  "Epoch 32 train loss: 0.2768, eval loss 1.4315171241760254\n",
                  "Epoch 33 train loss: 0.2821, eval loss 1.4295685291290283\n",
                  "Epoch 34 train loss: 0.2523, eval loss 1.4301154613494873\n",
                  "Epoch 35 train loss: 0.2095, eval loss 1.4304709434509277\n",
                  "Epoch 36 train loss: 0.3007, eval loss 1.4299983978271484\n",
                  "Epoch 37 train loss: 0.2919, eval loss 1.4287505149841309\n",
                  "Epoch 38 train loss: 0.2656, eval loss 1.4285881519317627\n",
                  "Epoch 39 train loss: 0.2096, eval loss 1.4284683465957642\n",
                  "Epoch 40 train loss: 0.2311, eval loss 1.4281625747680664\n",
                  "Epoch 41 train loss: 0.2161, eval loss 1.4285694360733032\n",
                  "Epoch 42 train loss: 0.2237, eval loss 1.4280891418457031\n",
                  "Epoch 43 train loss: 0.1916, eval loss 1.4275962114334106\n",
                  "Epoch 44 train loss: 0.2943, eval loss 1.4279855489730835\n",
                  "Epoch 45 train loss: 0.2258, eval loss 1.4281518459320068\n",
                  "Epoch 46 train loss: 0.1740, eval loss 1.4271790981292725\n",
                  "Epoch 47 train loss: 0.2333, eval loss 1.4270048141479492\n",
                  "Epoch 48 train loss: 0.1832, eval loss 1.4275346994400024\n",
                  "Epoch 49 train loss: 0.2300, eval loss 1.427873969078064\n",
                  "Epoch 50 train loss: 0.2279, eval loss 1.4270555973052979\n",
                  "Epoch 51 train loss: 0.1486, eval loss 1.426522135734558\n",
                  "Epoch 52 train loss: 0.1702, eval loss 1.4256455898284912\n",
                  "Epoch 53 train loss: 0.1438, eval loss 1.4264591932296753\n",
                  "Epoch 54 train loss: 0.1655, eval loss 1.4269726276397705\n",
                  "Epoch 55 train loss: 0.1944, eval loss 1.4255282878875732\n",
                  "Epoch 56 train loss: 0.2177, eval loss 1.4265128374099731\n",
                  "Epoch 57 train loss: 0.1733, eval loss 1.4263728857040405\n",
                  "Epoch 58 train loss: 0.2087, eval loss 1.4257477521896362\n",
                  "Epoch 59 train loss: 0.1793, eval loss 1.4254721403121948\n",
                  "Epoch 60 train loss: 0.2025, eval loss 1.4257327318191528\n",
                  "Epoch 61 train loss: 0.1235, eval loss 1.4251850843429565\n",
                  "Epoch 62 train loss: 0.1716, eval loss 1.4256469011306763\n",
                  "Epoch 63 train loss: 0.1915, eval loss 1.424390196800232\n",
                  "Epoch 64 train loss: 0.1908, eval loss 1.426061987876892\n",
                  "Epoch 65 train loss: 0.1588, eval loss 1.425337791442871\n",
                  "Epoch 66 train loss: 0.1713, eval loss 1.4250048398971558\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:54:39,619] Trial 1 finished with value: 1.424390196800232 and parameters: {'hidden_layers_size': 137, 'dropout_p': 0.3027271806643279, 'learning_rate': 0.00023935513600678034, 'batch_size': 473, 'l2_reg': 0.00039569760858591824}. Best is trial 0 with value: 1.4253966808319092.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.8268, eval loss 1.560943365097046\n",
                  "Epoch 1 train loss: 0.7713, eval loss 1.4803935289382935\n",
                  "Epoch 2 train loss: 0.7145, eval loss 1.4602059125900269\n",
                  "Epoch 3 train loss: 0.6996, eval loss 1.4514232873916626\n",
                  "Epoch 4 train loss: 0.6841, eval loss 1.4453935623168945\n",
                  "Epoch 5 train loss: 0.6674, eval loss 1.442694067955017\n",
                  "Epoch 6 train loss: 0.6668, eval loss 1.4393765926361084\n",
                  "Epoch 7 train loss: 0.6597, eval loss 1.438163161277771\n",
                  "Epoch 8 train loss: 0.6514, eval loss 1.4371010065078735\n",
                  "Epoch 9 train loss: 0.6369, eval loss 1.4360684156417847\n",
                  "Epoch 10 train loss: 0.6592, eval loss 1.435540795326233\n",
                  "Epoch 11 train loss: 0.6417, eval loss 1.4341555833816528\n",
                  "Epoch 12 train loss: 0.6659, eval loss 1.4337536096572876\n",
                  "Epoch 13 train loss: 0.6272, eval loss 1.432093620300293\n",
                  "Epoch 14 train loss: 0.6166, eval loss 1.4330215454101562\n",
                  "Epoch 15 train loss: 0.6370, eval loss 1.4315185546875\n",
                  "Epoch 16 train loss: 0.6157, eval loss 1.4313441514968872\n",
                  "Epoch 17 train loss: 0.6402, eval loss 1.4323139190673828\n",
                  "Epoch 18 train loss: 0.6153, eval loss 1.432389259338379\n",
                  "Epoch 19 train loss: 0.6220, eval loss 1.431091547012329\n",
                  "Epoch 20 train loss: 0.6321, eval loss 1.430795669555664\n",
                  "Epoch 21 train loss: 0.6243, eval loss 1.4310417175292969\n",
                  "Epoch 22 train loss: 0.6163, eval loss 1.4310872554779053\n",
                  "Epoch 23 train loss: 0.6500, eval loss 1.4305078983306885\n",
                  "Epoch 24 train loss: 0.6132, eval loss 1.431584358215332\n",
                  "Epoch 25 train loss: 0.6047, eval loss 1.4306786060333252\n",
                  "Epoch 26 train loss: 0.6057, eval loss 1.4295368194580078\n",
                  "Epoch 27 train loss: 0.6318, eval loss 1.430100679397583\n",
                  "Epoch 28 train loss: 0.6391, eval loss 1.4303549528121948\n",
                  "Epoch 29 train loss: 0.6077, eval loss 1.4292223453521729\n",
                  "Epoch 30 train loss: 0.6174, eval loss 1.4289270639419556\n",
                  "Epoch 31 train loss: 0.5935, eval loss 1.429765224456787\n",
                  "Epoch 32 train loss: 0.6255, eval loss 1.4295248985290527\n",
                  "Epoch 33 train loss: 0.6177, eval loss 1.428704857826233\n",
                  "Epoch 34 train loss: 0.6105, eval loss 1.4297442436218262\n",
                  "Epoch 35 train loss: 0.6055, eval loss 1.428587555885315\n",
                  "Epoch 36 train loss: 0.6217, eval loss 1.4266722202301025\n",
                  "Epoch 37 train loss: 0.6171, eval loss 1.4282159805297852\n",
                  "Epoch 38 train loss: 0.5999, eval loss 1.4280296564102173\n",
                  "Epoch 39 train loss: 0.6183, eval loss 1.4297757148742676\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:54:54,521] Trial 2 finished with value: 1.4266722202301025 and parameters: {'hidden_layers_size': 167, 'dropout_p': 0.3983427779942578, 'learning_rate': 0.0007384995504551768, 'batch_size': 498, 'l2_reg': 2.3695382664317218e-05}. Best is trial 2 with value: 1.4266722202301025.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7413, eval loss 1.4468261003494263\n",
                  "Epoch 1 train loss: 0.6681, eval loss 1.4434963464736938\n",
                  "Epoch 2 train loss: 0.6706, eval loss 1.441253900527954\n",
                  "Epoch 3 train loss: 0.6742, eval loss 1.4411455392837524\n",
                  "Epoch 4 train loss: 0.6564, eval loss 1.438961148262024\n",
                  "Epoch 5 train loss: 0.6533, eval loss 1.4368897676467896\n",
                  "Epoch 6 train loss: 0.6650, eval loss 1.4372481107711792\n",
                  "Epoch 7 train loss: 0.6563, eval loss 1.435738444328308\n",
                  "Epoch 8 train loss: 0.6441, eval loss 1.4371559619903564\n",
                  "Epoch 9 train loss: 0.6357, eval loss 1.4343631267547607\n",
                  "Epoch 10 train loss: 0.6774, eval loss 1.4370343685150146\n",
                  "Epoch 11 train loss: 0.6407, eval loss 1.4342138767242432\n",
                  "Epoch 12 train loss: 0.6522, eval loss 1.4328484535217285\n",
                  "Epoch 13 train loss: 0.6621, eval loss 1.432303547859192\n",
                  "Epoch 14 train loss: 0.6529, eval loss 1.4363073110580444\n",
                  "Epoch 15 train loss: 0.6252, eval loss 1.430784821510315\n",
                  "Epoch 16 train loss: 0.6355, eval loss 1.43209707736969\n",
                  "Epoch 17 train loss: 0.6403, eval loss 1.4341334104537964\n",
                  "Epoch 18 train loss: 0.6221, eval loss 1.4331467151641846\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:55:01,418] Trial 3 finished with value: 1.430784821510315 and parameters: {'hidden_layers_size': 88, 'dropout_p': 0.3666127723839712, 'learning_rate': 0.004845359548126126, 'batch_size': 509, 'l2_reg': 0.0008661869185245984}. Best is trial 3 with value: 1.430784821510315.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4560, eval loss 1.7419003248214722\n",
                  "Epoch 1 train loss: 1.3338, eval loss 1.7187784910202026\n",
                  "Epoch 2 train loss: 1.2677, eval loss 1.6996768712997437\n",
                  "Epoch 3 train loss: 1.2751, eval loss 1.6833022832870483\n",
                  "Epoch 4 train loss: 1.2461, eval loss 1.668806552886963\n",
                  "Epoch 5 train loss: 1.1946, eval loss 1.6555845737457275\n",
                  "Epoch 6 train loss: 1.1721, eval loss 1.6447018384933472\n",
                  "Epoch 7 train loss: 1.1944, eval loss 1.63323175907135\n",
                  "Epoch 8 train loss: 1.1612, eval loss 1.6225416660308838\n",
                  "Epoch 9 train loss: 0.9837, eval loss 1.6145731210708618\n",
                  "Epoch 10 train loss: 1.0237, eval loss 1.605860710144043\n",
                  "Epoch 11 train loss: 1.0361, eval loss 1.59811270236969\n",
                  "Epoch 12 train loss: 0.9987, eval loss 1.5896975994110107\n",
                  "Epoch 13 train loss: 0.9479, eval loss 1.5843249559402466\n",
                  "Epoch 14 train loss: 0.9307, eval loss 1.578379511833191\n",
                  "Epoch 15 train loss: 0.9731, eval loss 1.5713821649551392\n",
                  "Epoch 16 train loss: 0.9480, eval loss 1.5644980669021606\n",
                  "Epoch 17 train loss: 0.9661, eval loss 1.5607964992523193\n",
                  "Epoch 18 train loss: 0.8657, eval loss 1.5533368587493896\n",
                  "Epoch 19 train loss: 0.8977, eval loss 1.548988938331604\n",
                  "Epoch 20 train loss: 0.8171, eval loss 1.5447856187820435\n",
                  "Epoch 21 train loss: 0.9136, eval loss 1.5389862060546875\n",
                  "Epoch 22 train loss: 0.8596, eval loss 1.5355061292648315\n",
                  "Epoch 23 train loss: 0.8520, eval loss 1.531786322593689\n",
                  "Epoch 24 train loss: 0.8548, eval loss 1.526189923286438\n",
                  "Epoch 25 train loss: 0.7832, eval loss 1.5221729278564453\n",
                  "Epoch 26 train loss: 0.8734, eval loss 1.518396258354187\n",
                  "Epoch 27 train loss: 0.7840, eval loss 1.5155901908874512\n",
                  "Epoch 28 train loss: 0.7309, eval loss 1.5116877555847168\n",
                  "Epoch 29 train loss: 0.8791, eval loss 1.5079249143600464\n",
                  "Epoch 30 train loss: 0.8132, eval loss 1.504732608795166\n",
                  "Epoch 31 train loss: 0.8789, eval loss 1.5007175207138062\n",
                  "Epoch 32 train loss: 0.7391, eval loss 1.498665452003479\n",
                  "Epoch 33 train loss: 0.7238, eval loss 1.4947856664657593\n",
                  "Epoch 34 train loss: 0.7401, eval loss 1.4926503896713257\n",
                  "Epoch 35 train loss: 0.7676, eval loss 1.4896821975708008\n",
                  "Epoch 36 train loss: 0.7542, eval loss 1.4874025583267212\n",
                  "Epoch 37 train loss: 0.7821, eval loss 1.4846285581588745\n",
                  "Epoch 38 train loss: 0.6976, eval loss 1.4822142124176025\n",
                  "Epoch 39 train loss: 0.7498, eval loss 1.4809552431106567\n",
                  "Epoch 40 train loss: 0.8548, eval loss 1.477429986000061\n",
                  "Epoch 41 train loss: 0.7603, eval loss 1.476766586303711\n",
                  "Epoch 42 train loss: 0.8354, eval loss 1.4747319221496582\n",
                  "Epoch 43 train loss: 0.6574, eval loss 1.4714598655700684\n",
                  "Epoch 44 train loss: 0.7575, eval loss 1.4694900512695312\n",
                  "Epoch 45 train loss: 0.7177, eval loss 1.4681556224822998\n",
                  "Epoch 46 train loss: 0.7397, eval loss 1.4664373397827148\n",
                  "Epoch 47 train loss: 0.7395, eval loss 1.4652334451675415\n",
                  "Epoch 48 train loss: 0.7455, eval loss 1.4646410942077637\n",
                  "Epoch 49 train loss: 0.7127, eval loss 1.462437629699707\n",
                  "Epoch 50 train loss: 0.7224, eval loss 1.460503339767456\n",
                  "Epoch 51 train loss: 0.6504, eval loss 1.4597724676132202\n",
                  "Epoch 52 train loss: 0.6708, eval loss 1.4590740203857422\n",
                  "Epoch 53 train loss: 0.7199, eval loss 1.4576823711395264\n",
                  "Epoch 54 train loss: 0.6993, eval loss 1.4556775093078613\n",
                  "Epoch 55 train loss: 0.7104, eval loss 1.4556914567947388\n",
                  "Epoch 56 train loss: 0.6817, eval loss 1.4548187255859375\n",
                  "Epoch 57 train loss: 0.7944, eval loss 1.4543377161026\n",
                  "Epoch 58 train loss: 0.8136, eval loss 1.4526053667068481\n",
                  "Epoch 59 train loss: 0.7213, eval loss 1.4517617225646973\n",
                  "Epoch 60 train loss: 0.6065, eval loss 1.4509059190750122\n",
                  "Epoch 61 train loss: 0.7218, eval loss 1.450661063194275\n",
                  "Epoch 62 train loss: 0.7609, eval loss 1.4493837356567383\n",
                  "Epoch 63 train loss: 0.7091, eval loss 1.4486695528030396\n",
                  "Epoch 64 train loss: 0.7655, eval loss 1.4478528499603271\n",
                  "Epoch 65 train loss: 0.7504, eval loss 1.447399616241455\n",
                  "Epoch 66 train loss: 0.6538, eval loss 1.4476138353347778\n",
                  "Epoch 67 train loss: 0.7551, eval loss 1.4463176727294922\n",
                  "Epoch 68 train loss: 0.6935, eval loss 1.4468199014663696\n",
                  "Epoch 69 train loss: 0.6573, eval loss 1.4453387260437012\n",
                  "Epoch 70 train loss: 0.6816, eval loss 1.4462566375732422\n",
                  "Epoch 71 train loss: 0.7149, eval loss 1.4451878070831299\n",
                  "Epoch 72 train loss: 0.6505, eval loss 1.4451395273208618\n",
                  "Epoch 73 train loss: 0.6757, eval loss 1.4433377981185913\n",
                  "Epoch 74 train loss: 0.7462, eval loss 1.4437034130096436\n",
                  "Epoch 75 train loss: 0.7925, eval loss 1.443716049194336\n",
                  "Epoch 76 train loss: 0.7800, eval loss 1.4424574375152588\n",
                  "Epoch 77 train loss: 0.6946, eval loss 1.4424631595611572\n",
                  "Epoch 78 train loss: 0.6950, eval loss 1.4434067010879517\n",
                  "Epoch 79 train loss: 0.7264, eval loss 1.4418823719024658\n",
                  "Epoch 80 train loss: 0.7161, eval loss 1.4416981935501099\n",
                  "Epoch 81 train loss: 0.7053, eval loss 1.4417566061019897\n",
                  "Epoch 82 train loss: 0.6617, eval loss 1.4417067766189575\n",
                  "Epoch 83 train loss: 0.6719, eval loss 1.4414788484573364\n",
                  "Epoch 84 train loss: 0.6847, eval loss 1.4413437843322754\n",
                  "Epoch 85 train loss: 0.6731, eval loss 1.4414361715316772\n",
                  "Epoch 86 train loss: 0.7195, eval loss 1.4410347938537598\n",
                  "Epoch 87 train loss: 0.8492, eval loss 1.4403291940689087\n",
                  "Epoch 88 train loss: 0.6937, eval loss 1.440032958984375\n",
                  "Epoch 89 train loss: 0.6825, eval loss 1.44023597240448\n",
                  "Epoch 90 train loss: 0.8149, eval loss 1.4398469924926758\n",
                  "Epoch 91 train loss: 0.6729, eval loss 1.4397093057632446\n",
                  "Epoch 92 train loss: 0.5751, eval loss 1.4386723041534424\n",
                  "Epoch 93 train loss: 0.6445, eval loss 1.4395116567611694\n",
                  "Epoch 94 train loss: 0.5725, eval loss 1.4391509294509888\n",
                  "Epoch 95 train loss: 0.6462, eval loss 1.4393601417541504\n",
                  "Epoch 96 train loss: 0.6287, eval loss 1.437894582748413\n",
                  "Epoch 97 train loss: 0.6507, eval loss 1.4379346370697021\n",
                  "Epoch 98 train loss: 0.6909, eval loss 1.4377845525741577\n",
                  "Epoch 99 train loss: 0.7306, eval loss 1.4370088577270508\n",
                  "Epoch 100 train loss: 0.6398, eval loss 1.4376556873321533\n",
                  "Epoch 101 train loss: 0.6857, eval loss 1.4372634887695312\n",
                  "Epoch 102 train loss: 0.6399, eval loss 1.4366443157196045\n",
                  "Epoch 103 train loss: 0.6304, eval loss 1.4381086826324463\n",
                  "Epoch 104 train loss: 0.7226, eval loss 1.4376646280288696\n",
                  "Epoch 105 train loss: 0.6123, eval loss 1.4378947019577026\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:55:58,659] Trial 4 finished with value: 1.4366443157196045 and parameters: {'hidden_layers_size': 55, 'dropout_p': 0.32531481624838055, 'learning_rate': 2.7013444623979626e-05, 'batch_size': 129, 'l2_reg': 2.3681946724159617e-05}. Best is trial 4 with value: 1.4366443157196045.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.6389, eval loss 1.807981252670288\n",
                  "Epoch 1 train loss: 1.5956, eval loss 1.8004602193832397\n",
                  "Epoch 2 train loss: 1.5809, eval loss 1.7975214719772339\n",
                  "Epoch 3 train loss: 1.5369, eval loss 1.7919362783432007\n",
                  "Epoch 4 train loss: 1.5817, eval loss 1.7867006063461304\n",
                  "Epoch 5 train loss: 1.4635, eval loss 1.7788697481155396\n",
                  "Epoch 6 train loss: 1.4706, eval loss 1.775214433670044\n",
                  "Epoch 7 train loss: 1.4775, eval loss 1.7680587768554688\n",
                  "Epoch 8 train loss: 1.4599, eval loss 1.7652909755706787\n",
                  "Epoch 9 train loss: 1.3826, eval loss 1.7609078884124756\n",
                  "Epoch 10 train loss: 1.3957, eval loss 1.7545236349105835\n",
                  "Epoch 11 train loss: 1.3255, eval loss 1.7494133710861206\n",
                  "Epoch 12 train loss: 1.3689, eval loss 1.7447441816329956\n",
                  "Epoch 13 train loss: 1.3728, eval loss 1.7398957014083862\n",
                  "Epoch 14 train loss: 1.3422, eval loss 1.7362419366836548\n",
                  "Epoch 15 train loss: 1.3713, eval loss 1.7329868078231812\n",
                  "Epoch 16 train loss: 1.3096, eval loss 1.7280950546264648\n",
                  "Epoch 17 train loss: 1.3244, eval loss 1.723274827003479\n",
                  "Epoch 18 train loss: 1.2797, eval loss 1.719258189201355\n",
                  "Epoch 19 train loss: 1.3093, eval loss 1.7140400409698486\n",
                  "Epoch 20 train loss: 1.2757, eval loss 1.7097703218460083\n",
                  "Epoch 21 train loss: 1.2869, eval loss 1.7070400714874268\n",
                  "Epoch 22 train loss: 1.2148, eval loss 1.7022819519042969\n",
                  "Epoch 23 train loss: 1.2452, eval loss 1.7000596523284912\n",
                  "Epoch 24 train loss: 1.2141, eval loss 1.695723533630371\n",
                  "Epoch 25 train loss: 1.1896, eval loss 1.6921237707138062\n",
                  "Epoch 26 train loss: 1.2196, eval loss 1.688851237297058\n",
                  "Epoch 27 train loss: 1.2342, eval loss 1.6852936744689941\n",
                  "Epoch 28 train loss: 1.2051, eval loss 1.6805002689361572\n",
                  "Epoch 29 train loss: 1.1977, eval loss 1.6786463260650635\n",
                  "Epoch 30 train loss: 1.1930, eval loss 1.6732982397079468\n",
                  "Epoch 31 train loss: 1.2234, eval loss 1.6725456714630127\n",
                  "Epoch 32 train loss: 1.1967, eval loss 1.6687805652618408\n",
                  "Epoch 33 train loss: 1.1252, eval loss 1.6653518676757812\n",
                  "Epoch 34 train loss: 1.1028, eval loss 1.6631344556808472\n",
                  "Epoch 35 train loss: 1.1292, eval loss 1.6580599546432495\n",
                  "Epoch 36 train loss: 1.1319, eval loss 1.655574083328247\n",
                  "Epoch 37 train loss: 1.0893, eval loss 1.6540417671203613\n",
                  "Epoch 38 train loss: 1.1051, eval loss 1.6496236324310303\n",
                  "Epoch 39 train loss: 1.0971, eval loss 1.6463121175765991\n",
                  "Epoch 40 train loss: 1.1439, eval loss 1.6449763774871826\n",
                  "Epoch 41 train loss: 1.0698, eval loss 1.6426727771759033\n",
                  "Epoch 42 train loss: 1.0815, eval loss 1.638780951499939\n",
                  "Epoch 43 train loss: 1.0420, eval loss 1.6372480392456055\n",
                  "Epoch 44 train loss: 1.0708, eval loss 1.6332104206085205\n",
                  "Epoch 45 train loss: 1.0322, eval loss 1.6322230100631714\n",
                  "Epoch 46 train loss: 1.0684, eval loss 1.6286534070968628\n",
                  "Epoch 47 train loss: 1.0900, eval loss 1.6266547441482544\n",
                  "Epoch 48 train loss: 1.0862, eval loss 1.6261504888534546\n",
                  "Epoch 49 train loss: 1.0897, eval loss 1.6212263107299805\n",
                  "Epoch 50 train loss: 1.0679, eval loss 1.6191179752349854\n",
                  "Epoch 51 train loss: 1.0314, eval loss 1.6157625913619995\n",
                  "Epoch 52 train loss: 1.0753, eval loss 1.6142257452011108\n",
                  "Epoch 53 train loss: 1.0471, eval loss 1.611889123916626\n",
                  "Epoch 54 train loss: 1.0195, eval loss 1.609240174293518\n",
                  "Epoch 55 train loss: 0.9965, eval loss 1.6082473993301392\n",
                  "Epoch 56 train loss: 0.9910, eval loss 1.6044273376464844\n",
                  "Epoch 57 train loss: 0.9946, eval loss 1.6025632619857788\n",
                  "Epoch 58 train loss: 0.9410, eval loss 1.601472020149231\n",
                  "Epoch 59 train loss: 0.9912, eval loss 1.6000697612762451\n",
                  "Epoch 60 train loss: 0.9096, eval loss 1.597005844116211\n",
                  "Epoch 61 train loss: 0.9721, eval loss 1.59441339969635\n",
                  "Epoch 62 train loss: 0.9134, eval loss 1.5924111604690552\n",
                  "Epoch 63 train loss: 0.9564, eval loss 1.5903029441833496\n",
                  "Epoch 64 train loss: 0.9151, eval loss 1.5889801979064941\n",
                  "Epoch 65 train loss: 0.9482, eval loss 1.5861916542053223\n",
                  "Epoch 66 train loss: 0.9579, eval loss 1.5856679677963257\n",
                  "Epoch 67 train loss: 1.0227, eval loss 1.5832464694976807\n",
                  "Epoch 68 train loss: 0.9540, eval loss 1.5817313194274902\n",
                  "Epoch 69 train loss: 1.0035, eval loss 1.5791467428207397\n",
                  "Epoch 70 train loss: 0.8659, eval loss 1.5796198844909668\n",
                  "Epoch 71 train loss: 0.9342, eval loss 1.5761604309082031\n",
                  "Epoch 72 train loss: 0.9495, eval loss 1.574589490890503\n",
                  "Epoch 73 train loss: 0.9444, eval loss 1.5722711086273193\n",
                  "Epoch 74 train loss: 0.8868, eval loss 1.5688225030899048\n",
                  "Epoch 75 train loss: 0.8882, eval loss 1.5687799453735352\n",
                  "Epoch 76 train loss: 0.9303, eval loss 1.567748785018921\n",
                  "Epoch 77 train loss: 0.9343, eval loss 1.5667445659637451\n",
                  "Epoch 78 train loss: 0.8525, eval loss 1.5657554864883423\n",
                  "Epoch 79 train loss: 0.9005, eval loss 1.561161994934082\n",
                  "Epoch 80 train loss: 0.8587, eval loss 1.560178518295288\n",
                  "Epoch 81 train loss: 0.9289, eval loss 1.558426022529602\n",
                  "Epoch 82 train loss: 0.8427, eval loss 1.5572259426116943\n",
                  "Epoch 83 train loss: 0.8805, eval loss 1.5582681894302368\n",
                  "Epoch 84 train loss: 0.8632, eval loss 1.5540285110473633\n",
                  "Epoch 85 train loss: 0.9005, eval loss 1.5537208318710327\n",
                  "Epoch 86 train loss: 0.9289, eval loss 1.5518124103546143\n",
                  "Epoch 87 train loss: 0.8766, eval loss 1.5498582124710083\n",
                  "Epoch 88 train loss: 0.8451, eval loss 1.548403263092041\n",
                  "Epoch 89 train loss: 0.8440, eval loss 1.5473923683166504\n",
                  "Epoch 90 train loss: 0.7762, eval loss 1.5457491874694824\n",
                  "Epoch 91 train loss: 0.8902, eval loss 1.5424410104751587\n",
                  "Epoch 92 train loss: 0.8230, eval loss 1.541938066482544\n",
                  "Epoch 93 train loss: 0.8772, eval loss 1.5408225059509277\n",
                  "Epoch 94 train loss: 0.8097, eval loss 1.5402774810791016\n",
                  "Epoch 95 train loss: 0.8155, eval loss 1.537996768951416\n",
                  "Epoch 96 train loss: 0.9016, eval loss 1.5378551483154297\n",
                  "Epoch 97 train loss: 0.8407, eval loss 1.5341763496398926\n",
                  "Epoch 98 train loss: 0.8740, eval loss 1.5340921878814697\n",
                  "Epoch 99 train loss: 0.8065, eval loss 1.533538818359375\n",
                  "Epoch 100 train loss: 0.8494, eval loss 1.5316972732543945\n",
                  "Epoch 101 train loss: 0.8471, eval loss 1.5308178663253784\n",
                  "Epoch 102 train loss: 0.8487, eval loss 1.5299973487854004\n",
                  "Epoch 103 train loss: 0.8666, eval loss 1.5264668464660645\n",
                  "Epoch 104 train loss: 0.8670, eval loss 1.5268632173538208\n",
                  "Epoch 105 train loss: 0.7964, eval loss 1.5250738859176636\n",
                  "Epoch 106 train loss: 0.8587, eval loss 1.5238792896270752\n",
                  "Epoch 107 train loss: 0.7912, eval loss 1.5244953632354736\n",
                  "Epoch 108 train loss: 0.8079, eval loss 1.5215983390808105\n",
                  "Epoch 109 train loss: 0.7230, eval loss 1.5212825536727905\n",
                  "Epoch 110 train loss: 0.8328, eval loss 1.520555019378662\n",
                  "Epoch 111 train loss: 0.8084, eval loss 1.5180774927139282\n",
                  "Epoch 112 train loss: 0.7672, eval loss 1.516634225845337\n",
                  "Epoch 113 train loss: 0.8631, eval loss 1.5179320573806763\n",
                  "Epoch 114 train loss: 0.7525, eval loss 1.514899492263794\n",
                  "Epoch 115 train loss: 0.8114, eval loss 1.5147885084152222\n",
                  "Epoch 116 train loss: 0.8295, eval loss 1.5140551328659058\n",
                  "Epoch 117 train loss: 0.7710, eval loss 1.5130500793457031\n",
                  "Epoch 118 train loss: 0.8163, eval loss 1.511544942855835\n",
                  "Epoch 119 train loss: 0.8303, eval loss 1.5101221799850464\n",
                  "Epoch 120 train loss: 0.8378, eval loss 1.5094550848007202\n",
                  "Epoch 121 train loss: 0.8062, eval loss 1.5077991485595703\n",
                  "Epoch 122 train loss: 0.7668, eval loss 1.5066280364990234\n",
                  "Epoch 123 train loss: 0.8203, eval loss 1.5075558423995972\n",
                  "Epoch 124 train loss: 0.7535, eval loss 1.5055338144302368\n",
                  "Epoch 125 train loss: 0.7941, eval loss 1.5052447319030762\n",
                  "Epoch 126 train loss: 0.8121, eval loss 1.5031434297561646\n",
                  "Epoch 127 train loss: 0.7861, eval loss 1.502203345298767\n",
                  "Epoch 128 train loss: 0.7552, eval loss 1.5020610094070435\n",
                  "Epoch 129 train loss: 0.7748, eval loss 1.5006917715072632\n",
                  "Epoch 130 train loss: 0.7732, eval loss 1.5003228187561035\n",
                  "Epoch 131 train loss: 0.6950, eval loss 1.4994763135910034\n",
                  "Epoch 132 train loss: 0.7609, eval loss 1.498957633972168\n",
                  "Epoch 133 train loss: 0.8019, eval loss 1.497849941253662\n",
                  "Epoch 134 train loss: 0.8179, eval loss 1.4957046508789062\n",
                  "Epoch 135 train loss: 0.8114, eval loss 1.4963505268096924\n",
                  "Epoch 136 train loss: 0.7744, eval loss 1.494387149810791\n",
                  "Epoch 137 train loss: 0.7565, eval loss 1.4949891567230225\n",
                  "Epoch 138 train loss: 0.8262, eval loss 1.4931880235671997\n",
                  "Epoch 139 train loss: 0.7932, eval loss 1.4925055503845215\n",
                  "Epoch 140 train loss: 0.7469, eval loss 1.4912575483322144\n",
                  "Epoch 141 train loss: 0.7427, eval loss 1.4916573762893677\n",
                  "Epoch 142 train loss: 0.7874, eval loss 1.490714192390442\n",
                  "Epoch 143 train loss: 0.7740, eval loss 1.489362359046936\n",
                  "Epoch 144 train loss: 0.7191, eval loss 1.48931884765625\n",
                  "Epoch 145 train loss: 0.7861, eval loss 1.488510251045227\n",
                  "Epoch 146 train loss: 0.7194, eval loss 1.4877020120620728\n",
                  "Epoch 147 train loss: 0.7402, eval loss 1.4874275922775269\n",
                  "Epoch 148 train loss: 0.7472, eval loss 1.4861613512039185\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:57:17,339] Trial 5 finished with value: 1.4849458932876587 and parameters: {'hidden_layers_size': 79, 'dropout_p': 0.3892221148571247, 'learning_rate': 5.602285591827152e-06, 'batch_size': 147, 'l2_reg': 8.793989913918417e-05}. Best is trial 5 with value: 1.4849458932876587.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.7450, eval loss 1.4849458932876587\n",
                  "Epoch 0 train loss: 0.8143, eval loss 1.4574309587478638\n",
                  "Epoch 1 train loss: 0.7465, eval loss 1.4399099349975586\n",
                  "Epoch 2 train loss: 0.8463, eval loss 1.4369920492172241\n",
                  "Epoch 3 train loss: 0.7390, eval loss 1.4350630044937134\n",
                  "Epoch 4 train loss: 0.7134, eval loss 1.432953119277954\n",
                  "Epoch 5 train loss: 0.7348, eval loss 1.4316658973693848\n",
                  "Epoch 6 train loss: 0.6799, eval loss 1.4294854402542114\n",
                  "Epoch 7 train loss: 0.6812, eval loss 1.4324995279312134\n",
                  "Epoch 8 train loss: 0.6901, eval loss 1.4308865070343018\n",
                  "Epoch 9 train loss: 0.6832, eval loss 1.4288564920425415\n",
                  "Epoch 10 train loss: 0.6842, eval loss 1.4295388460159302\n",
                  "Epoch 11 train loss: 0.6777, eval loss 1.4291431903839111\n",
                  "Epoch 12 train loss: 0.6471, eval loss 1.4294958114624023\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:57:24,055] Trial 6 finished with value: 1.4288564920425415 and parameters: {'hidden_layers_size': 57, 'dropout_p': 0.323525811047646, 'learning_rate': 0.001639888995675964, 'batch_size': 163, 'l2_reg': 0.00011608784665298614}. Best is trial 5 with value: 1.4849458932876587.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.9078, eval loss 1.5830994844436646\n",
                  "Epoch 1 train loss: 0.7911, eval loss 1.5127074718475342\n",
                  "Epoch 2 train loss: 0.7519, eval loss 1.483310580253601\n",
                  "Epoch 3 train loss: 0.7245, eval loss 1.4668145179748535\n",
                  "Epoch 4 train loss: 0.7093, eval loss 1.457176923751831\n",
                  "Epoch 5 train loss: 0.6778, eval loss 1.4505764245986938\n",
                  "Epoch 6 train loss: 0.6856, eval loss 1.4463560581207275\n",
                  "Epoch 7 train loss: 0.7100, eval loss 1.444113850593567\n",
                  "Epoch 8 train loss: 0.6831, eval loss 1.4424225091934204\n",
                  "Epoch 9 train loss: 0.6624, eval loss 1.4395655393600464\n",
                  "Epoch 10 train loss: 0.6884, eval loss 1.4384881258010864\n",
                  "Epoch 11 train loss: 0.6852, eval loss 1.4374748468399048\n",
                  "Epoch 12 train loss: 0.6543, eval loss 1.437628149986267\n",
                  "Epoch 13 train loss: 0.6698, eval loss 1.435391902923584\n",
                  "Epoch 14 train loss: 0.6418, eval loss 1.4346774816513062\n",
                  "Epoch 15 train loss: 0.6515, eval loss 1.4336631298065186\n",
                  "Epoch 16 train loss: 0.6790, eval loss 1.4340733289718628\n",
                  "Epoch 17 train loss: 0.6480, eval loss 1.4336438179016113\n",
                  "Epoch 18 train loss: 0.6213, eval loss 1.4334313869476318\n",
                  "Epoch 19 train loss: 0.6609, eval loss 1.4321469068527222\n",
                  "Epoch 20 train loss: 0.6459, eval loss 1.4307550191879272\n",
                  "Epoch 21 train loss: 0.6377, eval loss 1.4321523904800415\n",
                  "Epoch 22 train loss: 0.6326, eval loss 1.4326740503311157\n",
                  "Epoch 23 train loss: 0.6384, eval loss 1.4315388202667236\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:57:33,055] Trial 7 finished with value: 1.4307550191879272 and parameters: {'hidden_layers_size': 111, 'dropout_p': 0.25927807638886824, 'learning_rate': 0.00048809904371579404, 'batch_size': 386, 'l2_reg': 1.3953238064565572e-05}. Best is trial 5 with value: 1.4849458932876587.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7237, eval loss 1.4412562847137451\n",
                  "Epoch 1 train loss: 0.7140, eval loss 1.4383854866027832\n",
                  "Epoch 2 train loss: 0.6877, eval loss 1.4352675676345825\n",
                  "Epoch 3 train loss: 0.6937, eval loss 1.4340802431106567\n",
                  "Epoch 4 train loss: 0.7252, eval loss 1.4334564208984375\n",
                  "Epoch 5 train loss: 0.6693, eval loss 1.434956431388855\n",
                  "Epoch 6 train loss: 0.6778, eval loss 1.432584524154663\n",
                  "Epoch 7 train loss: 0.7323, eval loss 1.432875633239746\n",
                  "Epoch 8 train loss: 0.6877, eval loss 1.4320319890975952\n",
                  "Epoch 9 train loss: 0.6865, eval loss 1.4326467514038086\n",
                  "Epoch 10 train loss: 0.6779, eval loss 1.4330477714538574\n",
                  "Epoch 11 train loss: 0.7092, eval loss 1.4316794872283936\n",
                  "Epoch 12 train loss: 0.6684, eval loss 1.430398941040039\n",
                  "Epoch 13 train loss: 0.6363, eval loss 1.4295066595077515\n",
                  "Epoch 14 train loss: 0.6839, eval loss 1.430540680885315\n",
                  "Epoch 15 train loss: 0.6811, eval loss 1.4294984340667725\n",
                  "Epoch 16 train loss: 0.6713, eval loss 1.4299521446228027\n",
                  "Epoch 17 train loss: 0.6645, eval loss 1.4309502840042114\n",
                  "Epoch 18 train loss: 0.6465, eval loss 1.4300217628479004\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:57:40,810] Trial 8 finished with value: 1.4294984340667725 and parameters: {'hidden_layers_size': 193, 'dropout_p': 0.440294896991348, 'learning_rate': 0.0033072981741150294, 'batch_size': 410, 'l2_reg': 0.0002210739711032062}. Best is trial 5 with value: 1.4849458932876587.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.2573, eval loss 1.7127341032028198\n",
                  "Epoch 1 train loss: 1.2439, eval loss 1.7114677429199219\n",
                  "Epoch 2 train loss: 1.2134, eval loss 1.7111068964004517\n",
                  "Epoch 3 train loss: 1.2367, eval loss 1.7094391584396362\n",
                  "Epoch 4 train loss: 1.3360, eval loss 1.708016276359558\n",
                  "Epoch 5 train loss: 1.2274, eval loss 1.7086055278778076\n",
                  "Epoch 6 train loss: 1.2834, eval loss 1.7064292430877686\n",
                  "Epoch 7 train loss: 1.2744, eval loss 1.7046992778778076\n",
                  "Epoch 8 train loss: 1.2906, eval loss 1.7039823532104492\n",
                  "Epoch 9 train loss: 1.2446, eval loss 1.7031527757644653\n",
                  "Epoch 10 train loss: 1.2651, eval loss 1.703141450881958\n",
                  "Epoch 11 train loss: 1.2363, eval loss 1.7012020349502563\n",
                  "Epoch 12 train loss: 1.2014, eval loss 1.7008224725723267\n",
                  "Epoch 13 train loss: 1.1951, eval loss 1.6991922855377197\n",
                  "Epoch 14 train loss: 1.2006, eval loss 1.697727084159851\n",
                  "Epoch 15 train loss: 1.1852, eval loss 1.6971427202224731\n",
                  "Epoch 16 train loss: 1.2667, eval loss 1.6980986595153809\n",
                  "Epoch 17 train loss: 1.2789, eval loss 1.6946078538894653\n",
                  "Epoch 18 train loss: 1.2565, eval loss 1.6942541599273682\n",
                  "Epoch 19 train loss: 1.2020, eval loss 1.6937856674194336\n",
                  "Epoch 20 train loss: 1.2672, eval loss 1.6916321516036987\n",
                  "Epoch 21 train loss: 1.2883, eval loss 1.6912548542022705\n",
                  "Epoch 22 train loss: 1.2003, eval loss 1.6900503635406494\n",
                  "Epoch 23 train loss: 1.2183, eval loss 1.6901581287384033\n",
                  "Epoch 24 train loss: 1.1961, eval loss 1.6883022785186768\n",
                  "Epoch 25 train loss: 1.2392, eval loss 1.6889828443527222\n",
                  "Epoch 26 train loss: 1.1990, eval loss 1.6863712072372437\n",
                  "Epoch 27 train loss: 1.1891, eval loss 1.6858855485916138\n",
                  "Epoch 28 train loss: 1.2184, eval loss 1.6848256587982178\n",
                  "Epoch 29 train loss: 1.1868, eval loss 1.6835073232650757\n",
                  "Epoch 30 train loss: 1.2343, eval loss 1.6830910444259644\n",
                  "Epoch 31 train loss: 1.1730, eval loss 1.6823004484176636\n",
                  "Epoch 32 train loss: 1.1535, eval loss 1.6822729110717773\n",
                  "Epoch 33 train loss: 1.1543, eval loss 1.680031657218933\n",
                  "Epoch 34 train loss: 1.1667, eval loss 1.679181456565857\n",
                  "Epoch 35 train loss: 1.1970, eval loss 1.678755521774292\n",
                  "Epoch 36 train loss: 1.1376, eval loss 1.6770927906036377\n",
                  "Epoch 37 train loss: 1.3039, eval loss 1.6781227588653564\n",
                  "Epoch 38 train loss: 1.1740, eval loss 1.6772444248199463\n",
                  "Epoch 39 train loss: 1.2086, eval loss 1.6757200956344604\n",
                  "Epoch 40 train loss: 1.1579, eval loss 1.6749647855758667\n",
                  "Epoch 41 train loss: 1.1887, eval loss 1.6730399131774902\n",
                  "Epoch 42 train loss: 1.1676, eval loss 1.6712980270385742\n",
                  "Epoch 43 train loss: 1.1472, eval loss 1.6719939708709717\n",
                  "Epoch 44 train loss: 1.1521, eval loss 1.6699522733688354\n",
                  "Epoch 45 train loss: 1.1199, eval loss 1.6698888540267944\n",
                  "Epoch 46 train loss: 1.1308, eval loss 1.6687670946121216\n",
                  "Epoch 47 train loss: 1.1465, eval loss 1.6691055297851562\n",
                  "Epoch 48 train loss: 1.1536, eval loss 1.6668626070022583\n",
                  "Epoch 49 train loss: 1.1150, eval loss 1.6667970418930054\n",
                  "Epoch 50 train loss: 1.1539, eval loss 1.6661622524261475\n",
                  "Epoch 51 train loss: 1.1108, eval loss 1.6648163795471191\n",
                  "Epoch 52 train loss: 1.1670, eval loss 1.6634364128112793\n",
                  "Epoch 53 train loss: 1.1627, eval loss 1.6632362604141235\n",
                  "Epoch 54 train loss: 1.1358, eval loss 1.662529468536377\n",
                  "Epoch 55 train loss: 1.1651, eval loss 1.6618810892105103\n",
                  "Epoch 56 train loss: 1.1727, eval loss 1.6600793600082397\n",
                  "Epoch 57 train loss: 1.0864, eval loss 1.6594868898391724\n",
                  "Epoch 58 train loss: 1.1241, eval loss 1.65828537940979\n",
                  "Epoch 59 train loss: 1.1697, eval loss 1.6577577590942383\n",
                  "Epoch 60 train loss: 1.1775, eval loss 1.6564899682998657\n",
                  "Epoch 61 train loss: 1.1506, eval loss 1.6557564735412598\n",
                  "Epoch 62 train loss: 1.1314, eval loss 1.6546927690505981\n",
                  "Epoch 63 train loss: 1.1033, eval loss 1.6544049978256226\n",
                  "Epoch 64 train loss: 1.1252, eval loss 1.654241919517517\n",
                  "Epoch 65 train loss: 1.1310, eval loss 1.6533994674682617\n",
                  "Epoch 66 train loss: 1.0995, eval loss 1.6524527072906494\n",
                  "Epoch 67 train loss: 1.1207, eval loss 1.6521174907684326\n",
                  "Epoch 68 train loss: 1.0828, eval loss 1.6517925262451172\n",
                  "Epoch 69 train loss: 1.1109, eval loss 1.64961576461792\n",
                  "Epoch 70 train loss: 1.2157, eval loss 1.649580717086792\n",
                  "Epoch 71 train loss: 1.0763, eval loss 1.647810459136963\n",
                  "Epoch 72 train loss: 1.1042, eval loss 1.6486845016479492\n",
                  "Epoch 73 train loss: 1.1192, eval loss 1.6455904245376587\n",
                  "Epoch 74 train loss: 1.1290, eval loss 1.6486022472381592\n",
                  "Epoch 75 train loss: 1.1300, eval loss 1.6462247371673584\n",
                  "Epoch 76 train loss: 1.1268, eval loss 1.6452935934066772\n",
                  "Epoch 77 train loss: 1.1712, eval loss 1.6449203491210938\n",
                  "Epoch 78 train loss: 1.1116, eval loss 1.6433414220809937\n",
                  "Epoch 79 train loss: 1.0762, eval loss 1.6433172225952148\n",
                  "Epoch 80 train loss: 1.1024, eval loss 1.6405138969421387\n",
                  "Epoch 81 train loss: 1.1264, eval loss 1.642269253730774\n",
                  "Epoch 82 train loss: 1.1088, eval loss 1.6408114433288574\n",
                  "Epoch 83 train loss: 1.0793, eval loss 1.6392171382904053\n",
                  "Epoch 84 train loss: 1.0894, eval loss 1.638628602027893\n",
                  "Epoch 85 train loss: 1.1117, eval loss 1.6374461650848389\n",
                  "Epoch 86 train loss: 1.1054, eval loss 1.6357313394546509\n",
                  "Epoch 87 train loss: 1.0788, eval loss 1.6359785795211792\n",
                  "Epoch 88 train loss: 1.0430, eval loss 1.6363909244537354\n",
                  "Epoch 89 train loss: 1.0853, eval loss 1.635183572769165\n",
                  "Epoch 90 train loss: 1.1124, eval loss 1.636392593383789\n",
                  "Epoch 91 train loss: 1.0697, eval loss 1.6344074010849\n",
                  "Epoch 92 train loss: 1.0685, eval loss 1.63225257396698\n",
                  "Epoch 93 train loss: 1.0872, eval loss 1.63227117061615\n",
                  "Epoch 94 train loss: 1.0596, eval loss 1.6310067176818848\n",
                  "Epoch 95 train loss: 1.0519, eval loss 1.631244421005249\n",
                  "Epoch 96 train loss: 1.0821, eval loss 1.630739450454712\n",
                  "Epoch 97 train loss: 1.0983, eval loss 1.6312788724899292\n",
                  "Epoch 98 train loss: 1.0424, eval loss 1.6308141946792603\n",
                  "Epoch 99 train loss: 1.0700, eval loss 1.628419280052185\n",
                  "Epoch 100 train loss: 1.0599, eval loss 1.6292638778686523\n",
                  "Epoch 101 train loss: 1.0954, eval loss 1.6264334917068481\n",
                  "Epoch 102 train loss: 1.0413, eval loss 1.6257668733596802\n",
                  "Epoch 103 train loss: 1.0626, eval loss 1.6250560283660889\n",
                  "Epoch 104 train loss: 1.0062, eval loss 1.6257600784301758\n",
                  "Epoch 105 train loss: 1.0634, eval loss 1.6242082118988037\n",
                  "Epoch 106 train loss: 1.0660, eval loss 1.6246479749679565\n",
                  "Epoch 107 train loss: 1.0468, eval loss 1.6222857236862183\n",
                  "Epoch 108 train loss: 1.1021, eval loss 1.621734619140625\n",
                  "Epoch 109 train loss: 1.0083, eval loss 1.6214075088500977\n",
                  "Epoch 110 train loss: 1.0417, eval loss 1.620761752128601\n",
                  "Epoch 111 train loss: 1.0411, eval loss 1.622240662574768\n",
                  "Epoch 112 train loss: 1.0691, eval loss 1.6195447444915771\n",
                  "Epoch 113 train loss: 1.0380, eval loss 1.6204872131347656\n",
                  "Epoch 114 train loss: 1.0049, eval loss 1.619309663772583\n",
                  "Epoch 115 train loss: 1.0259, eval loss 1.6187726259231567\n",
                  "Epoch 116 train loss: 0.9812, eval loss 1.618636965751648\n",
                  "Epoch 117 train loss: 1.0577, eval loss 1.6174511909484863\n",
                  "Epoch 118 train loss: 1.0278, eval loss 1.616117000579834\n",
                  "Epoch 119 train loss: 0.9692, eval loss 1.6141386032104492\n",
                  "Epoch 120 train loss: 0.9962, eval loss 1.614225149154663\n",
                  "Epoch 121 train loss: 1.0027, eval loss 1.615375280380249\n",
                  "Epoch 122 train loss: 0.9894, eval loss 1.6148650646209717\n",
                  "Epoch 123 train loss: 1.0603, eval loss 1.6131560802459717\n",
                  "Epoch 124 train loss: 1.0224, eval loss 1.612517237663269\n",
                  "Epoch 125 train loss: 1.0152, eval loss 1.6116422414779663\n",
                  "Epoch 126 train loss: 0.9781, eval loss 1.6119147539138794\n",
                  "Epoch 127 train loss: 0.9922, eval loss 1.6096264123916626\n",
                  "Epoch 128 train loss: 0.9771, eval loss 1.6104730367660522\n",
                  "Epoch 129 train loss: 0.9939, eval loss 1.6092194318771362\n",
                  "Epoch 130 train loss: 1.0479, eval loss 1.6095181703567505\n",
                  "Epoch 131 train loss: 0.9901, eval loss 1.6076582670211792\n",
                  "Epoch 132 train loss: 1.0413, eval loss 1.6082632541656494\n",
                  "Epoch 133 train loss: 1.0260, eval loss 1.6075447797775269\n",
                  "Epoch 134 train loss: 0.9938, eval loss 1.6063977479934692\n",
                  "Epoch 135 train loss: 1.1350, eval loss 1.6066120862960815\n",
                  "Epoch 136 train loss: 0.9725, eval loss 1.6060224771499634\n",
                  "Epoch 137 train loss: 0.9985, eval loss 1.6050561666488647\n",
                  "Epoch 138 train loss: 1.0007, eval loss 1.6055294275283813\n",
                  "Epoch 139 train loss: 0.9577, eval loss 1.604835033416748\n",
                  "Epoch 140 train loss: 1.0405, eval loss 1.6038038730621338\n",
                  "Epoch 141 train loss: 1.0578, eval loss 1.6042132377624512\n",
                  "Epoch 142 train loss: 1.0197, eval loss 1.6029815673828125\n",
                  "Epoch 143 train loss: 0.9749, eval loss 1.6023802757263184\n",
                  "Epoch 144 train loss: 1.0207, eval loss 1.6012595891952515\n",
                  "Epoch 145 train loss: 0.9965, eval loss 1.6002548933029175\n",
                  "Epoch 146 train loss: 0.9534, eval loss 1.6002719402313232\n",
                  "Epoch 147 train loss: 0.9949, eval loss 1.6012160778045654\n",
                  "Epoch 148 train loss: 0.9891, eval loss 1.5988456010818481\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 11:58:55,850] Trial 9 finished with value: 1.5983518362045288 and parameters: {'hidden_layers_size': 61, 'dropout_p': 0.3335171955849856, 'learning_rate': 1.218558277703623e-06, 'batch_size': 149, 'l2_reg': 8.82343506258215e-05}. Best is trial 9 with value: 1.5983518362045288.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.9757, eval loss 1.5983518362045288\n",
                  "Epoch 0 train loss: 1.8167, eval loss 1.8361661434173584\n",
                  "Epoch 1 train loss: 1.7844, eval loss 1.8327094316482544\n",
                  "Epoch 2 train loss: 1.7676, eval loss 1.830435037612915\n",
                  "Epoch 3 train loss: 1.6944, eval loss 1.8292911052703857\n",
                  "Epoch 4 train loss: 1.7405, eval loss 1.8237731456756592\n",
                  "Epoch 5 train loss: 1.7374, eval loss 1.8194588422775269\n",
                  "Epoch 6 train loss: 1.7057, eval loss 1.8185237646102905\n",
                  "Epoch 7 train loss: 1.6584, eval loss 1.8168212175369263\n",
                  "Epoch 8 train loss: 1.7315, eval loss 1.8118305206298828\n",
                  "Epoch 9 train loss: 1.6620, eval loss 1.8092132806777954\n",
                  "Epoch 10 train loss: 1.6265, eval loss 1.8066412210464478\n",
                  "Epoch 11 train loss: 1.6185, eval loss 1.8041006326675415\n",
                  "Epoch 12 train loss: 1.6677, eval loss 1.8003767728805542\n",
                  "Epoch 13 train loss: 1.6482, eval loss 1.8005175590515137\n",
                  "Epoch 14 train loss: 1.6992, eval loss 1.794893741607666\n",
                  "Epoch 15 train loss: 1.5990, eval loss 1.7939708232879639\n",
                  "Epoch 16 train loss: 1.5770, eval loss 1.7887749671936035\n",
                  "Epoch 17 train loss: 1.5592, eval loss 1.7892825603485107\n",
                  "Epoch 18 train loss: 1.5912, eval loss 1.7856582403182983\n",
                  "Epoch 19 train loss: 1.5880, eval loss 1.7808547019958496\n",
                  "Epoch 20 train loss: 1.5992, eval loss 1.7782596349716187\n",
                  "Epoch 21 train loss: 1.5902, eval loss 1.780239462852478\n",
                  "Epoch 22 train loss: 1.5704, eval loss 1.7746174335479736\n",
                  "Epoch 23 train loss: 1.4924, eval loss 1.7748056650161743\n",
                  "Epoch 24 train loss: 1.5439, eval loss 1.770710825920105\n",
                  "Epoch 25 train loss: 1.5738, eval loss 1.7679671049118042\n",
                  "Epoch 26 train loss: 1.5955, eval loss 1.7678672075271606\n",
                  "Epoch 27 train loss: 1.5668, eval loss 1.765138030052185\n",
                  "Epoch 28 train loss: 1.5277, eval loss 1.762365460395813\n",
                  "Epoch 29 train loss: 1.5569, eval loss 1.759416103363037\n",
                  "Epoch 30 train loss: 1.5503, eval loss 1.7556071281433105\n",
                  "Epoch 31 train loss: 1.5986, eval loss 1.7543643712997437\n",
                  "Epoch 32 train loss: 1.5532, eval loss 1.7513326406478882\n",
                  "Epoch 33 train loss: 1.4848, eval loss 1.7504955530166626\n",
                  "Epoch 34 train loss: 1.4539, eval loss 1.7484025955200195\n",
                  "Epoch 35 train loss: 1.5096, eval loss 1.7461527585983276\n",
                  "Epoch 36 train loss: 1.4404, eval loss 1.7442482709884644\n",
                  "Epoch 37 train loss: 1.4359, eval loss 1.7435311079025269\n",
                  "Epoch 38 train loss: 1.4028, eval loss 1.7410613298416138\n",
                  "Epoch 39 train loss: 1.4188, eval loss 1.737724781036377\n",
                  "Epoch 40 train loss: 1.4885, eval loss 1.7356441020965576\n",
                  "Epoch 41 train loss: 1.4414, eval loss 1.7356783151626587\n",
                  "Epoch 42 train loss: 1.3784, eval loss 1.7338496446609497\n",
                  "Epoch 43 train loss: 1.4270, eval loss 1.731167197227478\n",
                  "Epoch 44 train loss: 1.4527, eval loss 1.7274222373962402\n",
                  "Epoch 45 train loss: 1.4065, eval loss 1.7272014617919922\n",
                  "Epoch 46 train loss: 1.4510, eval loss 1.7263669967651367\n",
                  "Epoch 47 train loss: 1.3804, eval loss 1.7238377332687378\n",
                  "Epoch 48 train loss: 1.4424, eval loss 1.7192516326904297\n",
                  "Epoch 49 train loss: 1.3665, eval loss 1.7195457220077515\n",
                  "Epoch 50 train loss: 1.4120, eval loss 1.7166410684585571\n",
                  "Epoch 51 train loss: 1.4013, eval loss 1.7153890132904053\n",
                  "Epoch 52 train loss: 1.3654, eval loss 1.714573860168457\n",
                  "Epoch 53 train loss: 1.3835, eval loss 1.7115358114242554\n",
                  "Epoch 54 train loss: 1.3523, eval loss 1.709256887435913\n",
                  "Epoch 55 train loss: 1.3002, eval loss 1.7084074020385742\n",
                  "Epoch 56 train loss: 1.3150, eval loss 1.7093403339385986\n",
                  "Epoch 57 train loss: 1.3710, eval loss 1.7056174278259277\n",
                  "Epoch 58 train loss: 1.3182, eval loss 1.7044296264648438\n",
                  "Epoch 59 train loss: 1.3150, eval loss 1.7034944295883179\n",
                  "Epoch 60 train loss: 1.3808, eval loss 1.7002818584442139\n",
                  "Epoch 61 train loss: 1.3191, eval loss 1.7014824151992798\n",
                  "Epoch 62 train loss: 1.3233, eval loss 1.701650619506836\n",
                  "Epoch 63 train loss: 1.3115, eval loss 1.6963359117507935\n",
                  "Epoch 64 train loss: 1.2693, eval loss 1.693961262702942\n",
                  "Epoch 65 train loss: 1.2999, eval loss 1.6926281452178955\n",
                  "Epoch 66 train loss: 1.2821, eval loss 1.6928900480270386\n",
                  "Epoch 67 train loss: 1.3464, eval loss 1.688643217086792\n",
                  "Epoch 68 train loss: 1.3034, eval loss 1.6883078813552856\n",
                  "Epoch 69 train loss: 1.2987, eval loss 1.6882754564285278\n",
                  "Epoch 70 train loss: 1.2766, eval loss 1.6853604316711426\n",
                  "Epoch 71 train loss: 1.3768, eval loss 1.6848113536834717\n",
                  "Epoch 72 train loss: 1.2809, eval loss 1.682633876800537\n",
                  "Epoch 73 train loss: 1.2520, eval loss 1.6817876100540161\n",
                  "Epoch 74 train loss: 1.2735, eval loss 1.6785295009613037\n",
                  "Epoch 75 train loss: 1.1580, eval loss 1.6800874471664429\n",
                  "Epoch 76 train loss: 1.2212, eval loss 1.677596926689148\n",
                  "Epoch 77 train loss: 1.2178, eval loss 1.6764699220657349\n",
                  "Epoch 78 train loss: 1.2480, eval loss 1.6769782304763794\n",
                  "Epoch 79 train loss: 1.2250, eval loss 1.6742610931396484\n",
                  "Epoch 80 train loss: 1.2664, eval loss 1.6722652912139893\n",
                  "Epoch 81 train loss: 1.2598, eval loss 1.6725414991378784\n",
                  "Epoch 82 train loss: 1.2322, eval loss 1.6706420183181763\n",
                  "Epoch 83 train loss: 1.2019, eval loss 1.6691941022872925\n",
                  "Epoch 84 train loss: 1.2151, eval loss 1.6666035652160645\n",
                  "Epoch 85 train loss: 1.1985, eval loss 1.6657944917678833\n",
                  "Epoch 86 train loss: 1.2919, eval loss 1.664588212966919\n",
                  "Epoch 87 train loss: 1.1901, eval loss 1.6638559103012085\n",
                  "Epoch 88 train loss: 1.2675, eval loss 1.662807822227478\n",
                  "Epoch 89 train loss: 1.2180, eval loss 1.660475254058838\n",
                  "Epoch 90 train loss: 1.2410, eval loss 1.6623029708862305\n",
                  "Epoch 91 train loss: 1.1824, eval loss 1.6574316024780273\n",
                  "Epoch 92 train loss: 1.1974, eval loss 1.6583259105682373\n",
                  "Epoch 93 train loss: 1.1529, eval loss 1.656527042388916\n",
                  "Epoch 94 train loss: 1.1332, eval loss 1.6543810367584229\n",
                  "Epoch 95 train loss: 1.1658, eval loss 1.655651330947876\n",
                  "Epoch 96 train loss: 1.1706, eval loss 1.6528383493423462\n",
                  "Epoch 97 train loss: 1.2057, eval loss 1.651404619216919\n",
                  "Epoch 98 train loss: 1.1169, eval loss 1.6532381772994995\n",
                  "Epoch 99 train loss: 1.1894, eval loss 1.6503890752792358\n",
                  "Epoch 100 train loss: 1.1885, eval loss 1.6492618322372437\n",
                  "Epoch 101 train loss: 1.1665, eval loss 1.6470609903335571\n",
                  "Epoch 102 train loss: 1.1515, eval loss 1.6460553407669067\n",
                  "Epoch 103 train loss: 1.1720, eval loss 1.646386742591858\n",
                  "Epoch 104 train loss: 1.1217, eval loss 1.6456307172775269\n",
                  "Epoch 105 train loss: 1.1220, eval loss 1.6435588598251343\n",
                  "Epoch 106 train loss: 1.1492, eval loss 1.6429164409637451\n",
                  "Epoch 107 train loss: 1.0601, eval loss 1.6405631303787231\n",
                  "Epoch 108 train loss: 1.1053, eval loss 1.640823483467102\n",
                  "Epoch 109 train loss: 1.0905, eval loss 1.6399883031845093\n",
                  "Epoch 110 train loss: 1.1739, eval loss 1.6395364999771118\n",
                  "Epoch 111 train loss: 1.1071, eval loss 1.6388978958129883\n",
                  "Epoch 112 train loss: 1.1178, eval loss 1.6377876996994019\n",
                  "Epoch 113 train loss: 1.2030, eval loss 1.6358476877212524\n",
                  "Epoch 114 train loss: 1.0995, eval loss 1.6359442472457886\n",
                  "Epoch 115 train loss: 1.0798, eval loss 1.6344937086105347\n",
                  "Epoch 116 train loss: 1.1434, eval loss 1.6334943771362305\n",
                  "Epoch 117 train loss: 1.1110, eval loss 1.6306986808776855\n",
                  "Epoch 118 train loss: 1.0909, eval loss 1.629085659980774\n",
                  "Epoch 119 train loss: 1.1372, eval loss 1.6292608976364136\n",
                  "Epoch 120 train loss: 1.1303, eval loss 1.6294704675674438\n",
                  "Epoch 121 train loss: 1.0919, eval loss 1.6268686056137085\n",
                  "Epoch 122 train loss: 1.0932, eval loss 1.627995252609253\n",
                  "Epoch 123 train loss: 1.0845, eval loss 1.62576425075531\n",
                  "Epoch 124 train loss: 1.1455, eval loss 1.6260583400726318\n",
                  "Epoch 125 train loss: 1.1305, eval loss 1.6248648166656494\n",
                  "Epoch 126 train loss: 1.0633, eval loss 1.6250674724578857\n",
                  "Epoch 127 train loss: 1.0983, eval loss 1.6210755109786987\n",
                  "Epoch 128 train loss: 1.0554, eval loss 1.6214746236801147\n",
                  "Epoch 129 train loss: 1.0907, eval loss 1.6213020086288452\n",
                  "Epoch 130 train loss: 1.1121, eval loss 1.620283842086792\n",
                  "Epoch 131 train loss: 1.0395, eval loss 1.6196165084838867\n",
                  "Epoch 132 train loss: 1.0937, eval loss 1.618436574935913\n",
                  "Epoch 133 train loss: 1.0872, eval loss 1.6167919635772705\n",
                  "Epoch 134 train loss: 1.0055, eval loss 1.6163592338562012\n",
                  "Epoch 135 train loss: 1.0671, eval loss 1.615475058555603\n",
                  "Epoch 136 train loss: 1.0415, eval loss 1.6152039766311646\n",
                  "Epoch 137 train loss: 1.0718, eval loss 1.6131404638290405\n",
                  "Epoch 138 train loss: 1.0288, eval loss 1.6098557710647583\n",
                  "Epoch 139 train loss: 1.0318, eval loss 1.6122326850891113\n",
                  "Epoch 140 train loss: 1.0433, eval loss 1.611917495727539\n",
                  "Epoch 141 train loss: 1.0673, eval loss 1.6102412939071655\n",
                  "Epoch 142 train loss: 1.0582, eval loss 1.6084859371185303\n",
                  "Epoch 143 train loss: 1.0590, eval loss 1.6083885431289673\n",
                  "Epoch 144 train loss: 1.0507, eval loss 1.6067304611206055\n",
                  "Epoch 145 train loss: 0.9950, eval loss 1.6062841415405273\n",
                  "Epoch 146 train loss: 1.0280, eval loss 1.604134202003479\n",
                  "Epoch 147 train loss: 1.0157, eval loss 1.6054487228393555\n",
                  "Epoch 148 train loss: 1.0253, eval loss 1.6052470207214355\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:00:19,939] Trial 10 finished with value: 1.604019045829773 and parameters: {'hidden_layers_size': 74, 'dropout_p': 0.33136891009652225, 'learning_rate': 2.22717072173014e-06, 'batch_size': 128, 'l2_reg': 0.00012254471855770472}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 1.0482, eval loss 1.604019045829773\n",
                  "Epoch 0 train loss: 1.5965, eval loss 1.8047709465026855\n",
                  "Epoch 1 train loss: 1.5960, eval loss 1.7960457801818848\n",
                  "Epoch 2 train loss: 1.6215, eval loss 1.7882399559020996\n",
                  "Epoch 3 train loss: 1.5327, eval loss 1.7791756391525269\n",
                  "Epoch 4 train loss: 1.5479, eval loss 1.7713788747787476\n",
                  "Epoch 5 train loss: 1.4365, eval loss 1.7631067037582397\n",
                  "Epoch 6 train loss: 1.4975, eval loss 1.7570745944976807\n",
                  "Epoch 7 train loss: 1.5370, eval loss 1.7496657371520996\n",
                  "Epoch 8 train loss: 1.4846, eval loss 1.7437385320663452\n",
                  "Epoch 9 train loss: 1.3625, eval loss 1.7370630502700806\n",
                  "Epoch 10 train loss: 1.4339, eval loss 1.7307111024856567\n",
                  "Epoch 11 train loss: 1.4143, eval loss 1.7250785827636719\n",
                  "Epoch 12 train loss: 1.3309, eval loss 1.7185893058776855\n",
                  "Epoch 13 train loss: 1.3115, eval loss 1.7132006883621216\n",
                  "Epoch 14 train loss: 1.3895, eval loss 1.70828378200531\n",
                  "Epoch 15 train loss: 1.2887, eval loss 1.7028430700302124\n",
                  "Epoch 16 train loss: 1.3049, eval loss 1.6988258361816406\n",
                  "Epoch 17 train loss: 1.2574, eval loss 1.6921696662902832\n",
                  "Epoch 18 train loss: 1.2975, eval loss 1.6889865398406982\n",
                  "Epoch 19 train loss: 1.2391, eval loss 1.6834865808486938\n",
                  "Epoch 20 train loss: 1.2508, eval loss 1.6806379556655884\n",
                  "Epoch 21 train loss: 1.2860, eval loss 1.6763546466827393\n",
                  "Epoch 22 train loss: 1.2057, eval loss 1.6725358963012695\n",
                  "Epoch 23 train loss: 1.2217, eval loss 1.667621374130249\n",
                  "Epoch 24 train loss: 1.2402, eval loss 1.664218783378601\n",
                  "Epoch 25 train loss: 1.1415, eval loss 1.6594243049621582\n",
                  "Epoch 26 train loss: 1.1674, eval loss 1.6565260887145996\n",
                  "Epoch 27 train loss: 1.1395, eval loss 1.6535793542861938\n",
                  "Epoch 28 train loss: 1.1247, eval loss 1.6499924659729004\n",
                  "Epoch 29 train loss: 1.1655, eval loss 1.6471662521362305\n",
                  "Epoch 30 train loss: 1.1226, eval loss 1.6434201002120972\n",
                  "Epoch 31 train loss: 1.1107, eval loss 1.6403374671936035\n",
                  "Epoch 32 train loss: 1.1060, eval loss 1.636975884437561\n",
                  "Epoch 33 train loss: 1.0816, eval loss 1.6342321634292603\n",
                  "Epoch 34 train loss: 1.0822, eval loss 1.6304585933685303\n",
                  "Epoch 35 train loss: 1.0553, eval loss 1.6290152072906494\n",
                  "Epoch 36 train loss: 1.1426, eval loss 1.62554132938385\n",
                  "Epoch 37 train loss: 1.0232, eval loss 1.6226866245269775\n",
                  "Epoch 38 train loss: 1.0722, eval loss 1.6213773488998413\n",
                  "Epoch 39 train loss: 1.0195, eval loss 1.616672396659851\n",
                  "Epoch 40 train loss: 1.0296, eval loss 1.6145416498184204\n",
                  "Epoch 41 train loss: 1.0548, eval loss 1.6120898723602295\n",
                  "Epoch 42 train loss: 1.0686, eval loss 1.6108522415161133\n",
                  "Epoch 43 train loss: 1.0081, eval loss 1.6067380905151367\n",
                  "Epoch 44 train loss: 1.0434, eval loss 1.6042635440826416\n",
                  "Epoch 45 train loss: 1.0499, eval loss 1.6031956672668457\n",
                  "Epoch 46 train loss: 1.0106, eval loss 1.6000382900238037\n",
                  "Epoch 47 train loss: 1.0246, eval loss 1.5995795726776123\n",
                  "Epoch 48 train loss: 0.9890, eval loss 1.5955445766448975\n",
                  "Epoch 49 train loss: 1.0017, eval loss 1.5941462516784668\n",
                  "Epoch 50 train loss: 0.9861, eval loss 1.592423915863037\n",
                  "Epoch 51 train loss: 1.0414, eval loss 1.5899314880371094\n",
                  "Epoch 52 train loss: 0.9335, eval loss 1.589928150177002\n",
                  "Epoch 53 train loss: 1.0130, eval loss 1.586998462677002\n",
                  "Epoch 54 train loss: 0.9588, eval loss 1.5848557949066162\n",
                  "Epoch 55 train loss: 0.9901, eval loss 1.582783818244934\n",
                  "Epoch 56 train loss: 0.9870, eval loss 1.5803838968276978\n",
                  "Epoch 57 train loss: 0.9849, eval loss 1.5785179138183594\n",
                  "Epoch 58 train loss: 0.9274, eval loss 1.577549934387207\n",
                  "Epoch 59 train loss: 0.9801, eval loss 1.575279712677002\n",
                  "Epoch 60 train loss: 0.9249, eval loss 1.5731289386749268\n",
                  "Epoch 61 train loss: 0.8829, eval loss 1.5721896886825562\n",
                  "Epoch 62 train loss: 0.9142, eval loss 1.570094347000122\n",
                  "Epoch 63 train loss: 0.9219, eval loss 1.5692296028137207\n",
                  "Epoch 64 train loss: 0.9203, eval loss 1.5671964883804321\n",
                  "Epoch 65 train loss: 0.9123, eval loss 1.5653611421585083\n",
                  "Epoch 66 train loss: 0.9540, eval loss 1.564013957977295\n",
                  "Epoch 67 train loss: 0.8975, eval loss 1.5626975297927856\n",
                  "Epoch 68 train loss: 0.9339, eval loss 1.5608612298965454\n",
                  "Epoch 69 train loss: 0.9343, eval loss 1.5585311651229858\n",
                  "Epoch 70 train loss: 0.9428, eval loss 1.5577354431152344\n",
                  "Epoch 71 train loss: 0.9228, eval loss 1.55620539188385\n",
                  "Epoch 72 train loss: 0.9244, eval loss 1.5548557043075562\n",
                  "Epoch 73 train loss: 0.8825, eval loss 1.553346037864685\n",
                  "Epoch 74 train loss: 0.9163, eval loss 1.5535314083099365\n",
                  "Epoch 75 train loss: 0.8898, eval loss 1.5510737895965576\n",
                  "Epoch 76 train loss: 0.8985, eval loss 1.5494123697280884\n",
                  "Epoch 77 train loss: 0.9276, eval loss 1.5476759672164917\n",
                  "Epoch 78 train loss: 0.9054, eval loss 1.5469826459884644\n",
                  "Epoch 79 train loss: 0.8706, eval loss 1.545756220817566\n",
                  "Epoch 80 train loss: 0.9168, eval loss 1.5445961952209473\n",
                  "Epoch 81 train loss: 0.8355, eval loss 1.543640375137329\n",
                  "Epoch 82 train loss: 0.8943, eval loss 1.5411274433135986\n",
                  "Epoch 83 train loss: 0.8745, eval loss 1.5404229164123535\n",
                  "Epoch 84 train loss: 0.8580, eval loss 1.5393719673156738\n",
                  "Epoch 85 train loss: 0.9224, eval loss 1.5378347635269165\n",
                  "Epoch 86 train loss: 0.8567, eval loss 1.5370351076126099\n",
                  "Epoch 87 train loss: 0.8412, eval loss 1.5364819765090942\n",
                  "Epoch 88 train loss: 0.8406, eval loss 1.5342960357666016\n",
                  "Epoch 89 train loss: 0.8221, eval loss 1.53327476978302\n",
                  "Epoch 90 train loss: 0.8710, eval loss 1.5325006246566772\n",
                  "Epoch 91 train loss: 0.8729, eval loss 1.531151294708252\n",
                  "Epoch 92 train loss: 0.8620, eval loss 1.53004789352417\n",
                  "Epoch 93 train loss: 0.8602, eval loss 1.5298278331756592\n",
                  "Epoch 94 train loss: 0.7901, eval loss 1.5281070470809937\n",
                  "Epoch 95 train loss: 0.8277, eval loss 1.5262644290924072\n",
                  "Epoch 96 train loss: 0.8530, eval loss 1.5257666110992432\n",
                  "Epoch 97 train loss: 0.8260, eval loss 1.5251047611236572\n",
                  "Epoch 98 train loss: 0.8638, eval loss 1.523867130279541\n",
                  "Epoch 99 train loss: 0.8353, eval loss 1.5237336158752441\n",
                  "Epoch 100 train loss: 0.8196, eval loss 1.5221107006072998\n",
                  "Epoch 101 train loss: 0.8204, eval loss 1.5206139087677002\n",
                  "Epoch 102 train loss: 0.8353, eval loss 1.5200250148773193\n",
                  "Epoch 103 train loss: 0.8151, eval loss 1.5181622505187988\n",
                  "Epoch 104 train loss: 0.8102, eval loss 1.5182572603225708\n",
                  "Epoch 105 train loss: 0.8369, eval loss 1.5169703960418701\n",
                  "Epoch 106 train loss: 0.8126, eval loss 1.5151573419570923\n",
                  "Epoch 107 train loss: 0.8456, eval loss 1.5151909589767456\n",
                  "Epoch 108 train loss: 0.8110, eval loss 1.5144709348678589\n",
                  "Epoch 109 train loss: 0.7934, eval loss 1.5136274099349976\n",
                  "Epoch 110 train loss: 0.7934, eval loss 1.5122603178024292\n",
                  "Epoch 111 train loss: 0.7972, eval loss 1.5123575925827026\n",
                  "Epoch 112 train loss: 0.8203, eval loss 1.5116873979568481\n",
                  "Epoch 113 train loss: 0.7677, eval loss 1.5104711055755615\n",
                  "Epoch 114 train loss: 0.8113, eval loss 1.5098942518234253\n",
                  "Epoch 115 train loss: 0.8513, eval loss 1.5091516971588135\n",
                  "Epoch 116 train loss: 0.7929, eval loss 1.5085151195526123\n",
                  "Epoch 117 train loss: 0.7932, eval loss 1.507208228111267\n",
                  "Epoch 118 train loss: 0.8333, eval loss 1.5065340995788574\n",
                  "Epoch 119 train loss: 0.8069, eval loss 1.5062295198440552\n",
                  "Epoch 120 train loss: 0.8255, eval loss 1.5047645568847656\n",
                  "Epoch 121 train loss: 0.7454, eval loss 1.5045095682144165\n",
                  "Epoch 122 train loss: 0.8307, eval loss 1.5034726858139038\n",
                  "Epoch 123 train loss: 0.7626, eval loss 1.502123236656189\n",
                  "Epoch 124 train loss: 0.7675, eval loss 1.5026004314422607\n",
                  "Epoch 125 train loss: 0.7502, eval loss 1.5015901327133179\n",
                  "Epoch 126 train loss: 0.8253, eval loss 1.4990310668945312\n",
                  "Epoch 127 train loss: 0.7506, eval loss 1.5000358819961548\n",
                  "Epoch 128 train loss: 0.8028, eval loss 1.4982502460479736\n",
                  "Epoch 129 train loss: 0.7462, eval loss 1.4983563423156738\n",
                  "Epoch 130 train loss: 0.7609, eval loss 1.4980846643447876\n",
                  "Epoch 131 train loss: 0.7682, eval loss 1.4970035552978516\n",
                  "Epoch 132 train loss: 0.7523, eval loss 1.4964118003845215\n",
                  "Epoch 133 train loss: 0.7890, eval loss 1.49569833278656\n",
                  "Epoch 134 train loss: 0.7907, eval loss 1.4950915575027466\n",
                  "Epoch 135 train loss: 0.8370, eval loss 1.4946186542510986\n",
                  "Epoch 136 train loss: 0.7696, eval loss 1.4941221475601196\n",
                  "Epoch 137 train loss: 0.7949, eval loss 1.4935473203659058\n",
                  "Epoch 138 train loss: 0.7785, eval loss 1.492287278175354\n",
                  "Epoch 139 train loss: 0.7720, eval loss 1.492241621017456\n",
                  "Epoch 140 train loss: 0.7756, eval loss 1.4914593696594238\n",
                  "Epoch 141 train loss: 0.7802, eval loss 1.4913334846496582\n",
                  "Epoch 142 train loss: 0.8022, eval loss 1.4902968406677246\n",
                  "Epoch 143 train loss: 0.8101, eval loss 1.4895777702331543\n",
                  "Epoch 144 train loss: 0.7797, eval loss 1.4889781475067139\n",
                  "Epoch 145 train loss: 0.7918, eval loss 1.4883965253829956\n",
                  "Epoch 146 train loss: 0.7184, eval loss 1.48777174949646\n",
                  "Epoch 147 train loss: 0.7981, eval loss 1.4876115322113037\n",
                  "Epoch 148 train loss: 0.7108, eval loss 1.487038016319275\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:01:23,149] Trial 11 finished with value: 1.4864838123321533 and parameters: {'hidden_layers_size': 213, 'dropout_p': 0.36216709088446125, 'learning_rate': 5.2940213719974975e-06, 'batch_size': 383, 'l2_reg': 3.4424603013924284e-05}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.7635, eval loss 1.4864838123321533\n",
                  "Epoch 0 train loss: 0.8362, eval loss 1.4302172660827637\n",
                  "Epoch 1 train loss: 0.7828, eval loss 1.4340970516204834\n",
                  "Epoch 2 train loss: 0.7205, eval loss 1.4297698736190796\n",
                  "Epoch 3 train loss: 0.7146, eval loss 1.4280736446380615\n",
                  "Epoch 4 train loss: 0.6643, eval loss 1.4266185760498047\n",
                  "Epoch 5 train loss: 0.6408, eval loss 1.4290642738342285\n",
                  "Epoch 6 train loss: 0.6785, eval loss 1.4312639236450195\n",
                  "Epoch 7 train loss: 0.6536, eval loss 1.430992841720581\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:01:27,346] Trial 12 finished with value: 1.4266185760498047 and parameters: {'hidden_layers_size': 144, 'dropout_p': 0.3112749295395019, 'learning_rate': 0.022921845040939203, 'batch_size': 225, 'l2_reg': 0.00022311867281809498}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7834, eval loss 1.4314632415771484\n",
                  "Epoch 1 train loss: 0.8209, eval loss 1.4325178861618042\n",
                  "Epoch 2 train loss: 0.7661, eval loss 1.4285625219345093\n",
                  "Epoch 3 train loss: 0.6919, eval loss 1.4291861057281494\n",
                  "Epoch 4 train loss: 0.7505, eval loss 1.4273971319198608\n",
                  "Epoch 5 train loss: 0.7207, eval loss 1.4258660078048706\n",
                  "Epoch 6 train loss: 0.7354, eval loss 1.4294683933258057\n",
                  "Epoch 7 train loss: 0.7394, eval loss 1.4301553964614868\n",
                  "Epoch 8 train loss: 0.7040, eval loss 1.4268364906311035\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:01:32,700] Trial 13 finished with value: 1.4258660078048706 and parameters: {'hidden_layers_size': 129, 'dropout_p': 0.4287796405673766, 'learning_rate': 0.009527087967219797, 'batch_size': 162, 'l2_reg': 0.00013634559328734786}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.5125, eval loss 1.7800815105438232\n",
                  "Epoch 1 train loss: 1.4650, eval loss 1.7722440958023071\n",
                  "Epoch 2 train loss: 1.4603, eval loss 1.7623013257980347\n",
                  "Epoch 3 train loss: 1.3826, eval loss 1.7550312280654907\n",
                  "Epoch 4 train loss: 1.3752, eval loss 1.7458875179290771\n",
                  "Epoch 5 train loss: 1.3789, eval loss 1.7401067018508911\n",
                  "Epoch 6 train loss: 1.3527, eval loss 1.7303485870361328\n",
                  "Epoch 7 train loss: 1.2970, eval loss 1.7240208387374878\n",
                  "Epoch 8 train loss: 1.3195, eval loss 1.7164167165756226\n",
                  "Epoch 9 train loss: 1.2668, eval loss 1.7097499370574951\n",
                  "Epoch 10 train loss: 1.2803, eval loss 1.7041105031967163\n",
                  "Epoch 11 train loss: 1.2535, eval loss 1.6983213424682617\n",
                  "Epoch 12 train loss: 1.2369, eval loss 1.6921731233596802\n",
                  "Epoch 13 train loss: 1.1712, eval loss 1.6870054006576538\n",
                  "Epoch 14 train loss: 1.1953, eval loss 1.6814374923706055\n",
                  "Epoch 15 train loss: 1.1834, eval loss 1.6752749681472778\n",
                  "Epoch 16 train loss: 1.2173, eval loss 1.671687364578247\n",
                  "Epoch 17 train loss: 1.1439, eval loss 1.6675511598587036\n",
                  "Epoch 18 train loss: 1.1791, eval loss 1.6624234914779663\n",
                  "Epoch 19 train loss: 1.1203, eval loss 1.6578192710876465\n",
                  "Epoch 20 train loss: 1.1026, eval loss 1.6535190343856812\n",
                  "Epoch 21 train loss: 1.1050, eval loss 1.6493021249771118\n",
                  "Epoch 22 train loss: 1.1357, eval loss 1.6461398601531982\n",
                  "Epoch 23 train loss: 1.1262, eval loss 1.6416172981262207\n",
                  "Epoch 24 train loss: 1.0998, eval loss 1.6383745670318604\n",
                  "Epoch 25 train loss: 1.0735, eval loss 1.6342514753341675\n",
                  "Epoch 26 train loss: 1.1066, eval loss 1.6302067041397095\n",
                  "Epoch 27 train loss: 1.0575, eval loss 1.626824975013733\n",
                  "Epoch 28 train loss: 1.0483, eval loss 1.6240290403366089\n",
                  "Epoch 29 train loss: 1.0357, eval loss 1.6227538585662842\n",
                  "Epoch 30 train loss: 1.0517, eval loss 1.6183987855911255\n",
                  "Epoch 31 train loss: 1.0183, eval loss 1.6140269041061401\n",
                  "Epoch 32 train loss: 1.0277, eval loss 1.6114708185195923\n",
                  "Epoch 33 train loss: 1.0280, eval loss 1.6083341836929321\n",
                  "Epoch 34 train loss: 0.9783, eval loss 1.6061614751815796\n",
                  "Epoch 35 train loss: 1.0112, eval loss 1.6030172109603882\n",
                  "Epoch 36 train loss: 1.0002, eval loss 1.60051429271698\n",
                  "Epoch 37 train loss: 0.9863, eval loss 1.5956573486328125\n",
                  "Epoch 38 train loss: 1.0179, eval loss 1.5948337316513062\n",
                  "Epoch 39 train loss: 0.9464, eval loss 1.592589020729065\n",
                  "Epoch 40 train loss: 0.9999, eval loss 1.5894140005111694\n",
                  "Epoch 41 train loss: 0.9770, eval loss 1.5879014730453491\n",
                  "Epoch 42 train loss: 0.9273, eval loss 1.584403157234192\n",
                  "Epoch 43 train loss: 0.9468, eval loss 1.5825114250183105\n",
                  "Epoch 44 train loss: 0.9526, eval loss 1.5788646936416626\n",
                  "Epoch 45 train loss: 0.9328, eval loss 1.5771986246109009\n",
                  "Epoch 46 train loss: 0.9124, eval loss 1.5750328302383423\n",
                  "Epoch 47 train loss: 0.9070, eval loss 1.5746159553527832\n",
                  "Epoch 48 train loss: 0.9006, eval loss 1.5713592767715454\n",
                  "Epoch 49 train loss: 0.9223, eval loss 1.5685172080993652\n",
                  "Epoch 50 train loss: 0.9048, eval loss 1.566699743270874\n",
                  "Epoch 51 train loss: 0.9317, eval loss 1.5636540651321411\n",
                  "Epoch 52 train loss: 0.8862, eval loss 1.5620590448379517\n",
                  "Epoch 53 train loss: 0.9201, eval loss 1.5617526769638062\n",
                  "Epoch 54 train loss: 0.8909, eval loss 1.5589007139205933\n",
                  "Epoch 55 train loss: 0.8878, eval loss 1.556339144706726\n",
                  "Epoch 56 train loss: 0.8858, eval loss 1.555436611175537\n",
                  "Epoch 57 train loss: 0.8811, eval loss 1.5534043312072754\n",
                  "Epoch 58 train loss: 0.8627, eval loss 1.5520869493484497\n",
                  "Epoch 59 train loss: 0.8612, eval loss 1.550342321395874\n",
                  "Epoch 60 train loss: 0.8942, eval loss 1.5471140146255493\n",
                  "Epoch 61 train loss: 0.8523, eval loss 1.5455973148345947\n",
                  "Epoch 62 train loss: 0.8265, eval loss 1.5443336963653564\n",
                  "Epoch 63 train loss: 0.8194, eval loss 1.5427865982055664\n",
                  "Epoch 64 train loss: 0.8390, eval loss 1.540690541267395\n",
                  "Epoch 65 train loss: 0.8458, eval loss 1.5392043590545654\n",
                  "Epoch 66 train loss: 0.8720, eval loss 1.5381755828857422\n",
                  "Epoch 67 train loss: 0.8230, eval loss 1.5353269577026367\n",
                  "Epoch 68 train loss: 0.8383, eval loss 1.5339686870574951\n",
                  "Epoch 69 train loss: 0.8543, eval loss 1.5332931280136108\n",
                  "Epoch 70 train loss: 0.8493, eval loss 1.5306085348129272\n",
                  "Epoch 71 train loss: 0.8143, eval loss 1.5291410684585571\n",
                  "Epoch 72 train loss: 0.8120, eval loss 1.5282052755355835\n",
                  "Epoch 73 train loss: 0.8139, eval loss 1.5274966955184937\n",
                  "Epoch 74 train loss: 0.8107, eval loss 1.5254276990890503\n",
                  "Epoch 75 train loss: 0.8276, eval loss 1.5236082077026367\n",
                  "Epoch 76 train loss: 0.7818, eval loss 1.5222797393798828\n",
                  "Epoch 77 train loss: 0.8125, eval loss 1.5210288763046265\n",
                  "Epoch 78 train loss: 0.8216, eval loss 1.5199499130249023\n",
                  "Epoch 79 train loss: 0.7978, eval loss 1.51854407787323\n",
                  "Epoch 80 train loss: 0.7836, eval loss 1.5180697441101074\n",
                  "Epoch 81 train loss: 0.8089, eval loss 1.5158658027648926\n",
                  "Epoch 82 train loss: 0.7636, eval loss 1.5148696899414062\n",
                  "Epoch 83 train loss: 0.8081, eval loss 1.5138219594955444\n",
                  "Epoch 84 train loss: 0.7735, eval loss 1.5129307508468628\n",
                  "Epoch 85 train loss: 0.7837, eval loss 1.5098655223846436\n",
                  "Epoch 86 train loss: 0.7708, eval loss 1.5087555646896362\n",
                  "Epoch 87 train loss: 0.8247, eval loss 1.507851004600525\n",
                  "Epoch 88 train loss: 0.7876, eval loss 1.5066665410995483\n",
                  "Epoch 89 train loss: 0.7569, eval loss 1.505995750427246\n",
                  "Epoch 90 train loss: 0.7546, eval loss 1.505220651626587\n",
                  "Epoch 91 train loss: 0.7873, eval loss 1.5039628744125366\n",
                  "Epoch 92 train loss: 0.7897, eval loss 1.5029282569885254\n",
                  "Epoch 93 train loss: 0.7481, eval loss 1.5017133951187134\n",
                  "Epoch 94 train loss: 0.8000, eval loss 1.500206708908081\n",
                  "Epoch 95 train loss: 0.7825, eval loss 1.4999927282333374\n",
                  "Epoch 96 train loss: 0.7951, eval loss 1.498462200164795\n",
                  "Epoch 97 train loss: 0.7682, eval loss 1.4979103803634644\n",
                  "Epoch 98 train loss: 0.7304, eval loss 1.4967411756515503\n",
                  "Epoch 99 train loss: 0.7573, eval loss 1.4965530633926392\n",
                  "Epoch 100 train loss: 0.7738, eval loss 1.4942163228988647\n",
                  "Epoch 101 train loss: 0.7812, eval loss 1.4941184520721436\n",
                  "Epoch 102 train loss: 0.7735, eval loss 1.4933149814605713\n",
                  "Epoch 103 train loss: 0.7623, eval loss 1.4916367530822754\n",
                  "Epoch 104 train loss: 0.7387, eval loss 1.4912306070327759\n",
                  "Epoch 105 train loss: 0.7528, eval loss 1.490180492401123\n",
                  "Epoch 106 train loss: 0.7437, eval loss 1.4893099069595337\n",
                  "Epoch 107 train loss: 0.8045, eval loss 1.4875677824020386\n",
                  "Epoch 108 train loss: 0.7317, eval loss 1.4874423742294312\n",
                  "Epoch 109 train loss: 0.7270, eval loss 1.4865907430648804\n",
                  "Epoch 110 train loss: 0.7338, eval loss 1.4860684871673584\n",
                  "Epoch 111 train loss: 0.7172, eval loss 1.4853097200393677\n",
                  "Epoch 112 train loss: 0.7342, eval loss 1.483856201171875\n",
                  "Epoch 113 train loss: 0.7103, eval loss 1.4835052490234375\n",
                  "Epoch 114 train loss: 0.7223, eval loss 1.4830011129379272\n",
                  "Epoch 115 train loss: 0.7192, eval loss 1.4819740056991577\n",
                  "Epoch 116 train loss: 0.7699, eval loss 1.4803310632705688\n",
                  "Epoch 117 train loss: 0.7359, eval loss 1.4812531471252441\n",
                  "Epoch 118 train loss: 0.7160, eval loss 1.4795818328857422\n",
                  "Epoch 119 train loss: 0.6967, eval loss 1.4792604446411133\n",
                  "Epoch 120 train loss: 0.7374, eval loss 1.4784280061721802\n",
                  "Epoch 121 train loss: 0.7217, eval loss 1.477648377418518\n",
                  "Epoch 122 train loss: 0.7236, eval loss 1.4769177436828613\n",
                  "Epoch 123 train loss: 0.7189, eval loss 1.4765113592147827\n",
                  "Epoch 124 train loss: 0.7482, eval loss 1.475844144821167\n",
                  "Epoch 125 train loss: 0.7236, eval loss 1.474865198135376\n",
                  "Epoch 126 train loss: 0.7402, eval loss 1.4747536182403564\n",
                  "Epoch 127 train loss: 0.7057, eval loss 1.4743967056274414\n",
                  "Epoch 128 train loss: 0.7479, eval loss 1.473408579826355\n",
                  "Epoch 129 train loss: 0.6898, eval loss 1.472427248954773\n",
                  "Epoch 130 train loss: 0.7451, eval loss 1.472371220588684\n",
                  "Epoch 131 train loss: 0.7183, eval loss 1.4713860750198364\n",
                  "Epoch 132 train loss: 0.7207, eval loss 1.47138512134552\n",
                  "Epoch 133 train loss: 0.6905, eval loss 1.4702881574630737\n",
                  "Epoch 134 train loss: 0.7151, eval loss 1.4696130752563477\n",
                  "Epoch 135 train loss: 0.7383, eval loss 1.4691601991653442\n",
                  "Epoch 136 train loss: 0.6932, eval loss 1.4683960676193237\n",
                  "Epoch 137 train loss: 0.7051, eval loss 1.4679758548736572\n",
                  "Epoch 138 train loss: 0.6829, eval loss 1.4671460390090942\n",
                  "Epoch 139 train loss: 0.7320, eval loss 1.4667654037475586\n",
                  "Epoch 140 train loss: 0.7107, eval loss 1.466544508934021\n",
                  "Epoch 141 train loss: 0.7160, eval loss 1.4667267799377441\n",
                  "Epoch 142 train loss: 0.7213, eval loss 1.4657673835754395\n",
                  "Epoch 143 train loss: 0.7229, eval loss 1.465782880783081\n",
                  "Epoch 144 train loss: 0.7575, eval loss 1.4641485214233398\n",
                  "Epoch 145 train loss: 0.6849, eval loss 1.4639660120010376\n",
                  "Epoch 146 train loss: 0.7438, eval loss 1.4638805389404297\n",
                  "Epoch 147 train loss: 0.7334, eval loss 1.4637503623962402\n",
                  "Epoch 148 train loss: 0.6605, eval loss 1.4628188610076904\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:02:43,528] Trial 14 finished with value: 1.4627571105957031 and parameters: {'hidden_layers_size': 121, 'dropout_p': 0.29991383317927034, 'learning_rate': 6.0129957046778105e-06, 'batch_size': 215, 'l2_reg': 0.0009516744939626131}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.7196, eval loss 1.4627571105957031\n",
                  "Epoch 0 train loss: 1.1965, eval loss 1.7229923009872437\n",
                  "Epoch 1 train loss: 1.0994, eval loss 1.6612000465393066\n",
                  "Epoch 2 train loss: 1.1219, eval loss 1.627011775970459\n",
                  "Epoch 3 train loss: 0.9734, eval loss 1.6005452871322632\n",
                  "Epoch 4 train loss: 0.9406, eval loss 1.578818917274475\n",
                  "Epoch 5 train loss: 0.8827, eval loss 1.560258150100708\n",
                  "Epoch 6 train loss: 0.9022, eval loss 1.5453599691390991\n",
                  "Epoch 7 train loss: 0.8525, eval loss 1.5312225818634033\n",
                  "Epoch 8 train loss: 0.8162, eval loss 1.5202038288116455\n",
                  "Epoch 9 train loss: 0.7948, eval loss 1.5092308521270752\n",
                  "Epoch 10 train loss: 0.8507, eval loss 1.5018463134765625\n",
                  "Epoch 11 train loss: 0.7991, eval loss 1.4939216375350952\n",
                  "Epoch 12 train loss: 0.7755, eval loss 1.489458680152893\n",
                  "Epoch 13 train loss: 0.7466, eval loss 1.4835783243179321\n",
                  "Epoch 14 train loss: 0.8216, eval loss 1.4797132015228271\n",
                  "Epoch 15 train loss: 0.8275, eval loss 1.4761546850204468\n",
                  "Epoch 16 train loss: 0.6950, eval loss 1.4703694581985474\n",
                  "Epoch 17 train loss: 0.7240, eval loss 1.4685450792312622\n",
                  "Epoch 18 train loss: 0.7680, eval loss 1.4656745195388794\n",
                  "Epoch 19 train loss: 0.7634, eval loss 1.463205337524414\n",
                  "Epoch 20 train loss: 0.7628, eval loss 1.461527705192566\n",
                  "Epoch 21 train loss: 0.7211, eval loss 1.4593548774719238\n",
                  "Epoch 22 train loss: 0.7787, eval loss 1.457098126411438\n",
                  "Epoch 23 train loss: 0.7002, eval loss 1.4561187028884888\n",
                  "Epoch 24 train loss: 0.7583, eval loss 1.4554394483566284\n",
                  "Epoch 25 train loss: 0.6764, eval loss 1.4531835317611694\n",
                  "Epoch 26 train loss: 0.6943, eval loss 1.4510449171066284\n",
                  "Epoch 27 train loss: 0.7296, eval loss 1.451075792312622\n",
                  "Epoch 28 train loss: 0.7178, eval loss 1.4500254392623901\n",
                  "Epoch 29 train loss: 0.7614, eval loss 1.4497960805892944\n",
                  "Epoch 30 train loss: 0.7135, eval loss 1.4491852521896362\n",
                  "Epoch 31 train loss: 0.6522, eval loss 1.4475047588348389\n",
                  "Epoch 32 train loss: 0.7119, eval loss 1.4467535018920898\n",
                  "Epoch 33 train loss: 0.6721, eval loss 1.4459953308105469\n",
                  "Epoch 34 train loss: 0.6946, eval loss 1.4447720050811768\n",
                  "Epoch 35 train loss: 0.7276, eval loss 1.4446550607681274\n",
                  "Epoch 36 train loss: 0.6554, eval loss 1.4445022344589233\n",
                  "Epoch 37 train loss: 0.7159, eval loss 1.442472219467163\n",
                  "Epoch 38 train loss: 0.6779, eval loss 1.4426581859588623\n",
                  "Epoch 39 train loss: 0.6776, eval loss 1.4422427415847778\n",
                  "Epoch 40 train loss: 0.6977, eval loss 1.4419164657592773\n",
                  "Epoch 41 train loss: 0.7015, eval loss 1.441953420639038\n",
                  "Epoch 42 train loss: 0.6797, eval loss 1.4410251379013062\n",
                  "Epoch 43 train loss: 0.6084, eval loss 1.4410803318023682\n",
                  "Epoch 44 train loss: 0.6433, eval loss 1.4412370920181274\n",
                  "Epoch 45 train loss: 0.7059, eval loss 1.4396885633468628\n",
                  "Epoch 46 train loss: 0.6702, eval loss 1.439793586730957\n",
                  "Epoch 47 train loss: 0.6195, eval loss 1.4382132291793823\n",
                  "Epoch 48 train loss: 0.7075, eval loss 1.4391202926635742\n",
                  "Epoch 49 train loss: 0.7666, eval loss 1.4385161399841309\n",
                  "Epoch 50 train loss: 0.6372, eval loss 1.4388248920440674\n",
                  "Epoch 51 train loss: 0.7475, eval loss 1.4371999502182007\n",
                  "Epoch 52 train loss: 0.6417, eval loss 1.438188076019287\n",
                  "Epoch 53 train loss: 0.5711, eval loss 1.437935471534729\n",
                  "Epoch 54 train loss: 0.6197, eval loss 1.4368129968643188\n",
                  "Epoch 55 train loss: 0.6344, eval loss 1.437606930732727\n",
                  "Epoch 56 train loss: 0.6032, eval loss 1.4378089904785156\n",
                  "Epoch 57 train loss: 0.6905, eval loss 1.4367454051971436\n",
                  "Epoch 58 train loss: 0.5749, eval loss 1.4358049631118774\n",
                  "Epoch 59 train loss: 0.7271, eval loss 1.4367419481277466\n",
                  "Epoch 60 train loss: 0.5374, eval loss 1.4352587461471558\n",
                  "Epoch 61 train loss: 0.6163, eval loss 1.4358367919921875\n",
                  "Epoch 62 train loss: 0.6065, eval loss 1.4358454942703247\n",
                  "Epoch 63 train loss: 0.5077, eval loss 1.4355512857437134\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:03:06,385] Trial 15 finished with value: 1.4352587461471558 and parameters: {'hidden_layers_size': 71, 'dropout_p': 0.437871736266903, 'learning_rate': 0.00018625712671515435, 'batch_size': 461, 'l2_reg': 0.0003932995173590204}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4032, eval loss 1.7294985055923462\n",
                  "Epoch 1 train loss: 1.3037, eval loss 1.7234677076339722\n",
                  "Epoch 2 train loss: 1.2648, eval loss 1.7150424718856812\n",
                  "Epoch 3 train loss: 1.3452, eval loss 1.7081843614578247\n",
                  "Epoch 4 train loss: 1.2671, eval loss 1.700881004333496\n",
                  "Epoch 5 train loss: 1.2996, eval loss 1.6951887607574463\n",
                  "Epoch 6 train loss: 1.2462, eval loss 1.6875277757644653\n",
                  "Epoch 7 train loss: 1.2284, eval loss 1.6805890798568726\n",
                  "Epoch 8 train loss: 1.1391, eval loss 1.6751989126205444\n",
                  "Epoch 9 train loss: 1.2244, eval loss 1.6697666645050049\n",
                  "Epoch 10 train loss: 1.1645, eval loss 1.6647183895111084\n",
                  "Epoch 11 train loss: 1.1078, eval loss 1.659447193145752\n",
                  "Epoch 12 train loss: 1.1108, eval loss 1.6550815105438232\n",
                  "Epoch 13 train loss: 1.1122, eval loss 1.649787425994873\n",
                  "Epoch 14 train loss: 1.0835, eval loss 1.6450939178466797\n",
                  "Epoch 15 train loss: 1.1047, eval loss 1.6401314735412598\n",
                  "Epoch 16 train loss: 1.0294, eval loss 1.6355986595153809\n",
                  "Epoch 17 train loss: 1.0643, eval loss 1.6321371793746948\n",
                  "Epoch 18 train loss: 1.0372, eval loss 1.6287575960159302\n",
                  "Epoch 19 train loss: 1.0447, eval loss 1.6240595579147339\n",
                  "Epoch 20 train loss: 1.0620, eval loss 1.6213206052780151\n",
                  "Epoch 21 train loss: 1.0537, eval loss 1.6177815198898315\n",
                  "Epoch 22 train loss: 1.0439, eval loss 1.6137570142745972\n",
                  "Epoch 23 train loss: 1.0176, eval loss 1.6116408109664917\n",
                  "Epoch 24 train loss: 1.0257, eval loss 1.6069014072418213\n",
                  "Epoch 25 train loss: 0.9862, eval loss 1.6041208505630493\n",
                  "Epoch 26 train loss: 1.0595, eval loss 1.6009036302566528\n",
                  "Epoch 27 train loss: 0.9422, eval loss 1.5980719327926636\n",
                  "Epoch 28 train loss: 1.0064, eval loss 1.59475576877594\n",
                  "Epoch 29 train loss: 0.9460, eval loss 1.592268705368042\n",
                  "Epoch 30 train loss: 0.9883, eval loss 1.5900776386260986\n",
                  "Epoch 31 train loss: 0.9453, eval loss 1.5874894857406616\n",
                  "Epoch 32 train loss: 0.9375, eval loss 1.5841524600982666\n",
                  "Epoch 33 train loss: 0.9784, eval loss 1.5816247463226318\n",
                  "Epoch 34 train loss: 0.9328, eval loss 1.5791162252426147\n",
                  "Epoch 35 train loss: 0.9175, eval loss 1.577260136604309\n",
                  "Epoch 36 train loss: 0.9115, eval loss 1.5739381313323975\n",
                  "Epoch 37 train loss: 0.9014, eval loss 1.5718244314193726\n",
                  "Epoch 38 train loss: 0.9018, eval loss 1.5703731775283813\n",
                  "Epoch 39 train loss: 0.9222, eval loss 1.5670723915100098\n",
                  "Epoch 40 train loss: 0.8726, eval loss 1.5660054683685303\n",
                  "Epoch 41 train loss: 0.8873, eval loss 1.5636731386184692\n",
                  "Epoch 42 train loss: 0.8991, eval loss 1.5615546703338623\n",
                  "Epoch 43 train loss: 0.8841, eval loss 1.560177206993103\n",
                  "Epoch 44 train loss: 0.9099, eval loss 1.5583606958389282\n",
                  "Epoch 45 train loss: 0.8900, eval loss 1.5557101964950562\n",
                  "Epoch 46 train loss: 0.8893, eval loss 1.554482102394104\n",
                  "Epoch 47 train loss: 0.8502, eval loss 1.552092432975769\n",
                  "Epoch 48 train loss: 0.8374, eval loss 1.5505481958389282\n",
                  "Epoch 49 train loss: 0.8948, eval loss 1.5489269495010376\n",
                  "Epoch 50 train loss: 0.8719, eval loss 1.546019434928894\n",
                  "Epoch 51 train loss: 0.9034, eval loss 1.5449676513671875\n",
                  "Epoch 52 train loss: 0.8336, eval loss 1.5422923564910889\n",
                  "Epoch 53 train loss: 0.8408, eval loss 1.5412081480026245\n",
                  "Epoch 54 train loss: 0.8634, eval loss 1.5404728651046753\n",
                  "Epoch 55 train loss: 0.8414, eval loss 1.537904143333435\n",
                  "Epoch 56 train loss: 0.8113, eval loss 1.5358864068984985\n",
                  "Epoch 57 train loss: 0.8155, eval loss 1.5349228382110596\n",
                  "Epoch 58 train loss: 0.8481, eval loss 1.5341768264770508\n",
                  "Epoch 59 train loss: 0.8243, eval loss 1.5322972536087036\n",
                  "Epoch 60 train loss: 0.8190, eval loss 1.5312844514846802\n",
                  "Epoch 61 train loss: 0.7863, eval loss 1.5295488834381104\n",
                  "Epoch 62 train loss: 0.8452, eval loss 1.5288245677947998\n",
                  "Epoch 63 train loss: 0.8384, eval loss 1.527420997619629\n",
                  "Epoch 64 train loss: 0.8770, eval loss 1.5257296562194824\n",
                  "Epoch 65 train loss: 0.8065, eval loss 1.5244337320327759\n",
                  "Epoch 66 train loss: 0.8084, eval loss 1.5234249830245972\n",
                  "Epoch 67 train loss: 0.8289, eval loss 1.521203637123108\n",
                  "Epoch 68 train loss: 0.8023, eval loss 1.520541787147522\n",
                  "Epoch 69 train loss: 0.8155, eval loss 1.5191375017166138\n",
                  "Epoch 70 train loss: 0.8348, eval loss 1.5182442665100098\n",
                  "Epoch 71 train loss: 0.7992, eval loss 1.5159640312194824\n",
                  "Epoch 72 train loss: 0.8106, eval loss 1.5154958963394165\n",
                  "Epoch 73 train loss: 0.7732, eval loss 1.5142322778701782\n",
                  "Epoch 74 train loss: 0.8285, eval loss 1.5128520727157593\n",
                  "Epoch 75 train loss: 0.8323, eval loss 1.5114768743515015\n",
                  "Epoch 76 train loss: 0.8103, eval loss 1.5116199254989624\n",
                  "Epoch 77 train loss: 0.7845, eval loss 1.5097206830978394\n",
                  "Epoch 78 train loss: 0.8186, eval loss 1.5088664293289185\n",
                  "Epoch 79 train loss: 0.8075, eval loss 1.5080384016036987\n",
                  "Epoch 80 train loss: 0.8072, eval loss 1.5066016912460327\n",
                  "Epoch 81 train loss: 0.7601, eval loss 1.5052473545074463\n",
                  "Epoch 82 train loss: 0.7883, eval loss 1.5051946640014648\n",
                  "Epoch 83 train loss: 0.7800, eval loss 1.5039970874786377\n",
                  "Epoch 84 train loss: 0.8037, eval loss 1.5026942491531372\n",
                  "Epoch 85 train loss: 0.7500, eval loss 1.5018819570541382\n",
                  "Epoch 86 train loss: 0.7482, eval loss 1.5004186630249023\n",
                  "Epoch 87 train loss: 0.7848, eval loss 1.5002609491348267\n",
                  "Epoch 88 train loss: 0.7908, eval loss 1.4991527795791626\n",
                  "Epoch 89 train loss: 0.8240, eval loss 1.4981611967086792\n",
                  "Epoch 90 train loss: 0.7476, eval loss 1.4978272914886475\n",
                  "Epoch 91 train loss: 0.7730, eval loss 1.4964680671691895\n",
                  "Epoch 92 train loss: 0.7364, eval loss 1.4959276914596558\n",
                  "Epoch 93 train loss: 0.8080, eval loss 1.4950522184371948\n",
                  "Epoch 94 train loss: 0.7432, eval loss 1.4934895038604736\n",
                  "Epoch 95 train loss: 0.7565, eval loss 1.492944359779358\n",
                  "Epoch 96 train loss: 0.8040, eval loss 1.4915424585342407\n",
                  "Epoch 97 train loss: 0.7121, eval loss 1.491189956665039\n",
                  "Epoch 98 train loss: 0.7679, eval loss 1.4908051490783691\n",
                  "Epoch 99 train loss: 0.7518, eval loss 1.4900346994400024\n",
                  "Epoch 100 train loss: 0.7985, eval loss 1.4894148111343384\n",
                  "Epoch 101 train loss: 0.7392, eval loss 1.4884260892868042\n",
                  "Epoch 102 train loss: 0.7346, eval loss 1.488081932067871\n",
                  "Epoch 103 train loss: 0.7512, eval loss 1.4871083498001099\n",
                  "Epoch 104 train loss: 0.7385, eval loss 1.485919713973999\n",
                  "Epoch 105 train loss: 0.7443, eval loss 1.4865111112594604\n",
                  "Epoch 106 train loss: 0.7650, eval loss 1.4851058721542358\n",
                  "Epoch 107 train loss: 0.7857, eval loss 1.483662486076355\n",
                  "Epoch 108 train loss: 0.7389, eval loss 1.4839482307434082\n",
                  "Epoch 109 train loss: 0.7557, eval loss 1.4830716848373413\n",
                  "Epoch 110 train loss: 0.7328, eval loss 1.4823977947235107\n",
                  "Epoch 111 train loss: 0.7620, eval loss 1.4814033508300781\n",
                  "Epoch 112 train loss: 0.7532, eval loss 1.4810233116149902\n",
                  "Epoch 113 train loss: 0.7433, eval loss 1.4803357124328613\n",
                  "Epoch 114 train loss: 0.7776, eval loss 1.4800491333007812\n",
                  "Epoch 115 train loss: 0.6871, eval loss 1.4791650772094727\n",
                  "Epoch 116 train loss: 0.7235, eval loss 1.4790760278701782\n",
                  "Epoch 117 train loss: 0.7193, eval loss 1.4784877300262451\n",
                  "Epoch 118 train loss: 0.7785, eval loss 1.47694993019104\n",
                  "Epoch 119 train loss: 0.7623, eval loss 1.4767720699310303\n",
                  "Epoch 120 train loss: 0.6901, eval loss 1.4758062362670898\n",
                  "Epoch 121 train loss: 0.7356, eval loss 1.4757412672042847\n",
                  "Epoch 122 train loss: 0.7475, eval loss 1.4749208688735962\n",
                  "Epoch 123 train loss: 0.7074, eval loss 1.4745765924453735\n",
                  "Epoch 124 train loss: 0.7545, eval loss 1.4738744497299194\n",
                  "Epoch 125 train loss: 0.7649, eval loss 1.4728862047195435\n",
                  "Epoch 126 train loss: 0.7214, eval loss 1.4727572202682495\n",
                  "Epoch 127 train loss: 0.7279, eval loss 1.4723149538040161\n",
                  "Epoch 128 train loss: 0.7010, eval loss 1.4728752374649048\n",
                  "Epoch 129 train loss: 0.7239, eval loss 1.4717013835906982\n",
                  "Epoch 130 train loss: 0.6886, eval loss 1.4712884426116943\n",
                  "Epoch 131 train loss: 0.7142, eval loss 1.4711426496505737\n",
                  "Epoch 132 train loss: 0.7560, eval loss 1.4702496528625488\n",
                  "Epoch 133 train loss: 0.7252, eval loss 1.470263957977295\n",
                  "Epoch 134 train loss: 0.7335, eval loss 1.4695115089416504\n",
                  "Epoch 135 train loss: 0.7199, eval loss 1.468845248222351\n",
                  "Epoch 136 train loss: 0.6998, eval loss 1.4683955907821655\n",
                  "Epoch 137 train loss: 0.7112, eval loss 1.468226432800293\n",
                  "Epoch 138 train loss: 0.7025, eval loss 1.4680956602096558\n",
                  "Epoch 139 train loss: 0.7697, eval loss 1.467373013496399\n",
                  "Epoch 140 train loss: 0.7135, eval loss 1.4667811393737793\n",
                  "Epoch 141 train loss: 0.7153, eval loss 1.4664604663848877\n",
                  "Epoch 142 train loss: 0.6942, eval loss 1.46535325050354\n",
                  "Epoch 143 train loss: 0.7371, eval loss 1.4655250310897827\n",
                  "Epoch 144 train loss: 0.7277, eval loss 1.4654439687728882\n",
                  "Epoch 145 train loss: 0.6870, eval loss 1.4645206928253174\n",
                  "Epoch 146 train loss: 0.7514, eval loss 1.4641609191894531\n",
                  "Epoch 147 train loss: 0.7260, eval loss 1.4642982482910156\n",
                  "Epoch 148 train loss: 0.7304, eval loss 1.4637410640716553\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:04:09,529] Trial 16 finished with value: 1.4637410640716553 and parameters: {'hidden_layers_size': 107, 'dropout_p': 0.39266611891718056, 'learning_rate': 9.495161071185034e-06, 'batch_size': 295, 'l2_reg': 7.64616524773608e-05}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.7479, eval loss 1.4639668464660645\n",
                  "Epoch 0 train loss: 1.1123, eval loss 1.6468656063079834\n",
                  "Epoch 1 train loss: 1.0239, eval loss 1.5952845811843872\n",
                  "Epoch 2 train loss: 0.9695, eval loss 1.5622106790542603\n",
                  "Epoch 3 train loss: 0.8154, eval loss 1.5402562618255615\n",
                  "Epoch 4 train loss: 0.8050, eval loss 1.5221784114837646\n",
                  "Epoch 5 train loss: 0.8156, eval loss 1.5089564323425293\n",
                  "Epoch 6 train loss: 0.7940, eval loss 1.49684739112854\n",
                  "Epoch 7 train loss: 0.7689, eval loss 1.488135814666748\n",
                  "Epoch 8 train loss: 0.7531, eval loss 1.4811172485351562\n",
                  "Epoch 9 train loss: 0.7354, eval loss 1.4740455150604248\n",
                  "Epoch 10 train loss: 0.7903, eval loss 1.4686981439590454\n",
                  "Epoch 11 train loss: 0.7370, eval loss 1.4643141031265259\n",
                  "Epoch 12 train loss: 0.6562, eval loss 1.4606893062591553\n",
                  "Epoch 13 train loss: 0.7646, eval loss 1.457921028137207\n",
                  "Epoch 14 train loss: 0.7465, eval loss 1.4551681280136108\n",
                  "Epoch 15 train loss: 0.7428, eval loss 1.45291268825531\n",
                  "Epoch 16 train loss: 0.7621, eval loss 1.450881004333496\n",
                  "Epoch 17 train loss: 0.7122, eval loss 1.4477850198745728\n",
                  "Epoch 18 train loss: 0.6664, eval loss 1.4470264911651611\n",
                  "Epoch 19 train loss: 0.7078, eval loss 1.4462069272994995\n",
                  "Epoch 20 train loss: 0.6686, eval loss 1.4447389841079712\n",
                  "Epoch 21 train loss: 0.6657, eval loss 1.4434709548950195\n",
                  "Epoch 22 train loss: 0.7260, eval loss 1.4432791471481323\n",
                  "Epoch 23 train loss: 0.6543, eval loss 1.4418048858642578\n",
                  "Epoch 24 train loss: 0.7559, eval loss 1.4409958124160767\n",
                  "Epoch 25 train loss: 0.6988, eval loss 1.4399586915969849\n",
                  "Epoch 26 train loss: 0.6408, eval loss 1.4392576217651367\n",
                  "Epoch 27 train loss: 0.6103, eval loss 1.4391207695007324\n",
                  "Epoch 28 train loss: 0.6380, eval loss 1.4388588666915894\n",
                  "Epoch 29 train loss: 0.6269, eval loss 1.438184380531311\n",
                  "Epoch 30 train loss: 0.5537, eval loss 1.4370681047439575\n",
                  "Epoch 31 train loss: 0.7125, eval loss 1.437332272529602\n",
                  "Epoch 32 train loss: 0.6346, eval loss 1.436058521270752\n",
                  "Epoch 33 train loss: 0.6081, eval loss 1.4364352226257324\n",
                  "Epoch 34 train loss: 0.6112, eval loss 1.435857892036438\n",
                  "Epoch 35 train loss: 0.5416, eval loss 1.4353270530700684\n",
                  "Epoch 36 train loss: 0.7010, eval loss 1.4350593090057373\n",
                  "Epoch 37 train loss: 0.6511, eval loss 1.4351696968078613\n",
                  "Epoch 38 train loss: 0.6214, eval loss 1.4347686767578125\n",
                  "Epoch 39 train loss: 0.6849, eval loss 1.4337741136550903\n",
                  "Epoch 40 train loss: 0.5689, eval loss 1.4346067905426025\n",
                  "Epoch 41 train loss: 0.6732, eval loss 1.4341497421264648\n",
                  "Epoch 42 train loss: 0.6964, eval loss 1.4336254596710205\n",
                  "Epoch 43 train loss: 0.5712, eval loss 1.4326844215393066\n",
                  "Epoch 44 train loss: 0.6676, eval loss 1.43317449092865\n",
                  "Epoch 45 train loss: 0.6512, eval loss 1.4331047534942627\n",
                  "Epoch 46 train loss: 0.6076, eval loss 1.4330103397369385\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:04:33,116] Trial 17 finished with value: 1.4326844215393066 and parameters: {'hidden_layers_size': 134, 'dropout_p': 0.3366923291272439, 'learning_rate': 9.204712398723734e-05, 'batch_size': 212, 'l2_reg': 0.00018896143933117384}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.0890, eval loss 1.6460046768188477\n",
                  "Epoch 1 train loss: 0.9197, eval loss 1.5881035327911377\n",
                  "Epoch 2 train loss: 0.8918, eval loss 1.552332878112793\n",
                  "Epoch 3 train loss: 0.8152, eval loss 1.5277966260910034\n",
                  "Epoch 4 train loss: 0.8186, eval loss 1.509259819984436\n",
                  "Epoch 5 train loss: 0.7817, eval loss 1.4949637651443481\n",
                  "Epoch 6 train loss: 0.7019, eval loss 1.4851161241531372\n",
                  "Epoch 7 train loss: 0.7188, eval loss 1.4770994186401367\n",
                  "Epoch 8 train loss: 0.6954, eval loss 1.469916582107544\n",
                  "Epoch 9 train loss: 0.7371, eval loss 1.4644482135772705\n",
                  "Epoch 10 train loss: 0.6890, eval loss 1.4599238634109497\n",
                  "Epoch 11 train loss: 0.7532, eval loss 1.4572826623916626\n",
                  "Epoch 12 train loss: 0.7325, eval loss 1.4546782970428467\n",
                  "Epoch 13 train loss: 0.7467, eval loss 1.4518240690231323\n",
                  "Epoch 14 train loss: 0.7030, eval loss 1.4500946998596191\n",
                  "Epoch 15 train loss: 0.6650, eval loss 1.448241114616394\n",
                  "Epoch 16 train loss: 0.6627, eval loss 1.4467077255249023\n",
                  "Epoch 17 train loss: 0.6967, eval loss 1.4456870555877686\n",
                  "Epoch 18 train loss: 0.7667, eval loss 1.4438281059265137\n",
                  "Epoch 19 train loss: 0.6989, eval loss 1.442477822303772\n",
                  "Epoch 20 train loss: 0.6860, eval loss 1.4422006607055664\n",
                  "Epoch 21 train loss: 0.7199, eval loss 1.440866470336914\n",
                  "Epoch 22 train loss: 0.6641, eval loss 1.4404094219207764\n",
                  "Epoch 23 train loss: 0.7055, eval loss 1.4395477771759033\n",
                  "Epoch 24 train loss: 0.7242, eval loss 1.4400790929794312\n",
                  "Epoch 25 train loss: 0.6681, eval loss 1.4387720823287964\n",
                  "Epoch 26 train loss: 0.6686, eval loss 1.4383976459503174\n",
                  "Epoch 27 train loss: 0.6281, eval loss 1.4368913173675537\n",
                  "Epoch 28 train loss: 0.6836, eval loss 1.4365956783294678\n",
                  "Epoch 29 train loss: 0.7353, eval loss 1.4366986751556396\n",
                  "Epoch 30 train loss: 0.6782, eval loss 1.4364500045776367\n",
                  "Epoch 31 train loss: 0.6740, eval loss 1.436387300491333\n",
                  "Epoch 32 train loss: 0.6888, eval loss 1.4350252151489258\n",
                  "Epoch 33 train loss: 0.6733, eval loss 1.4354852437973022\n",
                  "Epoch 34 train loss: 0.6790, eval loss 1.4356602430343628\n",
                  "Epoch 35 train loss: 0.7082, eval loss 1.4356234073638916\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:04:50,505] Trial 18 finished with value: 1.4350252151489258 and parameters: {'hidden_layers_size': 94, 'dropout_p': 0.3038518730514599, 'learning_rate': 0.00011458291191919601, 'batch_size': 193, 'l2_reg': 0.00014386059578503607}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.0512, eval loss 1.4317905902862549\n",
                  "Epoch 1 train loss: 0.8688, eval loss 1.4331815242767334\n",
                  "Epoch 2 train loss: 0.5396, eval loss 1.4314998388290405\n",
                  "Epoch 3 train loss: 0.6318, eval loss 1.4319485425949097\n",
                  "Epoch 4 train loss: 0.7159, eval loss 1.431221842765808\n",
                  "Epoch 5 train loss: 0.5695, eval loss 1.4277889728546143\n",
                  "Epoch 6 train loss: 0.5628, eval loss 1.4275132417678833\n",
                  "Epoch 7 train loss: 0.4138, eval loss 1.4268102645874023\n",
                  "Epoch 8 train loss: 0.4200, eval loss 1.429624319076538\n",
                  "Epoch 9 train loss: 0.5735, eval loss 1.4269088506698608\n",
                  "Epoch 10 train loss: 0.3999, eval loss 1.428053617477417\n",
                  "Epoch 11 train loss: 0.3550, eval loss 1.426342487335205\n",
                  "Epoch 12 train loss: 0.5195, eval loss 1.425267219543457\n",
                  "Epoch 13 train loss: 0.4006, eval loss 1.4240643978118896\n",
                  "Epoch 14 train loss: 0.3292, eval loss 1.4246742725372314\n",
                  "Epoch 15 train loss: 0.3529, eval loss 1.4248906373977661\n",
                  "Epoch 16 train loss: 0.2981, eval loss 1.425062894821167\n",
                  "Epoch 17 train loss: 0.2729, eval loss 1.4224984645843506\n",
                  "Epoch 18 train loss: 0.5064, eval loss 1.4244859218597412\n",
                  "Epoch 19 train loss: 0.3513, eval loss 1.4240081310272217\n",
                  "Epoch 20 train loss: 0.3307, eval loss 1.4238722324371338\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:04:59,125] Trial 19 finished with value: 1.4224984645843506 and parameters: {'hidden_layers_size': 128, 'dropout_p': 0.4422921623955842, 'learning_rate': 0.005511537710141839, 'batch_size': 341, 'l2_reg': 2.3874142479923986e-05}. Best is trial 10 with value: 1.604019045829773.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4443, eval loss 1.7709441184997559\n",
                  "Epoch 1 train loss: 1.4473, eval loss 1.769761085510254\n",
                  "Epoch 2 train loss: 1.4151, eval loss 1.7692837715148926\n",
                  "Epoch 3 train loss: 1.4558, eval loss 1.767385482788086\n",
                  "Epoch 4 train loss: 1.3998, eval loss 1.7662322521209717\n",
                  "Epoch 5 train loss: 1.4646, eval loss 1.7656043767929077\n",
                  "Epoch 6 train loss: 1.4819, eval loss 1.765187382698059\n",
                  "Epoch 7 train loss: 1.4732, eval loss 1.7631601095199585\n",
                  "Epoch 8 train loss: 1.4541, eval loss 1.762381672859192\n",
                  "Epoch 9 train loss: 1.4287, eval loss 1.7601577043533325\n",
                  "Epoch 10 train loss: 1.4196, eval loss 1.7594207525253296\n",
                  "Epoch 11 train loss: 1.4224, eval loss 1.759424090385437\n",
                  "Epoch 12 train loss: 1.3831, eval loss 1.7568528652191162\n",
                  "Epoch 13 train loss: 1.3857, eval loss 1.7559165954589844\n",
                  "Epoch 14 train loss: 1.3854, eval loss 1.7551696300506592\n",
                  "Epoch 15 train loss: 1.3981, eval loss 1.7531284093856812\n",
                  "Epoch 16 train loss: 1.3655, eval loss 1.7532397508621216\n",
                  "Epoch 17 train loss: 1.4061, eval loss 1.7518978118896484\n",
                  "Epoch 18 train loss: 1.3961, eval loss 1.7497122287750244\n",
                  "Epoch 19 train loss: 1.4125, eval loss 1.74912428855896\n",
                  "Epoch 20 train loss: 1.4049, eval loss 1.7488234043121338\n",
                  "Epoch 21 train loss: 1.3354, eval loss 1.7483649253845215\n",
                  "Epoch 22 train loss: 1.3812, eval loss 1.7466039657592773\n",
                  "Epoch 23 train loss: 1.3642, eval loss 1.7454758882522583\n",
                  "Epoch 24 train loss: 1.3496, eval loss 1.7443695068359375\n",
                  "Epoch 25 train loss: 1.3710, eval loss 1.7430347204208374\n",
                  "Epoch 26 train loss: 1.3826, eval loss 1.742311954498291\n",
                  "Epoch 27 train loss: 1.3532, eval loss 1.7409262657165527\n",
                  "Epoch 28 train loss: 1.3427, eval loss 1.7395809888839722\n",
                  "Epoch 29 train loss: 1.3231, eval loss 1.739061951637268\n",
                  "Epoch 30 train loss: 1.3415, eval loss 1.7386006116867065\n",
                  "Epoch 31 train loss: 1.3482, eval loss 1.7369871139526367\n",
                  "Epoch 32 train loss: 1.3859, eval loss 1.7360285520553589\n",
                  "Epoch 33 train loss: 1.3192, eval loss 1.734147310256958\n",
                  "Epoch 34 train loss: 1.3425, eval loss 1.7336803674697876\n",
                  "Epoch 35 train loss: 1.3501, eval loss 1.732131004333496\n",
                  "Epoch 36 train loss: 1.3489, eval loss 1.731186866760254\n",
                  "Epoch 37 train loss: 1.3211, eval loss 1.7311885356903076\n",
                  "Epoch 38 train loss: 1.3021, eval loss 1.7306135892868042\n",
                  "Epoch 39 train loss: 1.3093, eval loss 1.7288172245025635\n",
                  "Epoch 40 train loss: 1.3367, eval loss 1.7272781133651733\n",
                  "Epoch 41 train loss: 1.3250, eval loss 1.7270042896270752\n",
                  "Epoch 42 train loss: 1.3551, eval loss 1.7254087924957275\n",
                  "Epoch 43 train loss: 1.3133, eval loss 1.7255879640579224\n",
                  "Epoch 44 train loss: 1.3390, eval loss 1.723724603652954\n",
                  "Epoch 45 train loss: 1.3233, eval loss 1.7232447862625122\n",
                  "Epoch 46 train loss: 1.3137, eval loss 1.721996784210205\n",
                  "Epoch 47 train loss: 1.2611, eval loss 1.720478892326355\n",
                  "Epoch 48 train loss: 1.3049, eval loss 1.7194079160690308\n",
                  "Epoch 49 train loss: 1.3052, eval loss 1.7181870937347412\n",
                  "Epoch 50 train loss: 1.2939, eval loss 1.7181614637374878\n",
                  "Epoch 51 train loss: 1.2563, eval loss 1.7172253131866455\n",
                  "Epoch 52 train loss: 1.2998, eval loss 1.7155989408493042\n",
                  "Epoch 53 train loss: 1.3055, eval loss 1.715700387954712\n",
                  "Epoch 54 train loss: 1.2465, eval loss 1.7141871452331543\n",
                  "Epoch 55 train loss: 1.2537, eval loss 1.7128559350967407\n",
                  "Epoch 56 train loss: 1.2852, eval loss 1.7121779918670654\n",
                  "Epoch 57 train loss: 1.2623, eval loss 1.7111788988113403\n",
                  "Epoch 58 train loss: 1.2407, eval loss 1.7106541395187378\n",
                  "Epoch 59 train loss: 1.2201, eval loss 1.7093825340270996\n",
                  "Epoch 60 train loss: 1.2450, eval loss 1.7089850902557373\n",
                  "Epoch 61 train loss: 1.2551, eval loss 1.708899974822998\n",
                  "Epoch 62 train loss: 1.2315, eval loss 1.7074428796768188\n",
                  "Epoch 63 train loss: 1.2710, eval loss 1.7059032917022705\n",
                  "Epoch 64 train loss: 1.2467, eval loss 1.7055450677871704\n",
                  "Epoch 65 train loss: 1.2503, eval loss 1.704084873199463\n",
                  "Epoch 66 train loss: 1.2234, eval loss 1.7031430006027222\n",
                  "Epoch 67 train loss: 1.2606, eval loss 1.7009117603302002\n",
                  "Epoch 68 train loss: 1.2215, eval loss 1.7015644311904907\n",
                  "Epoch 69 train loss: 1.2322, eval loss 1.6999163627624512\n",
                  "Epoch 70 train loss: 1.2776, eval loss 1.6997497081756592\n",
                  "Epoch 71 train loss: 1.2539, eval loss 1.699523687362671\n",
                  "Epoch 72 train loss: 1.2129, eval loss 1.697155237197876\n",
                  "Epoch 73 train loss: 1.1969, eval loss 1.6976526975631714\n",
                  "Epoch 74 train loss: 1.2373, eval loss 1.6963287591934204\n",
                  "Epoch 75 train loss: 1.2391, eval loss 1.6955171823501587\n",
                  "Epoch 76 train loss: 1.2060, eval loss 1.6940876245498657\n",
                  "Epoch 77 train loss: 1.2068, eval loss 1.6933999061584473\n",
                  "Epoch 78 train loss: 1.2341, eval loss 1.6931960582733154\n",
                  "Epoch 79 train loss: 1.1688, eval loss 1.6913490295410156\n",
                  "Epoch 80 train loss: 1.2507, eval loss 1.6903632879257202\n",
                  "Epoch 81 train loss: 1.1854, eval loss 1.6897671222686768\n",
                  "Epoch 82 train loss: 1.2209, eval loss 1.6900911331176758\n",
                  "Epoch 83 train loss: 1.2507, eval loss 1.6889501810073853\n",
                  "Epoch 84 train loss: 1.1655, eval loss 1.686850666999817\n",
                  "Epoch 85 train loss: 1.1693, eval loss 1.6866344213485718\n",
                  "Epoch 86 train loss: 1.2049, eval loss 1.6864694356918335\n",
                  "Epoch 87 train loss: 1.1924, eval loss 1.6853011846542358\n",
                  "Epoch 88 train loss: 1.2004, eval loss 1.6842775344848633\n",
                  "Epoch 89 train loss: 1.1598, eval loss 1.6833305358886719\n",
                  "Epoch 90 train loss: 1.1931, eval loss 1.6819392442703247\n",
                  "Epoch 91 train loss: 1.1713, eval loss 1.6825287342071533\n",
                  "Epoch 92 train loss: 1.1393, eval loss 1.6813730001449585\n",
                  "Epoch 93 train loss: 1.1787, eval loss 1.6808356046676636\n",
                  "Epoch 94 train loss: 1.1538, eval loss 1.6795673370361328\n",
                  "Epoch 95 train loss: 1.1687, eval loss 1.6786307096481323\n",
                  "Epoch 96 train loss: 1.1422, eval loss 1.678231120109558\n",
                  "Epoch 97 train loss: 1.1363, eval loss 1.6771743297576904\n",
                  "Epoch 98 train loss: 1.1769, eval loss 1.6762734651565552\n",
                  "Epoch 99 train loss: 1.1698, eval loss 1.6764785051345825\n",
                  "Epoch 100 train loss: 1.1813, eval loss 1.6746916770935059\n",
                  "Epoch 101 train loss: 1.1476, eval loss 1.6732432842254639\n",
                  "Epoch 102 train loss: 1.1461, eval loss 1.673274278640747\n",
                  "Epoch 103 train loss: 1.1441, eval loss 1.6724848747253418\n",
                  "Epoch 104 train loss: 1.1311, eval loss 1.6707789897918701\n",
                  "Epoch 105 train loss: 1.1491, eval loss 1.6712826490402222\n",
                  "Epoch 106 train loss: 1.1195, eval loss 1.6699340343475342\n",
                  "Epoch 107 train loss: 1.1670, eval loss 1.670265793800354\n",
                  "Epoch 108 train loss: 1.1391, eval loss 1.6682604551315308\n",
                  "Epoch 109 train loss: 1.1408, eval loss 1.6690868139266968\n",
                  "Epoch 110 train loss: 1.1395, eval loss 1.6675574779510498\n",
                  "Epoch 111 train loss: 1.1332, eval loss 1.6659936904907227\n",
                  "Epoch 112 train loss: 1.1455, eval loss 1.666291356086731\n",
                  "Epoch 113 train loss: 1.1245, eval loss 1.6643245220184326\n",
                  "Epoch 114 train loss: 1.1396, eval loss 1.6648701429367065\n",
                  "Epoch 115 train loss: 1.1124, eval loss 1.6624542474746704\n",
                  "Epoch 116 train loss: 1.1334, eval loss 1.662988305091858\n",
                  "Epoch 117 train loss: 1.1438, eval loss 1.6621911525726318\n",
                  "Epoch 118 train loss: 1.1469, eval loss 1.6606717109680176\n",
                  "Epoch 119 train loss: 1.1515, eval loss 1.6600812673568726\n",
                  "Epoch 120 train loss: 1.1130, eval loss 1.6594557762145996\n",
                  "Epoch 121 train loss: 1.1175, eval loss 1.6595051288604736\n",
                  "Epoch 122 train loss: 1.1588, eval loss 1.658706784248352\n",
                  "Epoch 123 train loss: 1.1192, eval loss 1.6584532260894775\n",
                  "Epoch 124 train loss: 1.1016, eval loss 1.6570827960968018\n",
                  "Epoch 125 train loss: 1.1240, eval loss 1.6569403409957886\n",
                  "Epoch 126 train loss: 1.1143, eval loss 1.6559182405471802\n",
                  "Epoch 127 train loss: 1.0916, eval loss 1.654486060142517\n",
                  "Epoch 128 train loss: 1.1237, eval loss 1.6539556980133057\n",
                  "Epoch 129 train loss: 1.1024, eval loss 1.653713583946228\n",
                  "Epoch 130 train loss: 1.1087, eval loss 1.652287483215332\n",
                  "Epoch 131 train loss: 1.0922, eval loss 1.652044415473938\n",
                  "Epoch 132 train loss: 1.1201, eval loss 1.6513556241989136\n",
                  "Epoch 133 train loss: 1.0793, eval loss 1.6516811847686768\n",
                  "Epoch 134 train loss: 1.0812, eval loss 1.6496145725250244\n",
                  "Epoch 135 train loss: 1.0750, eval loss 1.6495591402053833\n",
                  "Epoch 136 train loss: 1.1007, eval loss 1.6484277248382568\n",
                  "Epoch 137 train loss: 1.1189, eval loss 1.6487452983856201\n",
                  "Epoch 138 train loss: 1.0846, eval loss 1.6475638151168823\n",
                  "Epoch 139 train loss: 1.1092, eval loss 1.647822380065918\n",
                  "Epoch 140 train loss: 1.0838, eval loss 1.6465013027191162\n",
                  "Epoch 141 train loss: 1.0758, eval loss 1.6466832160949707\n",
                  "Epoch 142 train loss: 1.0683, eval loss 1.6454957723617554\n",
                  "Epoch 143 train loss: 1.0895, eval loss 1.643575668334961\n",
                  "Epoch 144 train loss: 1.0587, eval loss 1.6443095207214355\n",
                  "Epoch 145 train loss: 1.0496, eval loss 1.6430078744888306\n",
                  "Epoch 146 train loss: 1.0537, eval loss 1.6424428224563599\n",
                  "Epoch 147 train loss: 1.0492, eval loss 1.6427032947540283\n",
                  "Epoch 148 train loss: 1.1013, eval loss 1.6402353048324585\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:05:53,438] Trial 20 finished with value: 1.6402353048324585 and parameters: {'hidden_layers_size': 60, 'dropout_p': 0.3995040408348781, 'learning_rate': 3.318907011962408e-06, 'batch_size': 349, 'l2_reg': 0.00010193425277998233}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 1.0321, eval loss 1.6405385732650757\n",
                  "Epoch 0 train loss: 1.3830, eval loss 1.7371782064437866\n",
                  "Epoch 1 train loss: 1.2807, eval loss 1.696141004562378\n",
                  "Epoch 2 train loss: 1.2397, eval loss 1.6649166345596313\n",
                  "Epoch 3 train loss: 1.0758, eval loss 1.6402961015701294\n",
                  "Epoch 4 train loss: 1.0515, eval loss 1.620387315750122\n",
                  "Epoch 5 train loss: 1.0202, eval loss 1.605616807937622\n",
                  "Epoch 6 train loss: 0.9638, eval loss 1.5902318954467773\n",
                  "Epoch 7 train loss: 0.9073, eval loss 1.5799126625061035\n",
                  "Epoch 8 train loss: 0.8905, eval loss 1.5675199031829834\n",
                  "Epoch 9 train loss: 0.8480, eval loss 1.5578429698944092\n",
                  "Epoch 10 train loss: 0.9143, eval loss 1.5495572090148926\n",
                  "Epoch 11 train loss: 0.8841, eval loss 1.5412238836288452\n",
                  "Epoch 12 train loss: 0.8395, eval loss 1.5348477363586426\n",
                  "Epoch 13 train loss: 0.7609, eval loss 1.5291043519973755\n",
                  "Epoch 14 train loss: 0.7893, eval loss 1.522608757019043\n",
                  "Epoch 15 train loss: 0.7817, eval loss 1.5172967910766602\n",
                  "Epoch 16 train loss: 0.8228, eval loss 1.5120371580123901\n",
                  "Epoch 17 train loss: 0.7324, eval loss 1.507459044456482\n",
                  "Epoch 18 train loss: 0.8117, eval loss 1.504860281944275\n",
                  "Epoch 19 train loss: 0.7425, eval loss 1.5001507997512817\n",
                  "Epoch 20 train loss: 0.8056, eval loss 1.4965384006500244\n",
                  "Epoch 21 train loss: 0.7756, eval loss 1.4935815334320068\n",
                  "Epoch 22 train loss: 0.8186, eval loss 1.4898409843444824\n",
                  "Epoch 23 train loss: 0.7236, eval loss 1.4875209331512451\n",
                  "Epoch 24 train loss: 0.7971, eval loss 1.4843436479568481\n",
                  "Epoch 25 train loss: 0.7459, eval loss 1.482652187347412\n",
                  "Epoch 26 train loss: 0.7589, eval loss 1.4800506830215454\n",
                  "Epoch 27 train loss: 0.7395, eval loss 1.478090763092041\n",
                  "Epoch 28 train loss: 0.6652, eval loss 1.476558804512024\n",
                  "Epoch 29 train loss: 0.6782, eval loss 1.4741922616958618\n",
                  "Epoch 30 train loss: 0.7192, eval loss 1.4721797704696655\n",
                  "Epoch 31 train loss: 0.6831, eval loss 1.4708291292190552\n",
                  "Epoch 32 train loss: 0.6892, eval loss 1.4691131114959717\n",
                  "Epoch 33 train loss: 0.7128, eval loss 1.4678086042404175\n",
                  "Epoch 34 train loss: 0.7004, eval loss 1.467305064201355\n",
                  "Epoch 35 train loss: 0.7386, eval loss 1.4657089710235596\n",
                  "Epoch 36 train loss: 0.7261, eval loss 1.4636917114257812\n",
                  "Epoch 37 train loss: 0.7071, eval loss 1.463066577911377\n",
                  "Epoch 38 train loss: 0.7095, eval loss 1.4624637365341187\n",
                  "Epoch 39 train loss: 0.7304, eval loss 1.4615973234176636\n",
                  "Epoch 40 train loss: 0.7221, eval loss 1.459319829940796\n",
                  "Epoch 41 train loss: 0.6425, eval loss 1.4586422443389893\n",
                  "Epoch 42 train loss: 0.6327, eval loss 1.458749532699585\n",
                  "Epoch 43 train loss: 0.7070, eval loss 1.4565155506134033\n",
                  "Epoch 44 train loss: 0.6759, eval loss 1.4566681385040283\n",
                  "Epoch 45 train loss: 0.7634, eval loss 1.4554880857467651\n",
                  "Epoch 46 train loss: 0.6710, eval loss 1.4557381868362427\n",
                  "Epoch 47 train loss: 0.6936, eval loss 1.454275369644165\n",
                  "Epoch 48 train loss: 0.6504, eval loss 1.4536420106887817\n",
                  "Epoch 49 train loss: 0.7403, eval loss 1.4541763067245483\n",
                  "Epoch 50 train loss: 0.7858, eval loss 1.4527268409729004\n",
                  "Epoch 51 train loss: 0.7213, eval loss 1.4524339437484741\n",
                  "Epoch 52 train loss: 0.7364, eval loss 1.4510122537612915\n",
                  "Epoch 53 train loss: 0.6778, eval loss 1.4511215686798096\n",
                  "Epoch 54 train loss: 0.7029, eval loss 1.450154423713684\n",
                  "Epoch 55 train loss: 0.6977, eval loss 1.450442910194397\n",
                  "Epoch 56 train loss: 0.6777, eval loss 1.4497593641281128\n",
                  "Epoch 57 train loss: 0.6913, eval loss 1.4498450756072998\n",
                  "Epoch 58 train loss: 0.6838, eval loss 1.449217438697815\n",
                  "Epoch 59 train loss: 0.6872, eval loss 1.448731541633606\n",
                  "Epoch 60 train loss: 0.6833, eval loss 1.4477094411849976\n",
                  "Epoch 61 train loss: 0.7113, eval loss 1.4480429887771606\n",
                  "Epoch 62 train loss: 0.6599, eval loss 1.4478832483291626\n",
                  "Epoch 63 train loss: 0.6750, eval loss 1.4477041959762573\n",
                  "Epoch 64 train loss: 0.7093, eval loss 1.4466286897659302\n",
                  "Epoch 65 train loss: 0.6989, eval loss 1.4464609622955322\n",
                  "Epoch 66 train loss: 0.6696, eval loss 1.4470936059951782\n",
                  "Epoch 67 train loss: 0.7011, eval loss 1.4460997581481934\n",
                  "Epoch 68 train loss: 0.7000, eval loss 1.44597589969635\n",
                  "Epoch 69 train loss: 0.6781, eval loss 1.4454947710037231\n",
                  "Epoch 70 train loss: 0.6953, eval loss 1.4452099800109863\n",
                  "Epoch 71 train loss: 0.7324, eval loss 1.446273684501648\n",
                  "Epoch 72 train loss: 0.6983, eval loss 1.4451404809951782\n",
                  "Epoch 73 train loss: 0.7080, eval loss 1.4449023008346558\n",
                  "Epoch 74 train loss: 0.6968, eval loss 1.4447218179702759\n",
                  "Epoch 75 train loss: 0.6943, eval loss 1.4437730312347412\n",
                  "Epoch 76 train loss: 0.6719, eval loss 1.4441189765930176\n",
                  "Epoch 77 train loss: 0.6130, eval loss 1.4445521831512451\n",
                  "Epoch 78 train loss: 0.7207, eval loss 1.4442873001098633\n",
                  "Epoch 79 train loss: 0.6280, eval loss 1.4431986808776855\n",
                  "Epoch 80 train loss: 0.6880, eval loss 1.4425749778747559\n",
                  "Epoch 81 train loss: 0.6945, eval loss 1.4428850412368774\n",
                  "Epoch 82 train loss: 0.7365, eval loss 1.443195104598999\n",
                  "Epoch 83 train loss: 0.6768, eval loss 1.4433842897415161\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:06:39,212] Trial 21 finished with value: 1.4425749778747559 and parameters: {'hidden_layers_size': 210, 'dropout_p': 0.4443365775135407, 'learning_rate': 2.5065943882363546e-05, 'batch_size': 213, 'l2_reg': 1.3005984918832581e-05}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7789, eval loss 1.4850879907608032\n",
                  "Epoch 1 train loss: 0.7604, eval loss 1.4539666175842285\n",
                  "Epoch 2 train loss: 0.7526, eval loss 1.449983835220337\n",
                  "Epoch 3 train loss: 0.7159, eval loss 1.4458825588226318\n",
                  "Epoch 4 train loss: 0.6785, eval loss 1.4421112537384033\n",
                  "Epoch 5 train loss: 0.7153, eval loss 1.442028522491455\n",
                  "Epoch 6 train loss: 0.7099, eval loss 1.43940007686615\n",
                  "Epoch 7 train loss: 0.6742, eval loss 1.4411336183547974\n",
                  "Epoch 8 train loss: 0.6895, eval loss 1.4382628202438354\n",
                  "Epoch 9 train loss: 0.7108, eval loss 1.43754243850708\n",
                  "Epoch 10 train loss: 0.6746, eval loss 1.43584144115448\n",
                  "Epoch 11 train loss: 0.7077, eval loss 1.4382853507995605\n",
                  "Epoch 12 train loss: 0.7041, eval loss 1.4350024461746216\n",
                  "Epoch 13 train loss: 0.6717, eval loss 1.4353976249694824\n",
                  "Epoch 14 train loss: 0.6780, eval loss 1.43459951877594\n",
                  "Epoch 15 train loss: 0.6887, eval loss 1.4354734420776367\n",
                  "Epoch 16 train loss: 0.6831, eval loss 1.4336670637130737\n",
                  "Epoch 17 train loss: 0.6808, eval loss 1.4325919151306152\n",
                  "Epoch 18 train loss: 0.6926, eval loss 1.4353444576263428\n",
                  "Epoch 19 train loss: 0.6418, eval loss 1.4334651231765747\n",
                  "Epoch 20 train loss: 0.6602, eval loss 1.4313865900039673\n",
                  "Epoch 21 train loss: 0.6833, eval loss 1.4331157207489014\n",
                  "Epoch 22 train loss: 0.6615, eval loss 1.4316421747207642\n",
                  "Epoch 23 train loss: 0.6376, eval loss 1.4300856590270996\n",
                  "Epoch 24 train loss: 0.6509, eval loss 1.4318130016326904\n",
                  "Epoch 25 train loss: 0.6923, eval loss 1.429935097694397\n",
                  "Epoch 26 train loss: 0.6794, eval loss 1.4303407669067383\n",
                  "Epoch 27 train loss: 0.6736, eval loss 1.4311649799346924\n",
                  "Epoch 28 train loss: 0.6613, eval loss 1.427552580833435\n",
                  "Epoch 29 train loss: 0.6451, eval loss 1.4286506175994873\n",
                  "Epoch 30 train loss: 0.6639, eval loss 1.4300708770751953\n",
                  "Epoch 31 train loss: 0.6457, eval loss 1.4288045167922974\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:06:50,282] Trial 22 finished with value: 1.427552580833435 and parameters: {'hidden_layers_size': 69, 'dropout_p': 0.39611763614178486, 'learning_rate': 0.0024386476344047054, 'batch_size': 477, 'l2_reg': 1.4484273308865696e-05}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.2933, eval loss 1.7314453125\n",
                  "Epoch 1 train loss: 1.2086, eval loss 1.7016844749450684\n",
                  "Epoch 2 train loss: 1.1511, eval loss 1.6772552728652954\n",
                  "Epoch 3 train loss: 1.1418, eval loss 1.6586949825286865\n",
                  "Epoch 4 train loss: 1.0743, eval loss 1.6404985189437866\n",
                  "Epoch 5 train loss: 1.0518, eval loss 1.626481294631958\n",
                  "Epoch 6 train loss: 1.0078, eval loss 1.6103652715682983\n",
                  "Epoch 7 train loss: 0.9855, eval loss 1.5999683141708374\n",
                  "Epoch 8 train loss: 0.9664, eval loss 1.589551568031311\n",
                  "Epoch 9 train loss: 0.8701, eval loss 1.5822575092315674\n",
                  "Epoch 10 train loss: 0.9573, eval loss 1.572230339050293\n",
                  "Epoch 11 train loss: 0.8893, eval loss 1.565720558166504\n",
                  "Epoch 12 train loss: 0.9187, eval loss 1.5570095777511597\n",
                  "Epoch 13 train loss: 0.8952, eval loss 1.5520074367523193\n",
                  "Epoch 14 train loss: 0.8575, eval loss 1.5447314977645874\n",
                  "Epoch 15 train loss: 0.8216, eval loss 1.5409830808639526\n",
                  "Epoch 16 train loss: 0.8351, eval loss 1.5356281995773315\n",
                  "Epoch 17 train loss: 0.8560, eval loss 1.5290858745574951\n",
                  "Epoch 18 train loss: 0.8484, eval loss 1.5244874954223633\n",
                  "Epoch 19 train loss: 0.7885, eval loss 1.5213195085525513\n",
                  "Epoch 20 train loss: 0.7823, eval loss 1.5168616771697998\n",
                  "Epoch 21 train loss: 0.7777, eval loss 1.5125771760940552\n",
                  "Epoch 22 train loss: 0.8221, eval loss 1.508954644203186\n",
                  "Epoch 23 train loss: 0.8231, eval loss 1.5063139200210571\n",
                  "Epoch 24 train loss: 0.7748, eval loss 1.5028990507125854\n",
                  "Epoch 25 train loss: 0.7822, eval loss 1.49966561794281\n",
                  "Epoch 26 train loss: 0.8310, eval loss 1.4968448877334595\n",
                  "Epoch 27 train loss: 0.7944, eval loss 1.4940167665481567\n",
                  "Epoch 28 train loss: 0.7467, eval loss 1.4922462701797485\n",
                  "Epoch 29 train loss: 0.7548, eval loss 1.4895374774932861\n",
                  "Epoch 30 train loss: 0.7799, eval loss 1.4859609603881836\n",
                  "Epoch 31 train loss: 0.7857, eval loss 1.4844545125961304\n",
                  "Epoch 32 train loss: 0.7388, eval loss 1.4831383228302002\n",
                  "Epoch 33 train loss: 0.7884, eval loss 1.481903076171875\n",
                  "Epoch 34 train loss: 0.7442, eval loss 1.4790480136871338\n",
                  "Epoch 35 train loss: 0.7639, eval loss 1.47769033908844\n",
                  "Epoch 36 train loss: 0.7189, eval loss 1.4754191637039185\n",
                  "Epoch 37 train loss: 0.7762, eval loss 1.4740573167800903\n",
                  "Epoch 38 train loss: 0.7196, eval loss 1.4735445976257324\n",
                  "Epoch 39 train loss: 0.7620, eval loss 1.4718905687332153\n",
                  "Epoch 40 train loss: 0.7734, eval loss 1.4704569578170776\n",
                  "Epoch 41 train loss: 0.8021, eval loss 1.4694114923477173\n",
                  "Epoch 42 train loss: 0.7458, eval loss 1.4676167964935303\n",
                  "Epoch 43 train loss: 0.7411, eval loss 1.4679852724075317\n",
                  "Epoch 44 train loss: 0.7790, eval loss 1.4659152030944824\n",
                  "Epoch 45 train loss: 0.7481, eval loss 1.4651292562484741\n",
                  "Epoch 46 train loss: 0.7406, eval loss 1.4641214609146118\n",
                  "Epoch 47 train loss: 0.7638, eval loss 1.4639335870742798\n",
                  "Epoch 48 train loss: 0.7259, eval loss 1.4615176916122437\n",
                  "Epoch 49 train loss: 0.7600, eval loss 1.460246205329895\n",
                  "Epoch 50 train loss: 0.7205, eval loss 1.4593448638916016\n",
                  "Epoch 51 train loss: 0.7989, eval loss 1.4590893983840942\n",
                  "Epoch 52 train loss: 0.7171, eval loss 1.4590160846710205\n",
                  "Epoch 53 train loss: 0.7248, eval loss 1.4578484296798706\n",
                  "Epoch 54 train loss: 0.7197, eval loss 1.4569381475448608\n",
                  "Epoch 55 train loss: 0.7443, eval loss 1.4577845335006714\n",
                  "Epoch 56 train loss: 0.7698, eval loss 1.4570766687393188\n",
                  "Epoch 57 train loss: 0.7442, eval loss 1.4547001123428345\n",
                  "Epoch 58 train loss: 0.7085, eval loss 1.4548122882843018\n",
                  "Epoch 59 train loss: 0.7949, eval loss 1.454885482788086\n",
                  "Epoch 60 train loss: 0.6803, eval loss 1.4546256065368652\n",
                  "Epoch 61 train loss: 0.7052, eval loss 1.4536833763122559\n",
                  "Epoch 62 train loss: 0.7399, eval loss 1.4531315565109253\n",
                  "Epoch 63 train loss: 0.7978, eval loss 1.4535751342773438\n",
                  "Epoch 64 train loss: 0.6979, eval loss 1.4526584148406982\n",
                  "Epoch 65 train loss: 0.7059, eval loss 1.4524827003479004\n",
                  "Epoch 66 train loss: 0.7889, eval loss 1.4518046379089355\n",
                  "Epoch 67 train loss: 0.7202, eval loss 1.4511773586273193\n",
                  "Epoch 68 train loss: 0.6916, eval loss 1.4502146244049072\n",
                  "Epoch 69 train loss: 0.6757, eval loss 1.4505037069320679\n",
                  "Epoch 70 train loss: 0.7370, eval loss 1.4501161575317383\n",
                  "Epoch 71 train loss: 0.7789, eval loss 1.4514665603637695\n",
                  "Epoch 72 train loss: 0.7063, eval loss 1.4498069286346436\n",
                  "Epoch 73 train loss: 0.7829, eval loss 1.4498661756515503\n",
                  "Epoch 74 train loss: 0.7458, eval loss 1.4489243030548096\n",
                  "Epoch 75 train loss: 0.7434, eval loss 1.4487587213516235\n",
                  "Epoch 76 train loss: 0.7240, eval loss 1.4478939771652222\n",
                  "Epoch 77 train loss: 0.7324, eval loss 1.4483791589736938\n",
                  "Epoch 78 train loss: 0.7140, eval loss 1.4479044675827026\n",
                  "Epoch 79 train loss: 0.7669, eval loss 1.4485934972763062\n",
                  "Epoch 80 train loss: 0.7220, eval loss 1.447037935256958\n",
                  "Epoch 81 train loss: 0.7651, eval loss 1.4471873044967651\n",
                  "Epoch 82 train loss: 0.7510, eval loss 1.447634220123291\n",
                  "Epoch 83 train loss: 0.7383, eval loss 1.4464360475540161\n",
                  "Epoch 84 train loss: 0.7566, eval loss 1.446785569190979\n",
                  "Epoch 85 train loss: 0.7460, eval loss 1.4469836950302124\n",
                  "Epoch 86 train loss: 0.6711, eval loss 1.4456912279129028\n",
                  "Epoch 87 train loss: 0.7388, eval loss 1.44680655002594\n",
                  "Epoch 88 train loss: 0.7230, eval loss 1.4460053443908691\n",
                  "Epoch 89 train loss: 0.7506, eval loss 1.445310115814209\n",
                  "Epoch 90 train loss: 0.7094, eval loss 1.4455294609069824\n",
                  "Epoch 91 train loss: 0.7560, eval loss 1.445064663887024\n",
                  "Epoch 92 train loss: 0.6895, eval loss 1.4452184438705444\n",
                  "Epoch 93 train loss: 0.7833, eval loss 1.444801688194275\n",
                  "Epoch 94 train loss: 0.7490, eval loss 1.4443764686584473\n",
                  "Epoch 95 train loss: 0.7167, eval loss 1.4450417757034302\n",
                  "Epoch 96 train loss: 0.7436, eval loss 1.4453158378601074\n",
                  "Epoch 97 train loss: 0.7482, eval loss 1.4450098276138306\n",
                  "Epoch 98 train loss: 0.7916, eval loss 1.4441262483596802\n",
                  "Epoch 99 train loss: 0.6936, eval loss 1.4437282085418701\n",
                  "Epoch 100 train loss: 0.7566, eval loss 1.4439878463745117\n",
                  "Epoch 101 train loss: 0.7828, eval loss 1.4444514513015747\n",
                  "Epoch 102 train loss: 0.7451, eval loss 1.4440929889678955\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:08:02,301] Trial 23 finished with value: 1.4437282085418701 and parameters: {'hidden_layers_size': 204, 'dropout_p': 0.3859481194452074, 'learning_rate': 1.2170940438495193e-05, 'batch_size': 138, 'l2_reg': 3.185550507964966e-05}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 0.7436, eval loss 1.444145917892456\n",
                  "Epoch 1 train loss: 0.8436, eval loss 1.444703459739685\n",
                  "Epoch 2 train loss: 0.7930, eval loss 1.4479087591171265\n",
                  "Epoch 3 train loss: 0.6846, eval loss 1.4490834474563599\n",
                  "Epoch 4 train loss: 0.6191, eval loss 1.442521572113037\n",
                  "Epoch 5 train loss: 0.6479, eval loss 1.4385219812393188\n",
                  "Epoch 6 train loss: 0.6806, eval loss 1.451742172241211\n",
                  "Epoch 7 train loss: 0.6858, eval loss 1.4419848918914795\n",
                  "Epoch 8 train loss: 0.6659, eval loss 1.442987084388733\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:08:06,649] Trial 24 finished with value: 1.4385219812393188 and parameters: {'hidden_layers_size': 95, 'dropout_p': 0.27173970048254203, 'learning_rate': 0.09051468506534932, 'batch_size': 269, 'l2_reg': 0.0009163162083524767}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.3708, eval loss 1.7360703945159912\n",
                  "Epoch 1 train loss: 1.3578, eval loss 1.7321337461471558\n",
                  "Epoch 2 train loss: 1.3349, eval loss 1.7258684635162354\n",
                  "Epoch 3 train loss: 1.3180, eval loss 1.7237709760665894\n",
                  "Epoch 4 train loss: 1.3030, eval loss 1.7189867496490479\n",
                  "Epoch 5 train loss: 1.2951, eval loss 1.716554880142212\n",
                  "Epoch 6 train loss: 1.3146, eval loss 1.7115845680236816\n",
                  "Epoch 7 train loss: 1.2898, eval loss 1.7095144987106323\n",
                  "Epoch 8 train loss: 1.2528, eval loss 1.7061238288879395\n",
                  "Epoch 9 train loss: 1.2883, eval loss 1.7024821043014526\n",
                  "Epoch 10 train loss: 1.2450, eval loss 1.698730230331421\n",
                  "Epoch 11 train loss: 1.2515, eval loss 1.6951680183410645\n",
                  "Epoch 12 train loss: 1.2675, eval loss 1.6933263540267944\n",
                  "Epoch 13 train loss: 1.2367, eval loss 1.6891024112701416\n",
                  "Epoch 14 train loss: 1.2217, eval loss 1.6853199005126953\n",
                  "Epoch 15 train loss: 1.1748, eval loss 1.683685064315796\n",
                  "Epoch 16 train loss: 1.1975, eval loss 1.680625081062317\n",
                  "Epoch 17 train loss: 1.2074, eval loss 1.677560806274414\n",
                  "Epoch 18 train loss: 1.1550, eval loss 1.6745673418045044\n",
                  "Epoch 19 train loss: 1.1884, eval loss 1.6722434759140015\n",
                  "Epoch 20 train loss: 1.1453, eval loss 1.6683977842330933\n",
                  "Epoch 21 train loss: 1.1359, eval loss 1.6665650606155396\n",
                  "Epoch 22 train loss: 1.1677, eval loss 1.6652483940124512\n",
                  "Epoch 23 train loss: 1.0926, eval loss 1.6610538959503174\n",
                  "Epoch 24 train loss: 1.1369, eval loss 1.66008722782135\n",
                  "Epoch 25 train loss: 1.1572, eval loss 1.6559935808181763\n",
                  "Epoch 26 train loss: 1.1314, eval loss 1.654019832611084\n",
                  "Epoch 27 train loss: 1.1496, eval loss 1.6519497632980347\n",
                  "Epoch 28 train loss: 1.1188, eval loss 1.649198293685913\n",
                  "Epoch 29 train loss: 1.0782, eval loss 1.6470011472702026\n",
                  "Epoch 30 train loss: 1.1081, eval loss 1.644824743270874\n",
                  "Epoch 31 train loss: 1.1030, eval loss 1.6430189609527588\n",
                  "Epoch 32 train loss: 1.0869, eval loss 1.641332745552063\n",
                  "Epoch 33 train loss: 1.0777, eval loss 1.6384252309799194\n",
                  "Epoch 34 train loss: 1.0562, eval loss 1.6362366676330566\n",
                  "Epoch 35 train loss: 1.0798, eval loss 1.6357637643814087\n",
                  "Epoch 36 train loss: 1.0279, eval loss 1.632226586341858\n",
                  "Epoch 37 train loss: 1.0715, eval loss 1.6307013034820557\n",
                  "Epoch 38 train loss: 1.0464, eval loss 1.628829002380371\n",
                  "Epoch 39 train loss: 1.0406, eval loss 1.627294659614563\n",
                  "Epoch 40 train loss: 1.0350, eval loss 1.6247968673706055\n",
                  "Epoch 41 train loss: 1.0280, eval loss 1.6235599517822266\n",
                  "Epoch 42 train loss: 1.0168, eval loss 1.6227021217346191\n",
                  "Epoch 43 train loss: 0.9929, eval loss 1.6205832958221436\n",
                  "Epoch 44 train loss: 1.0070, eval loss 1.6185632944107056\n",
                  "Epoch 45 train loss: 1.0327, eval loss 1.6170443296432495\n",
                  "Epoch 46 train loss: 0.9799, eval loss 1.6152195930480957\n",
                  "Epoch 47 train loss: 0.9733, eval loss 1.6134679317474365\n",
                  "Epoch 48 train loss: 1.0257, eval loss 1.6127493381500244\n",
                  "Epoch 49 train loss: 0.9724, eval loss 1.610945224761963\n",
                  "Epoch 50 train loss: 1.0188, eval loss 1.610106348991394\n",
                  "Epoch 51 train loss: 0.9704, eval loss 1.6073914766311646\n",
                  "Epoch 52 train loss: 0.9976, eval loss 1.6057389974594116\n",
                  "Epoch 53 train loss: 0.9964, eval loss 1.6050482988357544\n",
                  "Epoch 54 train loss: 0.9763, eval loss 1.6034826040267944\n",
                  "Epoch 55 train loss: 0.9595, eval loss 1.6009557247161865\n",
                  "Epoch 56 train loss: 0.9747, eval loss 1.6006052494049072\n",
                  "Epoch 57 train loss: 0.9897, eval loss 1.5983885526657104\n",
                  "Epoch 58 train loss: 0.9497, eval loss 1.598095178604126\n",
                  "Epoch 59 train loss: 0.9799, eval loss 1.5960487127304077\n",
                  "Epoch 60 train loss: 0.9712, eval loss 1.5952479839324951\n",
                  "Epoch 61 train loss: 0.9925, eval loss 1.5937974452972412\n",
                  "Epoch 62 train loss: 0.9587, eval loss 1.5937669277191162\n",
                  "Epoch 63 train loss: 0.9203, eval loss 1.5907262563705444\n",
                  "Epoch 64 train loss: 0.9473, eval loss 1.5899640321731567\n",
                  "Epoch 65 train loss: 0.9472, eval loss 1.5890827178955078\n",
                  "Epoch 66 train loss: 0.9388, eval loss 1.5872563123703003\n",
                  "Epoch 67 train loss: 0.9390, eval loss 1.5861186981201172\n",
                  "Epoch 68 train loss: 0.9723, eval loss 1.5846681594848633\n",
                  "Epoch 69 train loss: 0.9358, eval loss 1.5846970081329346\n",
                  "Epoch 70 train loss: 0.9463, eval loss 1.5825523138046265\n",
                  "Epoch 71 train loss: 0.9381, eval loss 1.5819337368011475\n",
                  "Epoch 72 train loss: 0.9376, eval loss 1.5813071727752686\n",
                  "Epoch 73 train loss: 0.9228, eval loss 1.5794758796691895\n",
                  "Epoch 74 train loss: 0.9484, eval loss 1.5789446830749512\n",
                  "Epoch 75 train loss: 0.9309, eval loss 1.576988935470581\n",
                  "Epoch 76 train loss: 0.9232, eval loss 1.576200008392334\n",
                  "Epoch 77 train loss: 0.9161, eval loss 1.574525237083435\n",
                  "Epoch 78 train loss: 0.9058, eval loss 1.5743308067321777\n",
                  "Epoch 79 train loss: 0.9044, eval loss 1.5732382535934448\n",
                  "Epoch 80 train loss: 0.9012, eval loss 1.5716617107391357\n",
                  "Epoch 81 train loss: 0.8817, eval loss 1.5706137418746948\n",
                  "Epoch 82 train loss: 0.9153, eval loss 1.5698578357696533\n",
                  "Epoch 83 train loss: 0.9137, eval loss 1.5692652463912964\n",
                  "Epoch 84 train loss: 0.8915, eval loss 1.568211317062378\n",
                  "Epoch 85 train loss: 0.9065, eval loss 1.5670888423919678\n",
                  "Epoch 86 train loss: 0.8893, eval loss 1.5666111707687378\n",
                  "Epoch 87 train loss: 0.8902, eval loss 1.5653523206710815\n",
                  "Epoch 88 train loss: 0.8638, eval loss 1.5636107921600342\n",
                  "Epoch 89 train loss: 0.8591, eval loss 1.5634231567382812\n",
                  "Epoch 90 train loss: 0.8796, eval loss 1.5618677139282227\n",
                  "Epoch 91 train loss: 0.8822, eval loss 1.562185525894165\n",
                  "Epoch 92 train loss: 0.8896, eval loss 1.5607881546020508\n",
                  "Epoch 93 train loss: 0.8551, eval loss 1.56024169921875\n",
                  "Epoch 94 train loss: 0.8659, eval loss 1.5586429834365845\n",
                  "Epoch 95 train loss: 0.8657, eval loss 1.558495283126831\n",
                  "Epoch 96 train loss: 0.8246, eval loss 1.5575352907180786\n",
                  "Epoch 97 train loss: 0.8555, eval loss 1.5561387538909912\n",
                  "Epoch 98 train loss: 0.8651, eval loss 1.5555236339569092\n",
                  "Epoch 99 train loss: 0.8666, eval loss 1.5545153617858887\n",
                  "Epoch 100 train loss: 0.8654, eval loss 1.5537381172180176\n",
                  "Epoch 101 train loss: 0.8612, eval loss 1.552303671836853\n",
                  "Epoch 102 train loss: 0.8629, eval loss 1.5525825023651123\n",
                  "Epoch 103 train loss: 0.8460, eval loss 1.5503541231155396\n",
                  "Epoch 104 train loss: 0.8938, eval loss 1.550878882408142\n",
                  "Epoch 105 train loss: 0.8751, eval loss 1.5490264892578125\n",
                  "Epoch 106 train loss: 0.8518, eval loss 1.548208236694336\n",
                  "Epoch 107 train loss: 0.8501, eval loss 1.5484908819198608\n",
                  "Epoch 108 train loss: 0.8683, eval loss 1.5476917028427124\n",
                  "Epoch 109 train loss: 0.8679, eval loss 1.5474627017974854\n",
                  "Epoch 110 train loss: 0.8312, eval loss 1.5455570220947266\n",
                  "Epoch 111 train loss: 0.8590, eval loss 1.5459227561950684\n",
                  "Epoch 112 train loss: 0.8380, eval loss 1.54440176486969\n",
                  "Epoch 113 train loss: 0.8421, eval loss 1.5434681177139282\n",
                  "Epoch 114 train loss: 0.8584, eval loss 1.542917013168335\n",
                  "Epoch 115 train loss: 0.8197, eval loss 1.542732834815979\n",
                  "Epoch 116 train loss: 0.8582, eval loss 1.5418788194656372\n",
                  "Epoch 117 train loss: 0.8248, eval loss 1.5403538942337036\n",
                  "Epoch 118 train loss: 0.8513, eval loss 1.5399081707000732\n",
                  "Epoch 119 train loss: 0.8410, eval loss 1.5390794277191162\n",
                  "Epoch 120 train loss: 0.8176, eval loss 1.539129376411438\n",
                  "Epoch 121 train loss: 0.8444, eval loss 1.538482427597046\n",
                  "Epoch 122 train loss: 0.8597, eval loss 1.5372287034988403\n",
                  "Epoch 123 train loss: 0.8190, eval loss 1.537466049194336\n",
                  "Epoch 124 train loss: 0.8434, eval loss 1.5358333587646484\n",
                  "Epoch 125 train loss: 0.8108, eval loss 1.53514564037323\n",
                  "Epoch 126 train loss: 0.8337, eval loss 1.5348176956176758\n",
                  "Epoch 127 train loss: 0.8301, eval loss 1.534924030303955\n",
                  "Epoch 128 train loss: 0.8098, eval loss 1.5327378511428833\n",
                  "Epoch 129 train loss: 0.8133, eval loss 1.5317471027374268\n",
                  "Epoch 130 train loss: 0.8178, eval loss 1.5316686630249023\n",
                  "Epoch 131 train loss: 0.8251, eval loss 1.5313774347305298\n",
                  "Epoch 132 train loss: 0.8363, eval loss 1.52985680103302\n",
                  "Epoch 133 train loss: 0.8435, eval loss 1.529902696609497\n",
                  "Epoch 134 train loss: 0.8291, eval loss 1.52958345413208\n",
                  "Epoch 135 train loss: 0.8346, eval loss 1.5294675827026367\n",
                  "Epoch 136 train loss: 0.8465, eval loss 1.5288172960281372\n",
                  "Epoch 137 train loss: 0.8037, eval loss 1.5281198024749756\n",
                  "Epoch 138 train loss: 0.8421, eval loss 1.5276241302490234\n",
                  "Epoch 139 train loss: 0.7915, eval loss 1.5263813734054565\n",
                  "Epoch 140 train loss: 0.8173, eval loss 1.5263419151306152\n",
                  "Epoch 141 train loss: 0.8458, eval loss 1.5250763893127441\n",
                  "Epoch 142 train loss: 0.8351, eval loss 1.524495005607605\n",
                  "Epoch 143 train loss: 0.8030, eval loss 1.5250691175460815\n",
                  "Epoch 144 train loss: 0.8196, eval loss 1.523294448852539\n",
                  "Epoch 145 train loss: 0.8131, eval loss 1.5220526456832886\n",
                  "Epoch 146 train loss: 0.8256, eval loss 1.5219939947128296\n",
                  "Epoch 147 train loss: 0.8102, eval loss 1.5215859413146973\n",
                  "Epoch 148 train loss: 0.8052, eval loss 1.521398663520813\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:09:12,085] Trial 25 finished with value: 1.5209039449691772 and parameters: {'hidden_layers_size': 203, 'dropout_p': 0.35752682496918486, 'learning_rate': 2.433415539418087e-06, 'batch_size': 343, 'l2_reg': 4.730500605689594e-05}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.7997, eval loss 1.5209039449691772\n",
                  "Epoch 0 train loss: 0.7978, eval loss 1.520890712738037\n",
                  "Epoch 1 train loss: 0.8423, eval loss 1.4739967584609985\n",
                  "Epoch 2 train loss: 0.7315, eval loss 1.4575177431106567\n",
                  "Epoch 3 train loss: 0.7507, eval loss 1.44858980178833\n",
                  "Epoch 4 train loss: 0.6716, eval loss 1.4477856159210205\n",
                  "Epoch 5 train loss: 0.6811, eval loss 1.4423903226852417\n",
                  "Epoch 6 train loss: 0.7426, eval loss 1.4406148195266724\n",
                  "Epoch 7 train loss: 0.7338, eval loss 1.4413203001022339\n",
                  "Epoch 8 train loss: 0.7077, eval loss 1.4374449253082275\n",
                  "Epoch 9 train loss: 0.6267, eval loss 1.4368637800216675\n",
                  "Epoch 10 train loss: 0.6399, eval loss 1.437151312828064\n",
                  "Epoch 11 train loss: 0.6540, eval loss 1.4365317821502686\n",
                  "Epoch 12 train loss: 0.6476, eval loss 1.4364378452301025\n",
                  "Epoch 13 train loss: 0.7015, eval loss 1.4362049102783203\n",
                  "Epoch 14 train loss: 0.6631, eval loss 1.4352513551712036\n",
                  "Epoch 15 train loss: 0.7148, eval loss 1.4346340894699097\n",
                  "Epoch 16 train loss: 0.6838, eval loss 1.4338067770004272\n",
                  "Epoch 17 train loss: 0.6309, eval loss 1.432753324508667\n",
                  "Epoch 18 train loss: 0.6038, eval loss 1.433260440826416\n",
                  "Epoch 19 train loss: 0.6567, eval loss 1.4338245391845703\n",
                  "Epoch 20 train loss: 0.7021, eval loss 1.4334853887557983\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:09:22,357] Trial 26 finished with value: 1.432753324508667 and parameters: {'hidden_layers_size': 146, 'dropout_p': 0.4437772399358891, 'learning_rate': 0.000583487635507599, 'batch_size': 259, 'l2_reg': 1.0446072518469856e-05}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.4334, eval loss 1.7583783864974976\n",
                  "Epoch 1 train loss: 1.3669, eval loss 1.7524921894073486\n",
                  "Epoch 2 train loss: 1.4176, eval loss 1.7479381561279297\n",
                  "Epoch 3 train loss: 1.3921, eval loss 1.7434273958206177\n",
                  "Epoch 4 train loss: 1.3814, eval loss 1.7381194829940796\n",
                  "Epoch 5 train loss: 1.4211, eval loss 1.7342267036437988\n",
                  "Epoch 6 train loss: 1.3112, eval loss 1.7300132513046265\n",
                  "Epoch 7 train loss: 1.4159, eval loss 1.7261805534362793\n",
                  "Epoch 8 train loss: 1.4038, eval loss 1.7190601825714111\n",
                  "Epoch 9 train loss: 1.3156, eval loss 1.7181940078735352\n",
                  "Epoch 10 train loss: 1.3577, eval loss 1.7129648923873901\n",
                  "Epoch 11 train loss: 1.3491, eval loss 1.7095139026641846\n",
                  "Epoch 12 train loss: 1.2772, eval loss 1.7057806253433228\n",
                  "Epoch 13 train loss: 1.3048, eval loss 1.7013863325119019\n",
                  "Epoch 14 train loss: 1.2782, eval loss 1.6980935335159302\n",
                  "Epoch 15 train loss: 1.2805, eval loss 1.6947656869888306\n",
                  "Epoch 16 train loss: 1.2739, eval loss 1.692008137702942\n",
                  "Epoch 17 train loss: 1.2743, eval loss 1.6860318183898926\n",
                  "Epoch 18 train loss: 1.2573, eval loss 1.684167504310608\n",
                  "Epoch 19 train loss: 1.2292, eval loss 1.6822373867034912\n",
                  "Epoch 20 train loss: 1.2975, eval loss 1.6773804426193237\n",
                  "Epoch 21 train loss: 1.2292, eval loss 1.674608588218689\n",
                  "Epoch 22 train loss: 1.1688, eval loss 1.671287178993225\n",
                  "Epoch 23 train loss: 1.1491, eval loss 1.667820692062378\n",
                  "Epoch 24 train loss: 1.1818, eval loss 1.6664527654647827\n",
                  "Epoch 25 train loss: 1.1304, eval loss 1.6645265817642212\n",
                  "Epoch 26 train loss: 1.1789, eval loss 1.6605076789855957\n",
                  "Epoch 27 train loss: 1.1755, eval loss 1.657731294631958\n",
                  "Epoch 28 train loss: 1.2083, eval loss 1.6560872793197632\n",
                  "Epoch 29 train loss: 1.1150, eval loss 1.6527690887451172\n",
                  "Epoch 30 train loss: 1.1334, eval loss 1.6489778757095337\n",
                  "Epoch 31 train loss: 1.0917, eval loss 1.6460120677947998\n",
                  "Epoch 32 train loss: 1.0909, eval loss 1.6449304819107056\n",
                  "Epoch 33 train loss: 1.1074, eval loss 1.6410412788391113\n",
                  "Epoch 34 train loss: 1.1288, eval loss 1.6393193006515503\n",
                  "Epoch 35 train loss: 1.0620, eval loss 1.6380263566970825\n",
                  "Epoch 36 train loss: 1.0993, eval loss 1.6361957788467407\n",
                  "Epoch 37 train loss: 1.1082, eval loss 1.6328718662261963\n",
                  "Epoch 38 train loss: 1.0995, eval loss 1.6311753988265991\n",
                  "Epoch 39 train loss: 1.0345, eval loss 1.6282734870910645\n",
                  "Epoch 40 train loss: 1.1397, eval loss 1.6278085708618164\n",
                  "Epoch 41 train loss: 1.1025, eval loss 1.6257224082946777\n",
                  "Epoch 42 train loss: 1.0115, eval loss 1.62129807472229\n",
                  "Epoch 43 train loss: 1.0283, eval loss 1.620538353919983\n",
                  "Epoch 44 train loss: 1.0295, eval loss 1.6178863048553467\n",
                  "Epoch 45 train loss: 1.0121, eval loss 1.6149168014526367\n",
                  "Epoch 46 train loss: 1.0236, eval loss 1.6151485443115234\n",
                  "Epoch 47 train loss: 0.9728, eval loss 1.6118390560150146\n",
                  "Epoch 48 train loss: 1.0301, eval loss 1.610077142715454\n",
                  "Epoch 49 train loss: 1.0219, eval loss 1.6084773540496826\n",
                  "Epoch 50 train loss: 0.9951, eval loss 1.6073795557022095\n",
                  "Epoch 51 train loss: 1.0261, eval loss 1.6057225465774536\n",
                  "Epoch 52 train loss: 1.0137, eval loss 1.6044951677322388\n",
                  "Epoch 53 train loss: 1.0694, eval loss 1.6018093824386597\n",
                  "Epoch 54 train loss: 1.0368, eval loss 1.6001956462860107\n",
                  "Epoch 55 train loss: 0.9946, eval loss 1.597162127494812\n",
                  "Epoch 56 train loss: 0.9781, eval loss 1.5960116386413574\n",
                  "Epoch 57 train loss: 0.9896, eval loss 1.5954686403274536\n",
                  "Epoch 58 train loss: 1.0516, eval loss 1.5914775133132935\n",
                  "Epoch 59 train loss: 0.9632, eval loss 1.5927331447601318\n",
                  "Epoch 60 train loss: 0.9707, eval loss 1.5907833576202393\n",
                  "Epoch 61 train loss: 0.9590, eval loss 1.587378740310669\n",
                  "Epoch 62 train loss: 0.9759, eval loss 1.5872622728347778\n",
                  "Epoch 63 train loss: 0.9996, eval loss 1.5849937200546265\n",
                  "Epoch 64 train loss: 0.9842, eval loss 1.5844899415969849\n",
                  "Epoch 65 train loss: 0.9320, eval loss 1.5828194618225098\n",
                  "Epoch 66 train loss: 0.9252, eval loss 1.5798921585083008\n",
                  "Epoch 67 train loss: 0.9871, eval loss 1.5799452066421509\n",
                  "Epoch 68 train loss: 0.9606, eval loss 1.5766404867172241\n",
                  "Epoch 69 train loss: 0.9584, eval loss 1.5771677494049072\n",
                  "Epoch 70 train loss: 0.9426, eval loss 1.5754637718200684\n",
                  "Epoch 71 train loss: 0.9320, eval loss 1.5753040313720703\n",
                  "Epoch 72 train loss: 0.9047, eval loss 1.57326078414917\n",
                  "Epoch 73 train loss: 0.9052, eval loss 1.5717278718948364\n",
                  "Epoch 74 train loss: 0.9895, eval loss 1.5695232152938843\n",
                  "Epoch 75 train loss: 0.9097, eval loss 1.5683350563049316\n",
                  "Epoch 76 train loss: 0.9083, eval loss 1.5668282508850098\n",
                  "Epoch 77 train loss: 0.9024, eval loss 1.5660037994384766\n",
                  "Epoch 78 train loss: 0.9232, eval loss 1.5647563934326172\n",
                  "Epoch 79 train loss: 0.9421, eval loss 1.564778208732605\n",
                  "Epoch 80 train loss: 0.8359, eval loss 1.563469409942627\n",
                  "Epoch 81 train loss: 0.9553, eval loss 1.5602777004241943\n",
                  "Epoch 82 train loss: 0.9598, eval loss 1.559686303138733\n",
                  "Epoch 83 train loss: 0.8979, eval loss 1.558406114578247\n",
                  "Epoch 84 train loss: 0.9169, eval loss 1.5564568042755127\n",
                  "Epoch 85 train loss: 0.8799, eval loss 1.5562711954116821\n",
                  "Epoch 86 train loss: 0.9368, eval loss 1.5544480085372925\n",
                  "Epoch 87 train loss: 0.9065, eval loss 1.5537196397781372\n",
                  "Epoch 88 train loss: 0.9343, eval loss 1.5531175136566162\n",
                  "Epoch 89 train loss: 0.9318, eval loss 1.5515038967132568\n",
                  "Epoch 90 train loss: 0.9557, eval loss 1.5496736764907837\n",
                  "Epoch 91 train loss: 0.8506, eval loss 1.5482990741729736\n",
                  "Epoch 92 train loss: 0.8663, eval loss 1.5471982955932617\n",
                  "Epoch 93 train loss: 0.9369, eval loss 1.5480973720550537\n",
                  "Epoch 94 train loss: 0.8799, eval loss 1.5449488162994385\n",
                  "Epoch 95 train loss: 0.8854, eval loss 1.5459569692611694\n",
                  "Epoch 96 train loss: 0.9225, eval loss 1.544456124305725\n",
                  "Epoch 97 train loss: 0.8929, eval loss 1.5438028573989868\n",
                  "Epoch 98 train loss: 0.8839, eval loss 1.5424209833145142\n",
                  "Epoch 99 train loss: 0.8904, eval loss 1.5415749549865723\n",
                  "Epoch 100 train loss: 0.8734, eval loss 1.5404959917068481\n",
                  "Epoch 101 train loss: 0.8671, eval loss 1.539463758468628\n",
                  "Epoch 102 train loss: 0.8918, eval loss 1.5383927822113037\n",
                  "Epoch 103 train loss: 0.8759, eval loss 1.5366066694259644\n",
                  "Epoch 104 train loss: 0.9128, eval loss 1.5363173484802246\n",
                  "Epoch 105 train loss: 0.8662, eval loss 1.5359147787094116\n",
                  "Epoch 106 train loss: 0.8567, eval loss 1.5350338220596313\n",
                  "Epoch 107 train loss: 0.8853, eval loss 1.534287691116333\n",
                  "Epoch 108 train loss: 0.8308, eval loss 1.532236099243164\n",
                  "Epoch 109 train loss: 0.8848, eval loss 1.5310672521591187\n",
                  "Epoch 110 train loss: 0.9169, eval loss 1.5307579040527344\n",
                  "Epoch 111 train loss: 0.8719, eval loss 1.5297651290893555\n",
                  "Epoch 112 train loss: 0.8916, eval loss 1.529790997505188\n",
                  "Epoch 113 train loss: 0.8841, eval loss 1.5281336307525635\n",
                  "Epoch 114 train loss: 0.8916, eval loss 1.5267508029937744\n",
                  "Epoch 115 train loss: 0.8412, eval loss 1.5269895792007446\n",
                  "Epoch 116 train loss: 0.8612, eval loss 1.5248217582702637\n",
                  "Epoch 117 train loss: 0.8260, eval loss 1.523250699043274\n",
                  "Epoch 118 train loss: 0.8270, eval loss 1.5225390195846558\n",
                  "Epoch 119 train loss: 0.8430, eval loss 1.523844599723816\n",
                  "Epoch 120 train loss: 0.8811, eval loss 1.5220171213150024\n",
                  "Epoch 121 train loss: 0.8925, eval loss 1.5213176012039185\n",
                  "Epoch 122 train loss: 0.8760, eval loss 1.5206444263458252\n",
                  "Epoch 123 train loss: 0.8474, eval loss 1.5194451808929443\n",
                  "Epoch 124 train loss: 0.8576, eval loss 1.5186673402786255\n",
                  "Epoch 125 train loss: 0.8847, eval loss 1.517512559890747\n",
                  "Epoch 126 train loss: 0.8438, eval loss 1.5171189308166504\n",
                  "Epoch 127 train loss: 0.8678, eval loss 1.5165849924087524\n",
                  "Epoch 128 train loss: 0.8820, eval loss 1.5157948732376099\n",
                  "Epoch 129 train loss: 0.8486, eval loss 1.514670729637146\n",
                  "Epoch 130 train loss: 0.8726, eval loss 1.513278603553772\n",
                  "Epoch 131 train loss: 0.8621, eval loss 1.5133179426193237\n",
                  "Epoch 132 train loss: 0.7769, eval loss 1.5129404067993164\n",
                  "Epoch 133 train loss: 0.8120, eval loss 1.5132389068603516\n",
                  "Epoch 134 train loss: 0.8280, eval loss 1.511102557182312\n",
                  "Epoch 135 train loss: 0.8489, eval loss 1.5113794803619385\n",
                  "Epoch 136 train loss: 0.8019, eval loss 1.5103029012680054\n",
                  "Epoch 137 train loss: 0.8137, eval loss 1.5094672441482544\n",
                  "Epoch 138 train loss: 0.8328, eval loss 1.5080891847610474\n",
                  "Epoch 139 train loss: 0.8336, eval loss 1.5068824291229248\n",
                  "Epoch 140 train loss: 0.8769, eval loss 1.506847858428955\n",
                  "Epoch 141 train loss: 0.8006, eval loss 1.5076401233673096\n",
                  "Epoch 142 train loss: 0.8073, eval loss 1.5061432123184204\n",
                  "Epoch 143 train loss: 0.7893, eval loss 1.506118893623352\n",
                  "Epoch 144 train loss: 0.7861, eval loss 1.5063890218734741\n",
                  "Epoch 145 train loss: 0.8596, eval loss 1.504811406135559\n",
                  "Epoch 146 train loss: 0.8299, eval loss 1.504340410232544\n",
                  "Epoch 147 train loss: 0.8273, eval loss 1.5033886432647705\n",
                  "Epoch 148 train loss: 0.8032, eval loss 1.5021169185638428\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:10:40,224] Trial 27 finished with value: 1.5021169185638428 and parameters: {'hidden_layers_size': 147, 'dropout_p': 0.4464801354971336, 'learning_rate': 3.448691274811924e-06, 'batch_size': 207, 'l2_reg': 0.0002531025277093918}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.8266, eval loss 1.5026029348373413\n",
                  "Epoch 0 train loss: 1.4034, eval loss 1.751881718635559\n",
                  "Epoch 1 train loss: 1.3996, eval loss 1.7429258823394775\n",
                  "Epoch 2 train loss: 1.3324, eval loss 1.7408287525177002\n",
                  "Epoch 3 train loss: 1.3439, eval loss 1.7373714447021484\n",
                  "Epoch 4 train loss: 1.2854, eval loss 1.7330735921859741\n",
                  "Epoch 5 train loss: 1.3168, eval loss 1.728844404220581\n",
                  "Epoch 6 train loss: 1.3498, eval loss 1.7265520095825195\n",
                  "Epoch 7 train loss: 1.3167, eval loss 1.7220971584320068\n",
                  "Epoch 8 train loss: 1.2975, eval loss 1.7191718816757202\n",
                  "Epoch 9 train loss: 1.2581, eval loss 1.7148677110671997\n",
                  "Epoch 10 train loss: 1.2808, eval loss 1.7130153179168701\n",
                  "Epoch 11 train loss: 1.2857, eval loss 1.7096906900405884\n",
                  "Epoch 12 train loss: 1.2542, eval loss 1.7076174020767212\n",
                  "Epoch 13 train loss: 1.2420, eval loss 1.7028135061264038\n",
                  "Epoch 14 train loss: 1.2256, eval loss 1.7004765272140503\n",
                  "Epoch 15 train loss: 1.2199, eval loss 1.6972907781600952\n",
                  "Epoch 16 train loss: 1.2219, eval loss 1.6942501068115234\n",
                  "Epoch 17 train loss: 1.1683, eval loss 1.6901437044143677\n",
                  "Epoch 18 train loss: 1.1967, eval loss 1.6881163120269775\n",
                  "Epoch 19 train loss: 1.2162, eval loss 1.6865228414535522\n",
                  "Epoch 20 train loss: 1.1743, eval loss 1.6828641891479492\n",
                  "Epoch 21 train loss: 1.1666, eval loss 1.6805635690689087\n",
                  "Epoch 22 train loss: 1.1721, eval loss 1.6774054765701294\n",
                  "Epoch 23 train loss: 1.1496, eval loss 1.6752244234085083\n",
                  "Epoch 24 train loss: 1.1824, eval loss 1.6741610765457153\n",
                  "Epoch 25 train loss: 1.1214, eval loss 1.6709576845169067\n",
                  "Epoch 26 train loss: 1.1218, eval loss 1.6685893535614014\n",
                  "Epoch 27 train loss: 1.1325, eval loss 1.6665513515472412\n",
                  "Epoch 28 train loss: 1.1544, eval loss 1.6637914180755615\n",
                  "Epoch 29 train loss: 1.1375, eval loss 1.6625189781188965\n",
                  "Epoch 30 train loss: 1.0965, eval loss 1.6602283716201782\n",
                  "Epoch 31 train loss: 1.0838, eval loss 1.6574597358703613\n",
                  "Epoch 32 train loss: 1.0817, eval loss 1.6549289226531982\n",
                  "Epoch 33 train loss: 1.1067, eval loss 1.6534919738769531\n",
                  "Epoch 34 train loss: 1.0549, eval loss 1.6509873867034912\n",
                  "Epoch 35 train loss: 1.0857, eval loss 1.648719072341919\n",
                  "Epoch 36 train loss: 1.0467, eval loss 1.6478731632232666\n",
                  "Epoch 37 train loss: 1.0559, eval loss 1.6446763277053833\n",
                  "Epoch 38 train loss: 1.0622, eval loss 1.6432245969772339\n",
                  "Epoch 39 train loss: 1.0447, eval loss 1.6416735649108887\n",
                  "Epoch 40 train loss: 1.0636, eval loss 1.6399809122085571\n",
                  "Epoch 41 train loss: 1.0435, eval loss 1.6381481885910034\n",
                  "Epoch 42 train loss: 1.0421, eval loss 1.6367089748382568\n",
                  "Epoch 43 train loss: 1.0031, eval loss 1.6346006393432617\n",
                  "Epoch 44 train loss: 1.0368, eval loss 1.633191466331482\n",
                  "Epoch 45 train loss: 1.0436, eval loss 1.631851077079773\n",
                  "Epoch 46 train loss: 1.0073, eval loss 1.6291483640670776\n",
                  "Epoch 47 train loss: 1.0386, eval loss 1.6283109188079834\n",
                  "Epoch 48 train loss: 1.0356, eval loss 1.6266201734542847\n",
                  "Epoch 49 train loss: 1.0396, eval loss 1.6249696016311646\n",
                  "Epoch 50 train loss: 0.9672, eval loss 1.6231520175933838\n",
                  "Epoch 51 train loss: 1.0049, eval loss 1.6216084957122803\n",
                  "Epoch 52 train loss: 0.9795, eval loss 1.6211527585983276\n",
                  "Epoch 53 train loss: 0.9747, eval loss 1.618589162826538\n",
                  "Epoch 54 train loss: 1.0425, eval loss 1.6175211668014526\n",
                  "Epoch 55 train loss: 0.9962, eval loss 1.6160471439361572\n",
                  "Epoch 56 train loss: 0.9949, eval loss 1.6142431497573853\n",
                  "Epoch 57 train loss: 0.9868, eval loss 1.6127362251281738\n",
                  "Epoch 58 train loss: 0.9919, eval loss 1.61125910282135\n",
                  "Epoch 59 train loss: 0.9671, eval loss 1.6105847358703613\n",
                  "Epoch 60 train loss: 0.9416, eval loss 1.6093333959579468\n",
                  "Epoch 61 train loss: 0.9372, eval loss 1.6070832014083862\n",
                  "Epoch 62 train loss: 0.9638, eval loss 1.6064276695251465\n",
                  "Epoch 63 train loss: 0.9481, eval loss 1.6048983335494995\n",
                  "Epoch 64 train loss: 0.9542, eval loss 1.6032644510269165\n",
                  "Epoch 65 train loss: 0.9537, eval loss 1.6021294593811035\n",
                  "Epoch 66 train loss: 0.9703, eval loss 1.6012641191482544\n",
                  "Epoch 67 train loss: 0.9487, eval loss 1.5995173454284668\n",
                  "Epoch 68 train loss: 0.9637, eval loss 1.598789930343628\n",
                  "Epoch 69 train loss: 0.9766, eval loss 1.5975430011749268\n",
                  "Epoch 70 train loss: 0.9291, eval loss 1.5961337089538574\n",
                  "Epoch 71 train loss: 0.9143, eval loss 1.5957996845245361\n",
                  "Epoch 72 train loss: 0.9392, eval loss 1.5948330163955688\n",
                  "Epoch 73 train loss: 0.9280, eval loss 1.5934253931045532\n",
                  "Epoch 74 train loss: 0.9209, eval loss 1.592211127281189\n",
                  "Epoch 75 train loss: 0.9211, eval loss 1.590827465057373\n",
                  "Epoch 76 train loss: 0.9163, eval loss 1.5893443822860718\n",
                  "Epoch 77 train loss: 0.9265, eval loss 1.589523196220398\n",
                  "Epoch 78 train loss: 0.9208, eval loss 1.588916540145874\n",
                  "Epoch 79 train loss: 0.8937, eval loss 1.5869616270065308\n",
                  "Epoch 80 train loss: 0.9192, eval loss 1.5854651927947998\n",
                  "Epoch 81 train loss: 0.9068, eval loss 1.585152506828308\n",
                  "Epoch 82 train loss: 0.9248, eval loss 1.583631992340088\n",
                  "Epoch 83 train loss: 0.9294, eval loss 1.582671880722046\n",
                  "Epoch 84 train loss: 0.8912, eval loss 1.5817921161651611\n",
                  "Epoch 85 train loss: 0.9201, eval loss 1.5804402828216553\n",
                  "Epoch 86 train loss: 0.8965, eval loss 1.579346776008606\n",
                  "Epoch 87 train loss: 0.8998, eval loss 1.5781437158584595\n",
                  "Epoch 88 train loss: 0.9046, eval loss 1.5775830745697021\n",
                  "Epoch 89 train loss: 0.9185, eval loss 1.576920747756958\n",
                  "Epoch 90 train loss: 0.8818, eval loss 1.5759319067001343\n",
                  "Epoch 91 train loss: 0.8715, eval loss 1.5739599466323853\n",
                  "Epoch 92 train loss: 0.8476, eval loss 1.5744565725326538\n",
                  "Epoch 93 train loss: 0.8919, eval loss 1.5732166767120361\n",
                  "Epoch 94 train loss: 0.8871, eval loss 1.5709999799728394\n",
                  "Epoch 95 train loss: 0.8658, eval loss 1.570920705795288\n",
                  "Epoch 96 train loss: 0.9022, eval loss 1.5696130990982056\n",
                  "Epoch 97 train loss: 0.8525, eval loss 1.5681737661361694\n",
                  "Epoch 98 train loss: 0.9032, eval loss 1.5683817863464355\n",
                  "Epoch 99 train loss: 0.8577, eval loss 1.5669928789138794\n",
                  "Epoch 100 train loss: 0.9221, eval loss 1.5662020444869995\n",
                  "Epoch 101 train loss: 0.8724, eval loss 1.5654343366622925\n",
                  "Epoch 102 train loss: 0.8731, eval loss 1.5645312070846558\n",
                  "Epoch 103 train loss: 0.8888, eval loss 1.5645205974578857\n",
                  "Epoch 104 train loss: 0.8493, eval loss 1.5622562170028687\n",
                  "Epoch 105 train loss: 0.8855, eval loss 1.56257164478302\n",
                  "Epoch 106 train loss: 0.8628, eval loss 1.5615886449813843\n",
                  "Epoch 107 train loss: 0.8594, eval loss 1.560174822807312\n",
                  "Epoch 108 train loss: 0.8523, eval loss 1.559621810913086\n",
                  "Epoch 109 train loss: 0.8505, eval loss 1.5596803426742554\n",
                  "Epoch 110 train loss: 0.8372, eval loss 1.5579338073730469\n",
                  "Epoch 111 train loss: 0.8635, eval loss 1.5570577383041382\n",
                  "Epoch 112 train loss: 0.8336, eval loss 1.556088924407959\n",
                  "Epoch 113 train loss: 0.8838, eval loss 1.556275725364685\n",
                  "Epoch 114 train loss: 0.8571, eval loss 1.5556100606918335\n",
                  "Epoch 115 train loss: 0.8439, eval loss 1.5546226501464844\n",
                  "Epoch 116 train loss: 0.8657, eval loss 1.553517460823059\n",
                  "Epoch 117 train loss: 0.8554, eval loss 1.5534695386886597\n",
                  "Epoch 118 train loss: 0.8397, eval loss 1.5519579648971558\n",
                  "Epoch 119 train loss: 0.8541, eval loss 1.5513509511947632\n",
                  "Epoch 120 train loss: 0.8496, eval loss 1.5499463081359863\n",
                  "Epoch 121 train loss: 0.8424, eval loss 1.5498096942901611\n",
                  "Epoch 122 train loss: 0.8457, eval loss 1.5488086938858032\n",
                  "Epoch 123 train loss: 0.8296, eval loss 1.548893690109253\n",
                  "Epoch 124 train loss: 0.8225, eval loss 1.5474258661270142\n",
                  "Epoch 125 train loss: 0.8443, eval loss 1.5468205213546753\n",
                  "Epoch 126 train loss: 0.8184, eval loss 1.5466961860656738\n",
                  "Epoch 127 train loss: 0.8246, eval loss 1.5455855131149292\n",
                  "Epoch 128 train loss: 0.8395, eval loss 1.5445129871368408\n",
                  "Epoch 129 train loss: 0.8421, eval loss 1.544510841369629\n",
                  "Epoch 130 train loss: 0.8025, eval loss 1.543440580368042\n",
                  "Epoch 131 train loss: 0.8328, eval loss 1.5435973405838013\n",
                  "Epoch 132 train loss: 0.8351, eval loss 1.5416886806488037\n",
                  "Epoch 133 train loss: 0.8308, eval loss 1.5416501760482788\n",
                  "Epoch 134 train loss: 0.7902, eval loss 1.5405620336532593\n",
                  "Epoch 135 train loss: 0.8320, eval loss 1.5401484966278076\n",
                  "Epoch 136 train loss: 0.8235, eval loss 1.5389491319656372\n",
                  "Epoch 137 train loss: 0.8217, eval loss 1.5388944149017334\n",
                  "Epoch 138 train loss: 0.8074, eval loss 1.5383130311965942\n",
                  "Epoch 139 train loss: 0.7941, eval loss 1.5369093418121338\n",
                  "Epoch 140 train loss: 0.7957, eval loss 1.5355420112609863\n",
                  "Epoch 141 train loss: 0.8311, eval loss 1.53621506690979\n",
                  "Epoch 142 train loss: 0.7929, eval loss 1.535362958908081\n",
                  "Epoch 143 train loss: 0.8144, eval loss 1.5346816778182983\n",
                  "Epoch 144 train loss: 0.8045, eval loss 1.5342974662780762\n",
                  "Epoch 145 train loss: 0.8234, eval loss 1.533180594444275\n",
                  "Epoch 146 train loss: 0.7950, eval loss 1.5333094596862793\n",
                  "Epoch 147 train loss: 0.7639, eval loss 1.5317745208740234\n",
                  "Epoch 148 train loss: 0.8200, eval loss 1.5319392681121826\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:11:44,044] Trial 28 finished with value: 1.5315419435501099 and parameters: {'hidden_layers_size': 198, 'dropout_p': 0.27946060181245025, 'learning_rate': 2.280249025495501e-06, 'batch_size': 382, 'l2_reg': 1.0216986632808458e-05}. Best is trial 20 with value: 1.6402353048324585.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 0.8195, eval loss 1.5315419435501099\n",
                  "Epoch 0 train loss: 1.7420, eval loss 1.8367434740066528\n",
                  "Epoch 1 train loss: 1.7655, eval loss 1.8365254402160645\n",
                  "Epoch 2 train loss: 1.8151, eval loss 1.8361928462982178\n",
                  "Epoch 3 train loss: 1.7700, eval loss 1.8357040882110596\n",
                  "Epoch 4 train loss: 1.7694, eval loss 1.8361893892288208\n",
                  "Epoch 5 train loss: 1.7567, eval loss 1.834889531135559\n",
                  "Epoch 6 train loss: 1.8250, eval loss 1.8343600034713745\n",
                  "Epoch 7 train loss: 1.8244, eval loss 1.8344156742095947\n",
                  "Epoch 8 train loss: 1.7215, eval loss 1.8322224617004395\n",
                  "Epoch 9 train loss: 1.7227, eval loss 1.8320250511169434\n",
                  "Epoch 10 train loss: 1.7600, eval loss 1.83098566532135\n",
                  "Epoch 11 train loss: 1.7656, eval loss 1.8322641849517822\n",
                  "Epoch 12 train loss: 1.7256, eval loss 1.830580472946167\n",
                  "Epoch 13 train loss: 1.7514, eval loss 1.8304786682128906\n",
                  "Epoch 14 train loss: 1.7838, eval loss 1.8299674987792969\n",
                  "Epoch 15 train loss: 1.7590, eval loss 1.8290210962295532\n",
                  "Epoch 16 train loss: 1.7570, eval loss 1.8291414976119995\n",
                  "Epoch 17 train loss: 1.7353, eval loss 1.8279019594192505\n",
                  "Epoch 18 train loss: 1.7497, eval loss 1.8272310495376587\n",
                  "Epoch 19 train loss: 1.6884, eval loss 1.8267521858215332\n",
                  "Epoch 20 train loss: 1.7272, eval loss 1.8266817331314087\n",
                  "Epoch 21 train loss: 1.7358, eval loss 1.8252428770065308\n",
                  "Epoch 22 train loss: 1.7759, eval loss 1.8257657289505005\n",
                  "Epoch 23 train loss: 1.7423, eval loss 1.825995922088623\n",
                  "Epoch 24 train loss: 1.7631, eval loss 1.8227649927139282\n",
                  "Epoch 25 train loss: 1.7007, eval loss 1.8231455087661743\n",
                  "Epoch 26 train loss: 1.6818, eval loss 1.822570562362671\n",
                  "Epoch 27 train loss: 1.7630, eval loss 1.8226124048233032\n",
                  "Epoch 28 train loss: 1.6993, eval loss 1.8216596841812134\n",
                  "Epoch 29 train loss: 1.7191, eval loss 1.8217556476593018\n",
                  "Epoch 30 train loss: 1.7010, eval loss 1.822105884552002\n",
                  "Epoch 31 train loss: 1.7609, eval loss 1.8201680183410645\n",
                  "Epoch 32 train loss: 1.7592, eval loss 1.820066213607788\n",
                  "Epoch 33 train loss: 1.7363, eval loss 1.8200510740280151\n",
                  "Epoch 34 train loss: 1.7058, eval loss 1.8192825317382812\n",
                  "Epoch 35 train loss: 1.6903, eval loss 1.8175760507583618\n",
                  "Epoch 36 train loss: 1.7092, eval loss 1.8180334568023682\n",
                  "Epoch 37 train loss: 1.7200, eval loss 1.815900206565857\n",
                  "Epoch 38 train loss: 1.7384, eval loss 1.817353367805481\n",
                  "Epoch 39 train loss: 1.7077, eval loss 1.8161728382110596\n",
                  "Epoch 40 train loss: 1.7098, eval loss 1.815913438796997\n",
                  "Epoch 41 train loss: 1.6945, eval loss 1.8137080669403076\n",
                  "Epoch 42 train loss: 1.7155, eval loss 1.81320059299469\n",
                  "Epoch 43 train loss: 1.6871, eval loss 1.8139888048171997\n",
                  "Epoch 44 train loss: 1.6565, eval loss 1.8136446475982666\n",
                  "Epoch 45 train loss: 1.6802, eval loss 1.8124433755874634\n",
                  "Epoch 46 train loss: 1.6582, eval loss 1.812280297279358\n",
                  "Epoch 47 train loss: 1.6633, eval loss 1.8109116554260254\n",
                  "Epoch 48 train loss: 1.6798, eval loss 1.8115030527114868\n",
                  "Epoch 49 train loss: 1.7271, eval loss 1.8105055093765259\n",
                  "Epoch 50 train loss: 1.7041, eval loss 1.8092846870422363\n",
                  "Epoch 51 train loss: 1.6489, eval loss 1.809189796447754\n",
                  "Epoch 52 train loss: 1.6601, eval loss 1.809153437614441\n",
                  "Epoch 53 train loss: 1.7112, eval loss 1.808767557144165\n",
                  "Epoch 54 train loss: 1.6937, eval loss 1.8076329231262207\n",
                  "Epoch 55 train loss: 1.6857, eval loss 1.807673454284668\n",
                  "Epoch 56 train loss: 1.6358, eval loss 1.8063441514968872\n",
                  "Epoch 57 train loss: 1.6683, eval loss 1.8063786029815674\n",
                  "Epoch 58 train loss: 1.6762, eval loss 1.8063485622406006\n",
                  "Epoch 59 train loss: 1.6425, eval loss 1.8053815364837646\n",
                  "Epoch 60 train loss: 1.6665, eval loss 1.805729627609253\n",
                  "Epoch 61 train loss: 1.6527, eval loss 1.80384361743927\n",
                  "Epoch 62 train loss: 1.6370, eval loss 1.8043309450149536\n",
                  "Epoch 63 train loss: 1.6579, eval loss 1.803719162940979\n",
                  "Epoch 64 train loss: 1.6908, eval loss 1.802383303642273\n",
                  "Epoch 65 train loss: 1.6932, eval loss 1.801801323890686\n",
                  "Epoch 66 train loss: 1.6110, eval loss 1.800713062286377\n",
                  "Epoch 67 train loss: 1.6243, eval loss 1.8014951944351196\n",
                  "Epoch 68 train loss: 1.6458, eval loss 1.8009459972381592\n",
                  "Epoch 69 train loss: 1.6822, eval loss 1.8000388145446777\n",
                  "Epoch 70 train loss: 1.6324, eval loss 1.7999383211135864\n",
                  "Epoch 71 train loss: 1.6243, eval loss 1.7986526489257812\n",
                  "Epoch 72 train loss: 1.6115, eval loss 1.7984062433242798\n",
                  "Epoch 73 train loss: 1.6172, eval loss 1.7983282804489136\n",
                  "Epoch 74 train loss: 1.6856, eval loss 1.7977310419082642\n",
                  "Epoch 75 train loss: 1.6169, eval loss 1.7976120710372925\n",
                  "Epoch 76 train loss: 1.6158, eval loss 1.7967288494110107\n",
                  "Epoch 77 train loss: 1.6945, eval loss 1.7967787981033325\n",
                  "Epoch 78 train loss: 1.6227, eval loss 1.7961829900741577\n",
                  "Epoch 79 train loss: 1.6227, eval loss 1.794419765472412\n",
                  "Epoch 80 train loss: 1.6243, eval loss 1.7950119972229004\n",
                  "Epoch 81 train loss: 1.6323, eval loss 1.7937490940093994\n",
                  "Epoch 82 train loss: 1.6714, eval loss 1.7933928966522217\n",
                  "Epoch 83 train loss: 1.6223, eval loss 1.7937084436416626\n",
                  "Epoch 84 train loss: 1.6148, eval loss 1.793218970298767\n",
                  "Epoch 85 train loss: 1.5772, eval loss 1.7920958995819092\n",
                  "Epoch 86 train loss: 1.6310, eval loss 1.7913777828216553\n",
                  "Epoch 87 train loss: 1.6289, eval loss 1.7917646169662476\n",
                  "Epoch 88 train loss: 1.6061, eval loss 1.7910654544830322\n",
                  "Epoch 89 train loss: 1.6146, eval loss 1.789476752281189\n",
                  "Epoch 90 train loss: 1.6391, eval loss 1.7903623580932617\n",
                  "Epoch 91 train loss: 1.5577, eval loss 1.7894744873046875\n",
                  "Epoch 92 train loss: 1.5675, eval loss 1.7885305881500244\n",
                  "Epoch 93 train loss: 1.5840, eval loss 1.7874329090118408\n",
                  "Epoch 94 train loss: 1.5422, eval loss 1.7877241373062134\n",
                  "Epoch 95 train loss: 1.6290, eval loss 1.7870521545410156\n",
                  "Epoch 96 train loss: 1.6384, eval loss 1.7872459888458252\n",
                  "Epoch 97 train loss: 1.5651, eval loss 1.7866743803024292\n",
                  "Epoch 98 train loss: 1.5831, eval loss 1.7854722738265991\n",
                  "Epoch 99 train loss: 1.6093, eval loss 1.7859549522399902\n",
                  "Epoch 100 train loss: 1.6146, eval loss 1.7847747802734375\n",
                  "Epoch 101 train loss: 1.5746, eval loss 1.7844181060791016\n",
                  "Epoch 102 train loss: 1.5637, eval loss 1.7844030857086182\n",
                  "Epoch 103 train loss: 1.5635, eval loss 1.7847076654434204\n",
                  "Epoch 104 train loss: 1.5962, eval loss 1.7840512990951538\n",
                  "Epoch 105 train loss: 1.5279, eval loss 1.7824397087097168\n",
                  "Epoch 106 train loss: 1.5668, eval loss 1.7825682163238525\n",
                  "Epoch 107 train loss: 1.6233, eval loss 1.7812856435775757\n",
                  "Epoch 108 train loss: 1.5954, eval loss 1.7815934419631958\n",
                  "Epoch 109 train loss: 1.5504, eval loss 1.7799521684646606\n",
                  "Epoch 110 train loss: 1.5518, eval loss 1.7803772687911987\n",
                  "Epoch 111 train loss: 1.6174, eval loss 1.7799423933029175\n",
                  "Epoch 112 train loss: 1.5681, eval loss 1.7788307666778564\n",
                  "Epoch 113 train loss: 1.5913, eval loss 1.7793382406234741\n",
                  "Epoch 114 train loss: 1.5472, eval loss 1.7774096727371216\n",
                  "Epoch 115 train loss: 1.5673, eval loss 1.778714656829834\n",
                  "Epoch 116 train loss: 1.5131, eval loss 1.7775517702102661\n",
                  "Epoch 117 train loss: 1.5709, eval loss 1.7763144969940186\n",
                  "Epoch 118 train loss: 1.5419, eval loss 1.7769920825958252\n",
                  "Epoch 119 train loss: 1.5584, eval loss 1.776490330696106\n",
                  "Epoch 120 train loss: 1.5631, eval loss 1.775578498840332\n",
                  "Epoch 121 train loss: 1.5606, eval loss 1.7749913930892944\n",
                  "Epoch 122 train loss: 1.5474, eval loss 1.77384614944458\n",
                  "Epoch 123 train loss: 1.5775, eval loss 1.7740747928619385\n",
                  "Epoch 124 train loss: 1.5027, eval loss 1.774293303489685\n",
                  "Epoch 125 train loss: 1.5569, eval loss 1.7742143869400024\n",
                  "Epoch 126 train loss: 1.5613, eval loss 1.772764801979065\n",
                  "Epoch 127 train loss: 1.4835, eval loss 1.7726337909698486\n",
                  "Epoch 128 train loss: 1.5418, eval loss 1.772838830947876\n",
                  "Epoch 129 train loss: 1.5361, eval loss 1.7724931240081787\n",
                  "Epoch 130 train loss: 1.5050, eval loss 1.7717162370681763\n",
                  "Epoch 131 train loss: 1.5009, eval loss 1.7712922096252441\n",
                  "Epoch 132 train loss: 1.5121, eval loss 1.7708088159561157\n",
                  "Epoch 133 train loss: 1.5349, eval loss 1.7703909873962402\n",
                  "Epoch 134 train loss: 1.5211, eval loss 1.7705432176589966\n",
                  "Epoch 135 train loss: 1.5245, eval loss 1.7696729898452759\n",
                  "Epoch 136 train loss: 1.5414, eval loss 1.769102931022644\n",
                  "Epoch 137 train loss: 1.5210, eval loss 1.768677830696106\n",
                  "Epoch 138 train loss: 1.5424, eval loss 1.768642544746399\n",
                  "Epoch 139 train loss: 1.5574, eval loss 1.7667303085327148\n",
                  "Epoch 140 train loss: 1.5460, eval loss 1.7669581174850464\n",
                  "Epoch 141 train loss: 1.5032, eval loss 1.767021656036377\n",
                  "Epoch 142 train loss: 1.4669, eval loss 1.765872597694397\n",
                  "Epoch 143 train loss: 1.4595, eval loss 1.7657134532928467\n",
                  "Epoch 144 train loss: 1.4562, eval loss 1.7644996643066406\n",
                  "Epoch 145 train loss: 1.5081, eval loss 1.7650760412216187\n",
                  "Epoch 146 train loss: 1.5227, eval loss 1.7635375261306763\n",
                  "Epoch 147 train loss: 1.5396, eval loss 1.7642717361450195\n",
                  "Epoch 148 train loss: 1.5452, eval loss 1.7629363536834717\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[I 2023-11-27 12:12:40,012] Trial 29 finished with value: 1.7629363536834717 and parameters: {'hidden_layers_size': 56, 'dropout_p': 0.3454320083249626, 'learning_rate': 1.3472851063516434e-06, 'batch_size': 328, 'l2_reg': 0.0002133217242908211}. Best is trial 29 with value: 1.7629363536834717.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 149 train loss: 1.4796, eval loss 1.7639915943145752\n"
               ]
            }
         ],
         "source": [
            "study_random = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.RandomSampler())\n",
            "study_random.optimize(objective, n_trials=30)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 169,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'hidden_layers_size': 56,\n",
                     " 'dropout_p': 0.3454320083249626,\n",
                     " 'learning_rate': 1.3472851063516434e-06,\n",
                     " 'batch_size': 328,\n",
                     " 'l2_reg': 0.0002133217242908211}"
                  ]
               },
               "execution_count": 169,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "trial_random = study_random.best_trial\n",
            "\n",
            "trial_random.params"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 170,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 0 train loss: 1.5234, eval loss 1.7752177715301514\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 1 train loss: 1.4828, eval loss 1.7721816301345825\n",
                  "Epoch 2 train loss: 1.4684, eval loss 1.7726083993911743\n",
                  "Epoch 3 train loss: 1.4572, eval loss 1.7713658809661865\n",
                  "Epoch 4 train loss: 1.4982, eval loss 1.771571159362793\n",
                  "Epoch 5 train loss: 1.5056, eval loss 1.770401954650879\n",
                  "Epoch 6 train loss: 1.4066, eval loss 1.7702879905700684\n",
                  "Epoch 7 train loss: 1.4497, eval loss 1.7689074277877808\n",
                  "Epoch 8 train loss: 1.4960, eval loss 1.7686022520065308\n",
                  "Epoch 9 train loss: 1.4309, eval loss 1.7688196897506714\n",
                  "Epoch 10 train loss: 1.4651, eval loss 1.7683310508728027\n",
                  "Epoch 11 train loss: 1.4655, eval loss 1.7678643465042114\n",
                  "Epoch 12 train loss: 1.4323, eval loss 1.7667655944824219\n",
                  "Epoch 13 train loss: 1.4661, eval loss 1.7670286893844604\n",
                  "Epoch 14 train loss: 1.3735, eval loss 1.7656943798065186\n",
                  "Epoch 15 train loss: 1.4271, eval loss 1.7652122974395752\n",
                  "Epoch 16 train loss: 1.4654, eval loss 1.7640377283096313\n",
                  "Epoch 17 train loss: 1.4586, eval loss 1.763957142829895\n",
                  "Epoch 18 train loss: 1.4264, eval loss 1.762635350227356\n",
                  "Epoch 19 train loss: 1.4221, eval loss 1.7610281705856323\n",
                  "Epoch 20 train loss: 1.4485, eval loss 1.7609045505523682\n",
                  "Epoch 21 train loss: 1.4302, eval loss 1.7596080303192139\n",
                  "Epoch 22 train loss: 1.4636, eval loss 1.7605916261672974\n",
                  "Epoch 23 train loss: 1.3965, eval loss 1.759772539138794\n",
                  "Epoch 24 train loss: 1.3997, eval loss 1.7591346502304077\n",
                  "Epoch 25 train loss: 1.4104, eval loss 1.7586417198181152\n",
                  "Epoch 26 train loss: 1.4086, eval loss 1.756998062133789\n",
                  "Epoch 27 train loss: 1.4753, eval loss 1.7571126222610474\n",
                  "Epoch 28 train loss: 1.4424, eval loss 1.756380319595337\n",
                  "Epoch 29 train loss: 1.4141, eval loss 1.754570484161377\n",
                  "Epoch 30 train loss: 1.3869, eval loss 1.7559396028518677\n",
                  "Epoch 31 train loss: 1.4076, eval loss 1.7550626993179321\n",
                  "Epoch 32 train loss: 1.4194, eval loss 1.7541115283966064\n",
                  "Epoch 33 train loss: 1.3324, eval loss 1.7534199953079224\n",
                  "Epoch 34 train loss: 1.4279, eval loss 1.7539253234863281\n",
                  "Epoch 35 train loss: 1.3845, eval loss 1.751582145690918\n",
                  "Epoch 36 train loss: 1.4249, eval loss 1.7518678903579712\n",
                  "Epoch 37 train loss: 1.4115, eval loss 1.751571536064148\n",
                  "Epoch 38 train loss: 1.3253, eval loss 1.7502799034118652\n",
                  "Epoch 39 train loss: 1.3786, eval loss 1.7492566108703613\n",
                  "Epoch 40 train loss: 1.3578, eval loss 1.7503008842468262\n",
                  "Epoch 41 train loss: 1.3820, eval loss 1.7492573261260986\n",
                  "Epoch 42 train loss: 1.3509, eval loss 1.749833345413208\n",
                  "Epoch 43 train loss: 1.4147, eval loss 1.7479649782180786\n",
                  "Epoch 44 train loss: 1.4127, eval loss 1.7474602460861206\n",
                  "Epoch 45 train loss: 1.4127, eval loss 1.747550368309021\n",
                  "Epoch 46 train loss: 1.4000, eval loss 1.7468470335006714\n",
                  "Epoch 47 train loss: 1.4182, eval loss 1.7458488941192627\n",
                  "Epoch 48 train loss: 1.4019, eval loss 1.7451850175857544\n",
                  "Epoch 49 train loss: 1.3591, eval loss 1.745029091835022\n",
                  "Epoch 50 train loss: 1.3900, eval loss 1.7436933517456055\n",
                  "Epoch 51 train loss: 1.4384, eval loss 1.7436105012893677\n",
                  "Epoch 52 train loss: 1.4080, eval loss 1.7426146268844604\n",
                  "Epoch 53 train loss: 1.4084, eval loss 1.7423996925354004\n",
                  "Epoch 54 train loss: 1.4117, eval loss 1.7418992519378662\n",
                  "Epoch 55 train loss: 1.3750, eval loss 1.741796851158142\n",
                  "Epoch 56 train loss: 1.3890, eval loss 1.7411627769470215\n",
                  "Epoch 57 train loss: 1.3840, eval loss 1.7398992776870728\n",
                  "Epoch 58 train loss: 1.3708, eval loss 1.7384471893310547\n",
                  "Epoch 59 train loss: 1.3596, eval loss 1.740502953529358\n",
                  "Epoch 60 train loss: 1.3305, eval loss 1.7393910884857178\n",
                  "Epoch 61 train loss: 1.3962, eval loss 1.7374552488327026\n",
                  "Epoch 62 train loss: 1.3972, eval loss 1.7371798753738403\n",
                  "Epoch 63 train loss: 1.3143, eval loss 1.736654281616211\n",
                  "Epoch 64 train loss: 1.3579, eval loss 1.7371506690979004\n",
                  "Epoch 65 train loss: 1.3299, eval loss 1.7357773780822754\n",
                  "Epoch 66 train loss: 1.3793, eval loss 1.7343730926513672\n",
                  "Epoch 67 train loss: 1.3646, eval loss 1.7347381114959717\n",
                  "Epoch 68 train loss: 1.3623, eval loss 1.7328063249588013\n",
                  "Epoch 69 train loss: 1.3677, eval loss 1.7340543270111084\n",
                  "Epoch 70 train loss: 1.3319, eval loss 1.7332831621170044\n",
                  "Epoch 71 train loss: 1.3092, eval loss 1.733762502670288\n",
                  "Epoch 72 train loss: 1.3088, eval loss 1.732682466506958\n",
                  "Epoch 73 train loss: 1.3643, eval loss 1.7318841218948364\n",
                  "Epoch 74 train loss: 1.3505, eval loss 1.7311255931854248\n",
                  "Epoch 75 train loss: 1.3360, eval loss 1.7306102514266968\n",
                  "Epoch 76 train loss: 1.2997, eval loss 1.7302258014678955\n",
                  "Epoch 77 train loss: 1.3780, eval loss 1.7298179864883423\n",
                  "Epoch 78 train loss: 1.2921, eval loss 1.7291889190673828\n",
                  "Epoch 79 train loss: 1.3105, eval loss 1.7290464639663696\n",
                  "Epoch 80 train loss: 1.3422, eval loss 1.7297645807266235\n",
                  "Epoch 81 train loss: 1.3755, eval loss 1.7265268564224243\n",
                  "Epoch 82 train loss: 1.2977, eval loss 1.7260942459106445\n",
                  "Epoch 83 train loss: 1.3284, eval loss 1.7261171340942383\n",
                  "Epoch 84 train loss: 1.2991, eval loss 1.726261854171753\n",
                  "Epoch 85 train loss: 1.2764, eval loss 1.7262545824050903\n",
                  "Epoch 86 train loss: 1.2782, eval loss 1.7253443002700806\n",
                  "Epoch 87 train loss: 1.2895, eval loss 1.7243553400039673\n",
                  "Epoch 88 train loss: 1.3267, eval loss 1.7237964868545532\n",
                  "Epoch 89 train loss: 1.3711, eval loss 1.7232937812805176\n",
                  "Epoch 90 train loss: 1.2772, eval loss 1.7230740785598755\n",
                  "Epoch 91 train loss: 1.3736, eval loss 1.722597599029541\n",
                  "Epoch 92 train loss: 1.2625, eval loss 1.721769094467163\n",
                  "Epoch 93 train loss: 1.3404, eval loss 1.721799612045288\n",
                  "Epoch 94 train loss: 1.2703, eval loss 1.719663381576538\n",
                  "Epoch 95 train loss: 1.3365, eval loss 1.7200449705123901\n",
                  "Epoch 96 train loss: 1.3059, eval loss 1.7197694778442383\n",
                  "Epoch 97 train loss: 1.3486, eval loss 1.719436764717102\n",
                  "Epoch 98 train loss: 1.3114, eval loss 1.7190768718719482\n",
                  "Epoch 99 train loss: 1.2686, eval loss 1.7186874151229858\n",
                  "Epoch 100 train loss: 1.3426, eval loss 1.717756986618042\n",
                  "Epoch 101 train loss: 1.2994, eval loss 1.7169777154922485\n",
                  "Epoch 102 train loss: 1.3505, eval loss 1.717178463935852\n",
                  "Epoch 103 train loss: 1.2985, eval loss 1.7162079811096191\n",
                  "Epoch 104 train loss: 1.3398, eval loss 1.7162368297576904\n",
                  "Epoch 105 train loss: 1.2902, eval loss 1.7150640487670898\n",
                  "Epoch 106 train loss: 1.2452, eval loss 1.714449167251587\n",
                  "Epoch 107 train loss: 1.2460, eval loss 1.715423345565796\n",
                  "Epoch 108 train loss: 1.2621, eval loss 1.7131683826446533\n",
                  "Epoch 109 train loss: 1.2993, eval loss 1.7128523588180542\n",
                  "Epoch 110 train loss: 1.2605, eval loss 1.713813066482544\n",
                  "Epoch 111 train loss: 1.3042, eval loss 1.7117496728897095\n",
                  "Epoch 112 train loss: 1.2359, eval loss 1.711962103843689\n",
                  "Epoch 113 train loss: 1.3403, eval loss 1.711441159248352\n",
                  "Epoch 114 train loss: 1.2686, eval loss 1.7114522457122803\n",
                  "Epoch 115 train loss: 1.3096, eval loss 1.7107515335083008\n",
                  "Epoch 116 train loss: 1.2595, eval loss 1.7100147008895874\n",
                  "Epoch 117 train loss: 1.2471, eval loss 1.7093435525894165\n",
                  "Epoch 118 train loss: 1.2415, eval loss 1.7099603414535522\n",
                  "Epoch 119 train loss: 1.2588, eval loss 1.7078957557678223\n",
                  "Epoch 120 train loss: 1.2654, eval loss 1.7081518173217773\n",
                  "Epoch 121 train loss: 1.2385, eval loss 1.7082672119140625\n",
                  "Epoch 122 train loss: 1.2767, eval loss 1.7062586545944214\n",
                  "Epoch 123 train loss: 1.2648, eval loss 1.7071195840835571\n",
                  "Epoch 124 train loss: 1.2178, eval loss 1.7065311670303345\n",
                  "Epoch 125 train loss: 1.2609, eval loss 1.7066353559494019\n",
                  "Epoch 126 train loss: 1.2635, eval loss 1.7049071788787842\n",
                  "Epoch 127 train loss: 1.2267, eval loss 1.704676628112793\n",
                  "Epoch 128 train loss: 1.2598, eval loss 1.7052903175354004\n",
                  "Epoch 129 train loss: 1.2697, eval loss 1.7038352489471436\n",
                  "Epoch 130 train loss: 1.2771, eval loss 1.7049858570098877\n",
                  "Epoch 131 train loss: 1.2460, eval loss 1.7023937702178955\n",
                  "Epoch 132 train loss: 1.2501, eval loss 1.7041945457458496\n",
                  "Epoch 133 train loss: 1.2531, eval loss 1.7030245065689087\n",
                  "Epoch 134 train loss: 1.2464, eval loss 1.702046513557434\n",
                  "Epoch 135 train loss: 1.2130, eval loss 1.7015236616134644\n",
                  "Epoch 136 train loss: 1.2533, eval loss 1.7007428407669067\n",
                  "Epoch 137 train loss: 1.2147, eval loss 1.7014113664627075\n",
                  "Epoch 138 train loss: 1.1808, eval loss 1.699170708656311\n",
                  "Epoch 139 train loss: 1.2160, eval loss 1.6985676288604736\n",
                  "Epoch 140 train loss: 1.2174, eval loss 1.7006545066833496\n",
                  "Epoch 141 train loss: 1.2824, eval loss 1.6993298530578613\n",
                  "Epoch 142 train loss: 1.2142, eval loss 1.6995733976364136\n",
                  "Epoch 143 train loss: 1.1908, eval loss 1.6979917287826538\n",
                  "Epoch 144 train loss: 1.2530, eval loss 1.6981017589569092\n",
                  "Epoch 145 train loss: 1.2129, eval loss 1.6981616020202637\n",
                  "Epoch 146 train loss: 1.2578, eval loss 1.6976449489593506\n",
                  "Epoch 147 train loss: 1.2413, eval loss 1.6977267265319824\n",
                  "Epoch 148 train loss: 1.2536, eval loss 1.6972832679748535\n",
                  "Epoch 149 train loss: 1.2790, eval loss 1.6961599588394165\n",
                  "====================================\n",
                  "AUROC: 83.91%\n",
                  "F1: 59.81%\n",
                  "Precision: 54.20%\n",
                  "Recall: 66.71%\n"
               ]
            }
         ],
         "source": [
            "model_random, test_metrics = get_model_stats(trial_random)\n",
            "\n",
            "print(\"====================================\")\n",
            "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
            "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
            "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
            "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Podsumowanie\n",
            "Oba treningi zostaly przeprowadzone dla max 150 epok. Poniżej znajduje się porównanie wyniku obu samplerów (`TPESampler` oraz `RandomSampler`)\n",
            "\n",
            "\n",
            "<table>\n",
            "    <thead>\n",
            "        <tr>\n",
            "            <td><strong>Wskaźnik</strong></td>\n",
            "            <td><strong>TPE</strong></td>\n",
            "            <td><strong>Random</strong></td>\n",
            "        </tr>\n",
            "    </thead>\n",
            "    <tbody>\n",
            "        <tr>\n",
            "            <td>AUROC</td>\n",
            "            <td>80.45%</td>\n",
            "            <td>83.91%</td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td>F1</td>\n",
            "            <td>56.22%</td>\n",
            "            <td>59.81%</td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td>Precision</td>\n",
            "            <td>44.62%</td>\n",
            "            <td>54.20%</td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td>Recall</td>        \n",
            "            <td>75.96%</td>\n",
            "            <td>66.71%</td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td colspan=\"3\">\n",
            "                <center><strong>Otrzymane parametry</strong></center>\n",
            "            </td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td>rozmiar warstw ukrytych (N)</td>\n",
            "            <td>87</td>\n",
            "            <td>56</td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td>prawdopodobieństwo dropoutu</td>\n",
            "            <td>0.489907983890005</td>\n",
            "            <td>0.3454320083249626</td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td>stała ucząca</td>\n",
            "            <td>1.5461728272636294e-06</td>\n",
            "            <td>1.3472851063516434e-06</td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td>batch_size</td>\n",
            "            <td>217</td>\n",
            "            <td>328</td>\n",
            "        </tr>\n",
            "        <tr>\n",
            "            <td>siła regularyzacji L2</td>\n",
            "            <td>3.202440992612983e-05</td>\n",
            "            <td>0.0002133217242908211</td>\n",
            "        </tr>\n",
            "    </tbody>\n",
            "</table>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Jak możemy zauważyć, w naszym teście lepiej wypadł RandomSampler. Pomimo mniejszego rozmiaru warstw ukrytych, udało mu się osiągnąć większy AUROC(83.91% vs 80.45%). Posiadał jednocześnie większy batch_size oraz lepszy wskaźnik F1. \n",
            "\n",
            "Model wyuczony z wykorzystaniem TPE posiadał natomiast lepszą czułość oraz wymagał mniejszej regularyzacji."
         ]
      }
   ],
   "metadata": {
      "colab": {
         "collapsed_sections": [],
         "provenance": []
      },
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.8.10"
      },
      "vscode": {
         "interpreter": {
            "hash": "a5d7af91182035c53be6efb3f9b18ffc3e259c9c524705249407647c970de949"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 1
}
